{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"sC2Apg2i_SQM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1b71d30e-e904-4b40-b5cf-eaee09b54558"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"]}],"source":["# GEMMA x AraStance with Dynamic Pooling (CPU-Friendly Google Colab Notebook)\n","# SETUP: Install & Imports\n","!pip install --upgrade transformers\n","!pip install datasets scikit-learn matplotlib seaborn tqdm\n","!git clone https://github.com/PiotrNawrot/dynamic-pooling.git\n","!pip install ./dynamic-pooling # Install the cloned repository\n","\n","import os\n","import json\n","import html\n","import torch\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n","\n","from dynamic_pooling.models.pooling import downsample\n","\n","random.seed(42)\n","np.random.seed(42)\n","torch.manual_seed(42)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)"]},{"cell_type":"code","source":["\n","# DATA LOADING + PREPROCESSING\n","\n","data_path = \"/content/drive/MyDrive/SNLP Group Project/Datasets/AraStance\"\n","train_file = os.path.join(data_path, \"train.jsonl\")\n","test_file = os.path.join(data_path, \"test.jsonl\")\n","\n","def load_jsonl(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        return [json.loads(line) for line in f]\n","\n","train_raw = load_jsonl(train_file)\n","test_raw = load_jsonl(test_file)\n","\n","def explode_dataset(data):\n","    pairs = []\n","    for row in data:\n","        for article, stance in zip(row['article'], row['stance']):\n","            title = row['article_title'][0] if isinstance(row['article_title'], list) else row['article_title']\n","            pairs.append({\n","                'claim': row['claim'],\n","                'article': article,\n","                'article_title': title,\n","                'stance': stance\n","            })\n","    return pd.DataFrame(pairs)\n","\n","train_df = explode_dataset(train_raw)\n","test_df = explode_dataset(test_raw)\n","\n","# EDA: Before and After Preprocessing\n","print(\"\\nORIGINAL FORMAT EXAMPLE (Before Preprocessing):\")\n","print(json.dumps(train_raw[0], indent=2, ensure_ascii=False)[:500] + \"...\")\n","\n","print(\"\\nAFTER PREPROCESSING: Claim-Article Pairs\")\n","print(train_df.head(3))\n","\n","print(\"Train size:\", len(train_df), \"Test size:\", len(test_df))\n","print(\"Train stance distribution:\\n\", train_df['stance'].value_counts())\n","print(\"Test stance distribution:\\n\", test_df['stance'].value_counts())"],"metadata":{"id":"zoqRRxb4sohz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count number of unique claims and total claim-article pairs\n","num_unique_claims = len(set(row['claim'] for row in train_raw))\n","print(\"\\n Unique Claims:\", num_unique_claims)\n","print(\"Total Claim-Article Pairs (Train):\", len(train_df))\n","\n","# Distribution of number of articles per claim (before preprocessing)\n","claim_article_counts = [len(row['article']) for row in train_raw]\n","plt.figure(figsize=(10,5))\n","plt.hist(claim_article_counts, bins=30, color='skyblue', edgecolor='black')\n","plt.title('Distribution of Articles per Claim (Before Preprocessing)')\n","plt.xlabel('Number of Articles')\n","plt.ylabel('Number of Claims')\n","plt.grid(True, alpha=0.3)\n","plt.show()"],"metadata":{"id":"78TMs9nisuQu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DATASET CLASS\n","class StanceDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_length=512):\n","        self.df = dataframe\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.label_map = {'Agree': 0, 'Disagree': 1, 'Discuss': 2, 'Unrelated': 3}\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        text = f\"Claim: {html.unescape(row['claim'])} Article: {html.unescape(row['article'][:5000])}\"\n","        enc = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n","        enc = {k: v.squeeze(0) for k, v in enc.items()}\n","        enc['labels'] = torch.tensor(self.label_map[row['stance']])\n","        return enc"],"metadata":{"id":"LJdMPA3rsuTK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# MODEL WRAPPER WITH DYNAMIC POOLING\n","class StanceModelWithDynamicPooling(nn.Module):\n","    def __init__(self, base_model_name, hidden_size=768, num_labels=4):\n","        super().__init__()\n","        self.encoder = AutoModel.from_pretrained(base_model_name)\n","        self.pool = lambda x, mask: downsample(x, mask, k=1, mode='mean')\n","        self.classifier = nn.Sequential(\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(hidden_size, num_labels)\n","        )\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled = self.pool(out.last_hidden_state, attention_mask)\n","        logits = self.classifier(pooled.squeeze(1))\n","        loss = nn.CrossEntropyLoss()(logits, labels) if labels is not None else None\n","        return {'loss': loss, 'logits': logits}"],"metadata":{"id":"R8WyJ6tIsuVV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cBgEymJ8suX2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","\n","# üöÇ TRAINING + EVALUATION LOOP\n","\n","def train_model(train_df, test_df, model_name=\"google/gemma-7b\", use_pooling=True, epochs=3):\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","\n","    train_df, val_df = train_test_split(train_df, test_size=0.1, stratify=train_df['stance'], random_state=42)\n","    train_set = StanceDataset(train_df, tokenizer)\n","    val_set = StanceDataset(val_df, tokenizer)\n","    test_set = StanceDataset(test_df, tokenizer)\n","\n","    loader = lambda ds: DataLoader(ds, batch_size=1, shuffle=True)\n","    train_loader = loader(train_set)\n","    val_loader = loader(val_set)\n","    test_loader = loader(test_set)\n","\n","    model = StanceModelWithDynamicPooling(model_name).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n","    total_steps = len(train_loader) * epochs\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","\n","    history = []\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss = 0\n","\n","        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\"):\n","            batch = {k: v.to(device) for k, v in batch.items()}\n","            optimizer.zero_grad()\n","            out = model(**batch)\n","            out['loss'].backward()\n","            optimizer.step()\n","            scheduler.step()\n","            total_loss += out['loss'].item()\n","\n","        model.eval()\n","        preds, targets = [], []\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                batch = {k: v.to(device) for k, v in batch.items()}\n","                out = model(**batch)\n","                logits = out['logits']\n","                preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n","                targets.extend(batch['labels'].cpu().numpy())\n","\n","        acc = accuracy_score(targets, preds)\n","        f1 = f1_score(targets, preds, average='macro')\n","        p = precision_score(targets, preds, average='macro')\n","        r = recall_score(targets, preds, average='macro')\n","        val_loss = out['loss'].item()\n","\n","        print(f\"Epoch {epoch+1}: Loss={total_loss:.4f} | Val Acc={acc:.4f} | F1={f1:.4f}\")\n","        history.append([epoch+1, total_loss, val_loss, acc, p, r, f1])\n","\n","    df_hist = pd.DataFrame(history, columns=['Epoch', 'Train Loss', 'Val Loss', 'Accuracy', 'Precision', 'Recall', 'F1'])\n","    print(\"\\nFinal Epoch-wise Metrics:\")\n","    print(df_hist)\n","\n","    # Metric curves\n","    plt.figure(figsize=(14,6))\n","    for metric in ['Accuracy', 'F1', 'Precision', 'Recall']:\n","        plt.plot(df_hist['Epoch'], df_hist[metric], label=metric)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Score')\n","    plt.title('Validation Metrics Over Epochs')\n","    plt.legend()\n","    plt.grid(True, alpha=0.3)\n","    plt.show()\n","\n","    # Loss curve\n","    plt.figure(figsize=(10,4))\n","    plt.plot(df_hist['Epoch'], df_hist['Train Loss'], label='Train Loss')\n","    plt.plot(df_hist['Epoch'], df_hist['Val Loss'], label='Validation Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title('Training & Validation Loss')\n","    plt.legend()\n","    plt.grid(True, alpha=0.3)\n","    plt.show()\n","\n","    return df_hist\n","\n","\n","# ‚ñ∂Ô∏è RUN EVERYTHING\n","train_model(train_df, test_df, model_name=\"google/gemma-7b\", use_pooling=True, epochs=3)\n"],"metadata":{"id":"SRIndxTeshas"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"112PH1ANm06al9zo1rNrC-ZICnb1m5mqG","timestamp":1744203520338}],"authorship_tag":"ABX9TyP5hc3s698f1kPaR8E0Vhxf"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}