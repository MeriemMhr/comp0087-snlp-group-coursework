{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9WCYLo914AM"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZZ5K8oNoH70h",
    "outputId": "e22244ff-bf8c-4d3a-ddaa-082500737331"
   },
   "outputs": [],
   "source": [
    "!pip install peft --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CkRtn2YHQBS",
    "outputId": "84cc3189-fefa-4cec-9277-16d00a360267"
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers datasets pandas bitsandbytes accelerate evaluate scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5jZKAP40NYx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import display\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from datasets import Dataset, concatenate_datasets, DatasetDict\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator\n",
    ")\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CiN55Oma0Q-S",
    "outputId": "d22143dc-5317-4a40-cad0-d8658e1e66c0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jegqt5Cy2dLx",
    "outputId": "a68f2a42-4f43-4e1f-a266-7985e7a2cc48"
   },
   "outputs": [],
   "source": [
    "output_dir = \"/content/drive/MyDrive/estqa_model_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "model_save_path = os.path.join(output_dir, f\"{model_name.split('/')[-1]}_qa_model\")\n",
    "\n",
    "print(f\"Model will be saved to: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "7f99f3ede131460bb451a79a257b13fe",
      "17b7badb296c49059dbe7f674664fccc",
      "61c753b6d0884d79879bebe52dfd3ceb",
      "a1478254fcd24142bf7941e34dc6235e",
      "ab32fbb64416495182c54fd4de8f4262",
      "68b466eaa7cc4d6d937993ca41413452",
      "1b74e22a2705477ca5e74c4b48e053a5",
      "42d129c0ea5c4eaa89b852ff1d17aa67",
      "7c1a239d325e4e0da5b0d6a6ac69d339",
      "8a0abd7fa63641fd9d573901bf8a6f93",
      "92dfab6a73cd4b4199eaaf508c1eb2d7",
      "cf784da408ec46d2bed8ffbee381e9d9",
      "5a23525f96cf4647ba9ee168bd4d9c2c",
      "b3bb2ddc8cc545b0aaf9aef231a23d94",
      "f6b06d5de8ae4027b2e9994d70541839",
      "93a5c954d3c342b7b673df98ea5db98f",
      "fc21d1e28a5b4026b1e9c1213e0587b3",
      "2d17c351138645bdadaf728cab81a2ec",
      "393531ec79694509901d0a3c44e0c995",
      "c8f8cb8211374525853339688953f41c",
      "421607e689f945bd90f0eb08e71d59f3",
      "1193a4d053dd4a8ea945fb548b4c35d5"
     ]
    },
    "id": "d2vqSZtJHU2W",
    "outputId": "e5271153-eb38-457e-d72d-77d9d21bb7c2"
   },
   "outputs": [],
   "source": [
    "# Load the full dataset (train & test splits)\n",
    "data_path = \"/content/drive/MyDrive/SNLP Group Project/Datasets/EstQA\"\n",
    "\n",
    "# Assuming your JSON files are named 'train.json', 'test.json', etc.\n",
    "train_file = os.path.join(data_path, 'EstQA-train-v1.0.json')\n",
    "test_file = os.path.join(data_path, 'EstQA-test-v1.0.json')\n",
    "\n",
    "# Load the dataset with automatic schema inference\n",
    "dataset = load_dataset('json', data_files={'train': [train_file],\n",
    "                                          'test': test_file})\n",
    "\n",
    "print(\"EstQA Loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOHNQcQdcZLz"
   },
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "voUAmWx2H1Km",
    "outputId": "2f727077-5cb0-4514-a5d4-ba33844f3ede"
   },
   "outputs": [],
   "source": [
    "# General info about the dataset\n",
    "print(dataset)\n",
    "\n",
    "# Print a sample from the training set\n",
    "print(\"\\nTraining Sample:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BK546OjOH_v5",
    "outputId": "e5e0d94d-6e66-4fb7-d9f2-2d24ba393515"
   },
   "outputs": [],
   "source": [
    "# Show 3 random samples from the training set\n",
    "from random import sample\n",
    "\n",
    "for i in sample(range(len(dataset['train'])), 3):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(dataset['train'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eGpQaHT4vTl"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MGDpr8I86U8S",
    "outputId": "68f79536-1b22-4220-9fbf-a34e7ebd602b"
   },
   "outputs": [],
   "source": [
    "# Train/val split\n",
    "train_val_split = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "dataset['train'] = train_val_split['train']\n",
    "dataset['validation'] = train_val_split['test']\n",
    "\n",
    "print(\"Validation set created.\\n\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DKM1tp8h1OOq",
    "outputId": "be0ab9f2-3d5d-4df5-cb59-f668837eca56"
   },
   "outputs": [],
   "source": [
    "# Convert data to DataFrames\n",
    "df_train = pd.DataFrame(dataset['train'])\n",
    "df_val = pd.DataFrame(dataset['validation'])\n",
    "df_test = pd.DataFrame(dataset['test'])\n",
    "\n",
    "print(\"Converted data to DataFrames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T4wL16tZOVph",
    "outputId": "c8943373-d5d5-4c7e-d3e2-c82ad4438f59"
   },
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yX9pVDCiOVph",
    "outputId": "8ba3aa78-12e6-4731-b817-b0f0779cc490"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Flatten and expand dataframes for the EstQA dataset including all fields\n",
    "def flatten_and_expand_estqa(df_expanded):\n",
    "    rows = []\n",
    "    for i, row in df_expanded.iterrows():\n",
    "        # Extract main fields from each row\n",
    "        context = row.get('context')\n",
    "        question = row.get('question')\n",
    "        q_id = row.get('id')\n",
    "        title = row.get('title')\n",
    "\n",
    "        # Get lists of answer starting positions and answer texts from the nested 'answers' field\n",
    "        answer_starts = row.get('answers.answer_start', [])\n",
    "        answer_texts = row.get('answers.text', [])\n",
    "\n",
    "        # Iterate over both lists simultaneously using zip() to create one row per answer.\n",
    "        for answer_start, answer_text in zip(answer_starts, answer_texts):\n",
    "            rows.append({\n",
    "                'id': q_id,\n",
    "                'question': question,\n",
    "                'context': context,\n",
    "                'answer': answer_text,\n",
    "                'answer_start': answer_start,\n",
    "                'title': title\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Expand the 'data' column using pd.json_normalize for train, validation, and test sets.\n",
    "df_train = pd.json_normalize(df_train['data'])\n",
    "df_val   = pd.json_normalize(df_val['data'])\n",
    "df_test  = pd.json_normalize(df_test['data'])\n",
    "\n",
    "# Process the data using the updated flatten_and_expand_estqa function to include all fields\n",
    "df_train = flatten_and_expand_estqa(df_train)\n",
    "df_val   = flatten_and_expand_estqa(df_val)\n",
    "df_test  = flatten_and_expand_estqa(df_test)\n",
    "\n",
    "print(\"Data loaded and flattened.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZdtZzqOmOVph",
    "outputId": "2b67ed99-d358-40e9-aaef-7582b439d8e6"
   },
   "outputs": [],
   "source": [
    "df_train.info()\n",
    "display(df_train.head())\n",
    "display(df_train['question'].value_counts())\n",
    "display(df_train['answer'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pxryhHG5-eMW",
    "outputId": "27e6eb9d-912d-4f9a-8276-65b5f86444cd"
   },
   "outputs": [],
   "source": [
    "# Check for missing answers in the 'answer' column for all splits\n",
    "\n",
    "# For the train split\n",
    "na_train = df_train[df_train['answer'].isna()]\n",
    "if not na_train.empty:\n",
    "    print(\"Train: Missing answers found:\")\n",
    "    print(na_train)\n",
    "else:\n",
    "    print(\"Train: No missing answers found.\")\n",
    "\n",
    "# For the validation split\n",
    "na_val = df_val[df_val['answer'].isna()]\n",
    "if not na_val.empty:\n",
    "    print(\"Validation: Missing answers found:\")\n",
    "    print(na_val)\n",
    "else:\n",
    "    print(\"Validation: No missing answers found.\")\n",
    "\n",
    "# For the test split\n",
    "na_test = df_test[df_test['answer'].isna()]\n",
    "if not na_test.empty:\n",
    "    print(\"Test: Missing answers found:\")\n",
    "    print(na_test)\n",
    "else:\n",
    "    print(\"Test: No missing answers found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zpLbGXAUddFq",
    "outputId": "1ad5c16f-d9e3-474e-b568-87c9a8c59b18"
   },
   "outputs": [],
   "source": [
    "# Calculate length features for training data\n",
    "df_train['context_len'] = df_train['context'].apply(len)\n",
    "df_train['question_len'] = df_train['question'].apply(len)\n",
    "df_train['answer_len'] = df_train['answer'].apply(len)\n",
    "\n",
    "print(\"Training Data Length Features:\")\n",
    "print(df_train[['context_len', 'question_len', 'answer_len']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38CpuLqQyYsZ",
    "outputId": "8f613940-226e-4380-f24f-986fa3cc19a0"
   },
   "outputs": [],
   "source": [
    "# Display random examples from test set\n",
    "num_examples = 4\n",
    "top_answers = df_test['answer'].value_counts().head(num_examples).index.tolist()\n",
    "\n",
    "print(\"--- Random Examples from Test Data ---\\n\")\n",
    "for ans in top_answers:\n",
    "    subset = df_test[df_test['answer'] == ans]\n",
    "\n",
    "    if len(subset) > 0:\n",
    "        random_index = random.randint(0, len(subset) - 1)\n",
    "        example = subset.iloc[random_index]\n",
    "\n",
    "        print(f\"Context (first 150 chars): {example['context'][:150]}...\")\n",
    "        print(f\"Question: {example['question']}\")\n",
    "        print(f\"Answer: {example['answer']}\")\n",
    "        print(f\"Title: {example['title']}\")\n",
    "        print(f\"ID: {example['id']}\")\n",
    "        print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmxtGM8L_9t0"
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FlJdzrTxZzwt",
    "outputId": "ca905e25-d6f0-4bf6-8243-900c9cf62183"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login\n",
    "\n",
    "\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fXmaQUtRN1qh",
    "outputId": "f67a815e-7e82-4296-da00-d1983ad665cc"
   },
   "outputs": [],
   "source": [
    "# Check dataset size\n",
    "print(\"Dataset sizes:\")\n",
    "print(\"Train set:\", len(df_train))\n",
    "print(\"Validation set:\", len(df_val))\n",
    "print(\"Test set:\", len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "7f0d5bab976c402a962705fb35fc3354",
      "b83db09d6d894b82b2b358cb2dbaf204",
      "11c6ff7f5d8a44e7901d961b62a2eaf2",
      "a8c4914e77da43f893b4dfc3c74a386b",
      "0f4c2e9640ce4cfcbe7300716c887c36",
      "a50c63dac92f4bf4a36082b941a406d4",
      "3ed6e792aebf4edd96e44bf9af7293a8",
      "2b16e6ef0edf4ed0a0f5aa13d8e8e4cf",
      "9e066bef49ee4448a72e84ce5293cc5d",
      "aa7526722512460dbe776e54ea2c322c",
      "9bae1c099b5c41a0819c802257ab5728",
      "a9fb573962ba4d2abbd218e84b6012a4",
      "ec9a0df36824439488dfc2ff4a199846",
      "cd2eec4018ed4337a5bd223fa9e79adc",
      "00b59c8d5c094e8bbfd0f3d86d6c0b91",
      "a956e0d1ac2443f685570a645faac36c",
      "ab26af8aacf64aceb91abdb6a4085aeb",
      "c1a2b0e94ad34e0198e65f9fcdf6a16e",
      "4ee51ec446d14716832967f820d9588a",
      "8b2520b5af554161a656c5a8629e2e8a",
      "425ee6954b614056a360be36abc33b26",
      "1f4a25d186704326a58e80f9535362f1",
      "6b10a14f2fd74256a68564b6108344b7",
      "dd22e8b5db5d4a47abcf33234d7a2115",
      "3f920e163d134de6a8ac69845ea0094f",
      "334712d8d0f8445b9fe163c7c29379c0",
      "14cab3c0877d44b3841a2f7f7da8ae1c",
      "03e980ae8f7841d2aafd87345f059243",
      "58abb0c3a5bb4d17b711d231bf610642",
      "c4b9adad875e4098935983d7fb5fbda9",
      "1e01514e673d447cbe02bde0e7fa27a7",
      "67f12ea391f6403e990ad453791fed40",
      "645b1e24db694aedac392da1bd62efc2",
      "4289413c2ab44e8c8d993f1e264e1821",
      "4c72e3c9196146f4bf5b4b132a8a7227",
      "5766de11d8b048e4be3e64c48c303ff1",
      "f0404e0118ad473ba7d50d50c8e4f20a",
      "6f8f172810a641a2ab387ba423d3eb7b",
      "015cebb0b474485c916b4fbeaf9b7858",
      "d47b62b7da3d4b4b97d642a2f389ac82",
      "e7e6f15f99a843acae7010f7e0cad61a",
      "d83547c68ccd40188d9621655083da3c",
      "66a3bbcf3627459185e8a7f875e79d26",
      "82e6f763333a4af98f5e5fe15a622b4b",
      "2b1451765137418b851083fcc526b90a",
      "7573eedb2fc9401992a7c26d0591195b",
      "680019ad299d45e2bd9b52d2c349ca8b",
      "812f983bfd3349b6a25499b51ee7a19c",
      "04ad865a668a4542b765cc8bef697384",
      "96d19b93e9234c6e9765c24c5e3646f0",
      "07729709946a489997d8f5a46302f8a6",
      "7731a4836c664cf9841f4a6b9632e72a",
      "01d1b7e7c64f488c9ea50f6dae74396d",
      "2413159c899d4c24a67ea3ef1a8a3798",
      "6349f3badc34412cae55abd382632e5d",
      "3c005040f01642638de23a9da7c22c2f",
      "69ce44327e074bf48822f18cc857589b",
      "ddc1749676e446148877449887bb4835",
      "ec8b919fa3dc4973a733c713656de23b",
      "d1d2b8b2455d4519976888b643b1a95d",
      "e16e660931784e8692f8d94980c573f1",
      "c3ee2abd8a524221b35f46ec6a21623a",
      "7ca66b5d4a9647819b0788d6e72d0213",
      "706917c741f04d9e99d7d412b3f92374",
      "c40c94638a134cd996971ad76840081c",
      "bec567fd854c42479ddd0c59bdf035e7",
      "d6db0dd7a3fd4ed599d0aa82902f513c",
      "4d3715ab95714757b5de72c59c1b7001",
      "e7aae24cca3c49f59f72355de6876636",
      "af496da88c5c4fc19d1521f4a0c90e06",
      "f90cf5f2da754476919c483ce2d258ec",
      "2db4123499404fdd9ba763fbd7cff130",
      "38fb66dc48934fcbabdf4e5e4dd635f8",
      "79922b0cb3ce4775925d6636019d5193",
      "2e7133980c6144c08d0e3380e8580461",
      "0ae399eb42bc4a80b67fb158711d4036",
      "5e2df7612ca54f5280c7b1414f9817f7",
      "a8df0f9d2b3a43fba09c5607a55110cc",
      "b0912494387a40e983f63bb856519bde",
      "5bc79efddb2d43dda4ccfa45d7a9ec0f",
      "c0b49c336f564c1eace5819bfb4509df",
      "e679269b59a04a5bbe5f0267dcf9ca5e",
      "0984a81e8b07475280c3bf96c705fa60",
      "6ee3625bc8f14e47b0ba2400158709ea",
      "07df636869cf4997a6f13a6d3985cf5e",
      "a2ec24882dce4ddd800162d1d47fac09",
      "9e2ae9082f91420099d034543643cacf",
      "39dbfa3d9b4e4044a5c0cdd335084612",
      "7d43f4be40624c98b68ae65f368fb26c",
      "f895d548c7c849f0bbf2af2db5a4d417",
      "883e79464c1040f18b0aed4632cb6844",
      "a2454f207b564152b888faef72b91275",
      "bbeb54fc5c3f4d2080d13b2a9641766c",
      "7d0925f23cb6487ba4fd1c08ddbdaffc",
      "0ff7892132f942929e87bf4e4064e975",
      "1827797ed8b6468795256e0fbe14a346",
      "a45b3518ca9e4346bd3e014653d3ee4e",
      "9ba775a3297041ff9c669821f6ea9815",
      "f0d597681382491281bfcda76b358492",
      "35b76609190345e0afa8dcf403d4112a",
      "6bb47bb60eeb432a8ab284af0a33f9f6",
      "851f95ef3c0c43e195ae0557b2f05b35",
      "e7719014d4f54d51930513021d47dc97",
      "d25939c22e42485d9bb93973954f122d",
      "2917d311380247e5b4bdc3b6dd36dd83",
      "6d838ef56c28412e809b48ba980af884",
      "c46f0d96d53845a1a91c950b6201ee8f",
      "657d3d02352b41a4b1a605b31f7119b7",
      "4b8bcd3d401d41e2aea4de4f7177cc2e",
      "4085975ca05e4c7db605369d4899a384",
      "dde3c3406a3d4200ad95f565de37a306",
      "351de3ed612a44e4981a45f237dba3bf",
      "f2277311b99d4282bea4ac3bdce265cc",
      "417262c804cb452791ff8efbc536b264",
      "e0deefe6a472459ca4e7d16431977527",
      "806d8e3cb69d478baab8f06650243d29",
      "334fd13cedd24758a58ae08e34008019",
      "37e800caa00c4239a1e5ecbef56cb61b",
      "26495fcb575941dc92b6ed5859840d52",
      "9b47d99b015a4e6499d591c99b0d5cc4",
      "783539cd6d764af28ec1b4a17bb2b1d4",
      "e4680d0e0cb645f28ac935b565a50243",
      "1873142cc0e84365957467fdc2e3b5e2",
      "3c5f17db2f9946aa93155e24515429e4",
      "125bfdb6227c4adc9224bd3fdbf1d131",
      "b2346205ac834dd2b33daeb3e972569e",
      "d8dc09186f684940aa1399385b31a9fa",
      "5771abe749d64ffeb97ada8a0bb1d397",
      "f78d0fb870ba405e8a3bb5e335179d21",
      "d15a674731e347f49a43491df07d8eed",
      "f1bd9262588f421499c76f7b479f6536",
      "b81d59cee3124a41870ce2c32344868c"
     ]
    },
    "id": "K8qoD708aduJ",
    "outputId": "06f86fca-c08c-4090-af08-a59f5510d459"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import html\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# Import PEFT classes for LoRA support\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig\n",
    "\n",
    "# -------------------------------\n",
    "# QADataset and qa_collate_fn definitions for estQA\n",
    "# -------------------------------\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Expects estQA dataframe columns: 'question', 'context', 'answer', 'answer_start', and 'title'\n",
    "        \"\"\"\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        question = html.unescape(row['question'])\n",
    "        context = html.unescape(row['context'])\n",
    "        answer_text = html.unescape(row['answer'])\n",
    "        # Use the given answer_start value or fallback to searching the answer in context\n",
    "        answer_start = row['answer_start'] if 'answer_start' in row and not pd.isna(row['answer_start']) else context.find(answer_text)\n",
    "        if answer_start == -1:\n",
    "            answer_start = 0\n",
    "            answer_end = 0\n",
    "        else:\n",
    "            answer_end = answer_start + len(answer_text)\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            question,\n",
    "            context,\n",
    "            truncation=\"only_second\",\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        offset_mapping = encoding.pop(\"offset_mapping\")\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        offset_mapping = offset_mapping.squeeze(0)\n",
    "\n",
    "        token_type_ids = encoding.get(\"token_type_ids\", None)\n",
    "        start_pos, end_pos = None, None\n",
    "        for idx_tok, (start_char, end_char) in enumerate(offset_mapping):\n",
    "            start_char = int(start_char)\n",
    "            end_char = int(end_char)\n",
    "            # Focus on the context portion (second sequence)\n",
    "            if token_type_ids is None or token_type_ids[idx_tok] == 1:\n",
    "                if start_pos is None and start_char <= answer_start < end_char:\n",
    "                    start_pos = idx_tok\n",
    "                if start_pos is not None and end_char >= answer_end:\n",
    "                    end_pos = idx_tok\n",
    "                    break\n",
    "        if start_pos is None or end_pos is None:\n",
    "            # Use -100 for ignored tokens when answer span not found\n",
    "            start_pos, end_pos = -100, -100\n",
    "\n",
    "        encoding[\"start_positions\"] = torch.tensor(start_pos, dtype=torch.long)\n",
    "        encoding[\"end_positions\"] = torch.tensor(end_pos, dtype=torch.long)\n",
    "        encoding[\"context\"] = context\n",
    "        encoding[\"question\"] = question\n",
    "        encoding[\"answer_text\"] = answer_text\n",
    "        encoding[\"offset_mapping\"] = offset_mapping.tolist()\n",
    "        return encoding\n",
    "\n",
    "def qa_collate_fn(batch):\n",
    "    collated = {}\n",
    "    for key in ['input_ids', 'attention_mask', 'start_positions', 'end_positions', 'token_type_ids']:\n",
    "        if key in batch[0]:\n",
    "            collated[key] = torch.stack([sample[key] for sample in batch])\n",
    "    collated['context'] = [sample['context'] for sample in batch]\n",
    "    collated['question'] = [sample['question'] for sample in batch]\n",
    "    collated['answer_text'] = [sample['answer_text'] for sample in batch]\n",
    "    collated['offset_mapping'] = [sample['offset_mapping'] for sample in batch]\n",
    "    return collated\n",
    "\n",
    "# -------------------------------\n",
    "# QA Tokenization Analysis on the Full estQA Dataset (Train+Val+Test)\n",
    "# -------------------------------\n",
    "def analyze_tokenization_qa_full(df_train, df_val, df_test):\n",
    "    \"\"\"\n",
    "    Analyze tokenization for QA on the entire estQA dataset (train, validation, test).\n",
    "    Computes metrics and produces visualization plots:\n",
    "      - Scatter plot: Token count vs. Text Length\n",
    "      - Histogram: Distribution of Token Fertility\n",
    "      - Bar plot: Token Length Distribution\n",
    "      - Histogram: Compression Ratio Distribution (derived from token fertility)\n",
    "    \"\"\"\n",
    "    print(\"Loading tokenizer for QA tokenization analysis on the full estQA dataset...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\", use_fast=True)\n",
    "    combined_df = pd.concat([df_train, df_val, df_test])\n",
    "    print(f\"Analyzing tokenization on {len(combined_df)} samples...\")\n",
    "\n",
    "    results = []\n",
    "    total_chars = 0\n",
    "    total_tokens = 0\n",
    "    token_lengths = []\n",
    "    tokens_per_char_values = []\n",
    "    oov_count = 0\n",
    "    unique_tokens = set()\n",
    "    vocabulary_size = len(tokenizer.get_vocab())\n",
    "    highly_fragmented_words = []\n",
    "    words_analyzed = 0\n",
    "\n",
    "    for _, row in combined_df.iterrows():\n",
    "        question = html.unescape(row['question'])\n",
    "        # Use a truncated version of the context for speed (adjust if needed)\n",
    "        context = html.unescape(row['context'][:500])\n",
    "        q_chars = len(question)\n",
    "        q_tokens = tokenizer.encode(question, add_special_tokens=False)\n",
    "        q_tokens_texts = tokenizer.convert_ids_to_tokens(q_tokens)\n",
    "\n",
    "        # Token fertility: tokens per character in question\n",
    "        q_fertility = len(q_tokens) / q_chars if q_chars > 0 else 0\n",
    "        tokens_per_char_values.append(q_fertility)\n",
    "\n",
    "        for token in q_tokens_texts:\n",
    "            token_lengths.append(len(token))\n",
    "            unique_tokens.add(token)\n",
    "\n",
    "        total_chars += q_chars\n",
    "        total_tokens += len(q_tokens)\n",
    "\n",
    "        # OOV estimation: count words that get split into 3 or more tokens\n",
    "        for word in question.split():\n",
    "            if len(word) >= 3:\n",
    "                words_analyzed += 1\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                if len(word_tokens) >= 3:\n",
    "                    highly_fragmented_words.append(word)\n",
    "                    oov_count += 1\n",
    "\n",
    "        results.append({\n",
    "            'text_length': q_chars + len(context),\n",
    "            'llama_tokens': len(q_tokens) + len(tokenizer.encode(context, add_special_tokens=False)),\n",
    "            'fertility': q_fertility\n",
    "        })\n",
    "\n",
    "    token_df = pd.DataFrame(results)\n",
    "    avg_token_fertility = np.mean(tokens_per_char_values)\n",
    "    token_length_distribution = Counter(token_lengths)\n",
    "    avg_token_length = np.mean(token_lengths)\n",
    "    median_token_length = np.median(token_lengths)\n",
    "    compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0\n",
    "    vocabulary_coverage = len(unique_tokens) / vocabulary_size\n",
    "    oov_rate = oov_count / words_analyzed if words_analyzed > 0 else 0\n",
    "\n",
    "    print(\"\\n===== Tokenization Analysis Results =====\")\n",
    "    print(f\"1. Token Fertility (tokens/char): {avg_token_fertility:.4f}\")\n",
    "    print(f\"2. Token Length: Mean = {avg_token_length:.2f}, Median = {median_token_length:.2f}\")\n",
    "    print(f\"3. Compression Ratio (chars/token): {compression_ratio:.4f}\")\n",
    "    print(f\"4. Vocabulary: Used {len(unique_tokens)} of {vocabulary_size} tokens ({vocabulary_coverage:.2%})\")\n",
    "    print(f\"5. OOV Rate: {oov_rate:.4f} ({oov_count}/{words_analyzed} words)\")\n",
    "\n",
    "    if highly_fragmented_words:\n",
    "        print(\"\\nExample highly fragmented words (potential OOVs):\")\n",
    "        sample_oov = random.sample(highly_fragmented_words, min(10, len(highly_fragmented_words)))\n",
    "        for word in sample_oov:\n",
    "            tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "            token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
    "            print(f\"  '{word}' â†’ {len(tokens)} tokens: {token_texts}\")\n",
    "\n",
    "    # Visualize tokenization with 4 subplots.\n",
    "    plt.figure(figsize=(16, 12))\n",
    "\n",
    "    # Plot 1: Scatter plot of Token Count vs Text Length\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(token_df['text_length'], token_df['llama_tokens'], alpha=0.7)\n",
    "    plt.xlabel('Text Length (characters)')\n",
    "    plt.ylabel('Llama Token Count')\n",
    "    plt.title('Token Count vs Text Length')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Histogram of Token Fertility\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.hist(tokens_per_char_values, bins=20)\n",
    "    plt.xlabel('Token Fertility (tokens/char)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Token Fertility')\n",
    "\n",
    "    # Plot 3: Bar plot of Token Length Distribution\n",
    "    plt.subplot(2, 2, 3)\n",
    "    lengths = list(token_length_distribution.keys())\n",
    "    frequencies = list(token_length_distribution.values())\n",
    "    plt.bar(lengths, frequencies)\n",
    "    plt.xlabel('Token Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Token Length Distribution')\n",
    "\n",
    "    # Plot 4: Histogram of Compression Ratio Distribution (chars/token)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    compression_values = [1/f if f > 0 else 0 for f in tokens_per_char_values]\n",
    "    plt.hist(compression_values, bins=20)\n",
    "    plt.xlabel('Compression Ratio (chars/token)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Compression Ratio Distribution')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tokenization_analysis.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the computed metrics to a JSON file.\n",
    "    tokenization_metrics = {\n",
    "        'token_fertility': avg_token_fertility,\n",
    "        'token_length_mean': avg_token_length,\n",
    "        'token_length_median': median_token_length,\n",
    "        'compression_ratio': compression_ratio,\n",
    "        'vocabulary_size': vocabulary_size,\n",
    "        'vocabulary_coverage': vocabulary_coverage,\n",
    "        'oov_rate': oov_rate,\n",
    "        'token_length_distribution': dict(token_length_distribution)\n",
    "    }\n",
    "    with open('qa_tokenization_metrics_full.json', 'w') as f:\n",
    "        json.dump(tokenization_metrics, f, indent=2)\n",
    "\n",
    "    return token_df\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluate QA Model Function\n",
    "# -------------------------------\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
    "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
    "\n",
    "def compute_exact(a_pred, a_gold):\n",
    "    return int(normalize_answer(a_pred) == normalize_answer(a_gold))\n",
    "\n",
    "def compute_f1(a_pred, a_gold):\n",
    "    pred_tokens = normalize_answer(a_pred).split()\n",
    "    gold_tokens = normalize_answer(a_gold).split()\n",
    "    common = Counter(pred_tokens) & Counter(gold_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "        return int(pred_tokens == gold_tokens)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gold_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def evaluate_qa_model(model, data_loader, tokenizer, device, verbose=True):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    all_em = []\n",
    "    all_f1 = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch_tensors = {k: v.to(device) for k, v in batch.items() if k in\n",
    "                             ['input_ids', 'attention_mask', 'start_positions', 'end_positions', 'token_type_ids']}\n",
    "            outputs = model(**batch_tensors)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            start_logits = outputs.start_logits\n",
    "            end_logits = outputs.end_logits\n",
    "            batch_size = start_logits.size(0)\n",
    "            for i in range(batch_size):\n",
    "                # Obtain predictions by using argmax on logits\n",
    "                input_ids = batch['input_ids'][i]\n",
    "                offsets = batch['offset_mapping'][i]\n",
    "                start_idx = torch.argmax(start_logits[i]).item()\n",
    "                end_idx = torch.argmax(end_logits[i]).item()\n",
    "                if end_idx < start_idx:\n",
    "                    pred_answer = \"\"\n",
    "                else:\n",
    "                    context = batch['context'][i]\n",
    "                    char_start = offsets[start_idx][0]\n",
    "                    char_end = offsets[end_idx][1]\n",
    "                    pred_answer = context[char_start:char_end]\n",
    "                true_answer = batch['answer_text'][i]\n",
    "                all_em.append(compute_exact(pred_answer, true_answer))\n",
    "                all_f1.append(compute_f1(pred_answer, true_answer))\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    avg_em = np.mean(all_em) if all_em else 0\n",
    "    avg_f1 = np.mean(all_f1) if all_f1 else 0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"  Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  EM: {avg_em:.4f}\")\n",
    "        print(f\"  F1: {avg_f1:.4f}\")\n",
    "    return {'loss': avg_loss, 'exact_match': avg_em, 'f1': avg_f1}\n",
    "\n",
    "# -------------------------------\n",
    "# Training Function for QA with LoRA Integration on estQA\n",
    "# -------------------------------\n",
    "def train_model_qa(model_name, df_train, df_val, df_test, output_dir, use_lora=False, epochs=10):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    batch_size = 4\n",
    "    grad_accum_steps = 8\n",
    "    learning_rate = 2e-5\n",
    "    max_length = 512\n",
    "    model_save_path = os.path.join(output_dir, f\"{model_name.split('/')[-1]}_qa_model\")\n",
    "\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Training {model_name} model for Question Answering on estQA\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"Set EOS token as padding token\")\n",
    "\n",
    "    print(\"Preparing QA datasets...\")\n",
    "    train_dataset = QADataset(df_train, tokenizer, max_length)\n",
    "    val_dataset = QADataset(df_val, tokenizer, max_length)\n",
    "    test_dataset = QADataset(df_test, tokenizer, max_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=qa_collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=qa_collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=qa_collate_fn)\n",
    "\n",
    "    # === Sanity Check: Count fallback occurrences in the estQA dataset ===\n",
    "    fallback_count = 0\n",
    "    total_samples = len(train_dataset)\n",
    "    fallback_samples = []  # For possible inspection\n",
    "\n",
    "    for i in range(total_samples):\n",
    "        sample = train_dataset[i]\n",
    "        if sample[\"start_positions\"].item() == 0 and sample[\"end_positions\"].item() == 0:\n",
    "            fallback_count += 1\n",
    "            fallback_samples.append((df_train.iloc[i]['context'], df_train.iloc[i]['answer']))\n",
    "\n",
    "    print(f\"Fallback occurrences: {fallback_count} out of {total_samples} samples\")\n",
    "    print(f\"Percentage of fallbacks: {100 * fallback_count / total_samples:.2f}%\")\n",
    "\n",
    "    print(\"Initializing QA model...\")\n",
    "    if use_lora:\n",
    "        base_model = AutoModelForQuestionAnswering.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "        if base_model.config.pad_token_id is None:\n",
    "            base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        peft_config = LoraConfig(\n",
    "            task_type='QUESTION_ANS',\n",
    "            inference_mode=False,\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "        )\n",
    "        model = get_peft_model(base_model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "    else:\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "        if model.config.pad_token_id is None:\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_loader) * epochs // grad_accum_steps\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=total_steps // 10, num_training_steps=total_steps)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_f1 = 0\n",
    "    val_em_list = []\n",
    "    val_f1_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch_tensors = {k: v.to(device) for k, v in batch.items()\n",
    "                             if k in ['input_ids', 'attention_mask', 'start_positions', 'end_positions', 'token_type_ids']}\n",
    "            outputs = model(**batch_tensors)\n",
    "            loss = outputs.loss / grad_accum_steps\n",
    "            loss.backward()\n",
    "            epoch_train_loss += loss.item() * grad_accum_steps\n",
    "            if (step + 1) % grad_accum_steps == 0 or (step + 1) == len(train_loader):\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch_tensors = {k: v.to(device) for k, v in batch.items()\n",
    "                                 if k in ['input_ids', 'attention_mask', 'start_positions', 'end_positions', 'token_type_ids']}\n",
    "                outputs = model(**batch_tensors)\n",
    "                if outputs.loss is not None:\n",
    "                    epoch_val_loss += outputs.loss.item()\n",
    "        avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        eval_results = evaluate_qa_model(model, val_loader, tokenizer, device, verbose=False)\n",
    "        print(f\"  Val EM: {eval_results['exact_match']:.4f}\")\n",
    "        print(f\"  Val F1: {eval_results['f1']:.4f}\")\n",
    "\n",
    "        val_em_list.append(eval_results['exact_match'])\n",
    "        val_f1_list.append(eval_results['f1'])\n",
    "\n",
    "        if eval_results['f1'] > best_val_f1:\n",
    "            best_val_f1 = eval_results['f1']\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            model_to_save.save_pretrained(model_save_path)\n",
    "            tokenizer.save_pretrained(model_save_path)\n",
    "            print(f\"  Model saved to {model_save_path} with F1: {best_val_f1:.4f}\")\n",
    "\n",
    "    # Plot training and evaluation metrics after training\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, 'b-', label='Train Loss')\n",
    "    plt.plot(val_losses, 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_em_list, 'g-', label='Validation EM')\n",
    "    plt.plot(val_f1_list, 'm-', label='Validation F1')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Validation Evaluation Metrics')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{model_name.split(\"/\")[-1]}_evaluation_metrics.png'))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    if use_lora:\n",
    "        # Load LoRA adapter configuration from the saved model path\n",
    "        config = PeftConfig.from_pretrained(model_save_path, local_files_only=True)\n",
    "        base_model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "            config.base_model_name_or_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        if base_model.config.pad_token_id is None:\n",
    "            base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        best_model = PeftModel.from_pretrained(base_model, model_save_path, local_files_only=True)\n",
    "    else:\n",
    "        best_model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "            model_save_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        if best_model.config.pad_token_id is None:\n",
    "            best_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "    test_results = evaluate_qa_model(best_model, test_loader, tokenizer, device, verbose=True)\n",
    "    with open(os.path.join(output_dir, f'{model_name.split(\"/\")[-1]}_test_results.json'), 'w') as f:\n",
    "        json.dump(test_results, f, indent=2)\n",
    "    return test_results\n",
    "\n",
    "# -------------------------------\n",
    "# Pipeline Function for QA with LoRA Option on estQA\n",
    "# -------------------------------\n",
    "def run_pipeline_qa(df_train, df_val, df_test, run_tokenization=True, epochs=10, use_lora=False):\n",
    "    print(f\"Train set: {len(df_train)} samples\")\n",
    "    print(f\"Validation set: {len(df_val)} samples\")\n",
    "    print(f\"Test set: {len(df_test)} samples\")\n",
    "    print(\"\\nSample answer distribution (first 5 samples):\")\n",
    "    print(df_train[['question', 'answer']].head())\n",
    "    if run_tokenization:\n",
    "        print(\"\\n=== Running Tokenization Analysis for QA ===\")\n",
    "        analyze_tokenization_qa_full(df_train, df_val, df_test)\n",
    "    model_name = \"meta-llama/Llama-3.1-8B\"  # Change as required.\n",
    "    results = train_model_qa(model_name, df_train, df_val, df_test, output_dir=\"output\", epochs=epochs, use_lora=use_lora)\n",
    "    return results\n",
    "\n",
    "# -------------------------------\n",
    "# Main\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming df_train, df_val, and df_test are loaded with your estQA data.\n",
    "    results = run_pipeline_qa(df_train, df_val, df_test, run_tokenization=True, epochs=10, use_lora=True)\n",
    "    print(\"\\n=== Final QA Test Results Summary ===\")\n",
    "    print(f\"Loss: {results['loss']:.4f}\")\n",
    "    print(f\"Exact Match (EM): {results['exact_match']:.4f}\")\n",
    "    print(f\"F1 Score: {results['f1']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "lOHNQcQdcZLz"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
