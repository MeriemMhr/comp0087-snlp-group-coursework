{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9WCYLo914AM"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71270,
     "status": "ok",
     "timestamp": 1743951636681,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "ZZ5K8oNoH70h",
    "outputId": "31208998-9287-4025-fa37-4f263b4a04a0"
   },
   "outputs": [],
   "source": [
    "!pip install peft --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6943,
     "status": "ok",
     "timestamp": 1743951643627,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "-CkRtn2YHQBS",
    "outputId": "933cc8b0-97d4-4059-d13a-c22e8a7242b2"
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers datasets pandas bitsandbytes accelerate evaluate scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5jZKAP40NYx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator\n",
    ")\n",
    "import torch\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29440,
     "status": "ok",
     "timestamp": 1743951690339,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "CiN55Oma0Q-S",
    "outputId": "f4c48cb3-1cee-470f-a560-7c0899966c34"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363,
     "referenced_widgets": [
      "e90e746ae1a443fc844e3148b2d66eda",
      "0c8595d5b0fa490db6fc5fb4531073df",
      "9668f388bde74fa8ac00dfbc7d1d4334",
      "c896e4ca728949a79c3dd5aa423c2eff",
      "f549409729f14aaf9323e4959c0c37a6",
      "5b7ad30150a04b5d957650404618553f",
      "15f684aa4fa148b48b2225412dfbee3c",
      "4a7fb1c87e6c4351b58214fe7a5b60d9",
      "2e40025d13774481a91ae160bc5efe11",
      "7fda32778a894b94a0b7e5b4b7f6a98b",
      "32bdd4f99bec47d0b02eb95844556dfe",
      "1f229ab883aa4543aec173536f63ec90",
      "683d10c710234275bb8f462779dfc311",
      "7bad0c2413484f7095effe2e1b425565",
      "c5bf386734bc40aaa0464618b3ab4699",
      "f628e4154f7842dba7e72cbc30cd35ab",
      "18add9a0f3ee44cc8c27d526b8cdb3c1",
      "0099bb60fc4f411b9070ea12b94f71f6",
      "7c0a02abb26c4cfc951149b3db31e2df",
      "99e9fb1da8d04b9b9877ee70b81820bc",
      "00d2f66c16284b06a7a10592cc04c2d8",
      "b7cfacf4e3b94aa396b5c2350378ce1b",
      "7e57d3748f1d47ab8d5a799d22b79190",
      "c0e75a8f5d0b4f5b961af51e4dfa103b",
      "09ae48432f874896b76bc5a5e60dbc6e",
      "bcb8298a5e2c4d689896755428c45435",
      "5f00a444958b4604825263278d8336cc",
      "2d54e1b09dfa4ed5981f40acc7b0213f",
      "836227c4a83747218086640d2044a099",
      "1615955ec2624ef08fc321133ac8a554",
      "99de746f4bfb48dd864609fa1690a88a",
      "8f4bf2fd085d4bfc9431a2766b8ba45b",
      "0f82b2c2cb46436596a5994bf0f3ab5a",
      "7d272cd685304229bcd701d6e208d093",
      "0f9134d125ae469eb1f685b9b4faa4c1",
      "30585a4ebca545f99adf5c3f23aa4bb5",
      "3b96232b0ecf45ca9af5a218da6279e4",
      "968a9f2ab7cc4bef9c2a22ffb0ece433",
      "906dc15dc0d54e65878ce1fc142866f5",
      "0d31dc18acb24d2ca173adfa24de9970",
      "2803e4cb8917475b9284259b78ee6241",
      "c8858ce5b6284462ae3a75e491a3fce8",
      "cee6aaa7b9cc4894b2a575c0fb4deded",
      "ae6b4e14a50041c68cf869ace7080aca",
      "e005056ded984f7ead24beb8b31d8036",
      "fa53422ad458448eaf45c37da5e1a22a",
      "bce0068de26a4c7b829175a1b7deb57d",
      "82f077e6d7434653a9c51cc7c27cbc9b",
      "f6f92d0674434c13a8519ed79f6e9778",
      "41618738fad94276acc736c334be94b7",
      "d19f397d0376443a92b545847b746699",
      "8c12ae561219410093d81049e431a493",
      "fc3424f9683045809f31231148df36b2",
      "8376ffec9fc4404e9d453ce7f703b6b6",
      "dba72f50e3eb47dda35894dc50564929",
      "8be08cb740304d268280a562a375f608",
      "7776faa2dc464502a05bbdb15516ec23",
      "33e314fcf986411fa899bd11fedeee25",
      "efd919335ada4b3f8f419820e22bb53f",
      "e56b786c196a4489b06604b943f7147c",
      "90eccc38022844c0abfab14cdd081b19",
      "3518877dddeb4d86abab24b8f5cf6041",
      "2eb45fcbaaff41f08f2b160ab0d4b22e",
      "eb824b0bcead41c19e839e95d9290f90",
      "41420635ca694589b5ab4602e404fce6",
      "2ed1ed8b1ac0418b83a62d499f7fab9b",
      "606a045343784406a4603dd34f07e3f2",
      "3ed3fcda22e047259e338dc553ea7d06",
      "55cf621c37064187847c15184f149497",
      "3d0e392bc2e7413fadfd569ca0963c01",
      "5f061267cfcd43e588ed09db159681e8",
      "b7ca295c5019417481ff1ee84571fbe2",
      "3c437bfe65474dfebdc8491d739d07c9",
      "e55969161fcb449f9dfee16c770cab4a",
      "7b70408930ec4e79b4d632b152ef2906",
      "76521b797b6546fca72bd46529f58771",
      "7ebfdcc3c4ca4e95b8cbc0b5bdaa561a"
     ]
    },
    "executionInfo": {
     "elapsed": 15552,
     "status": "ok",
     "timestamp": 1743951705894,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "d2vqSZtJHU2W",
    "outputId": "af5b8e16-395b-42c1-92ed-6c2d34c29050"
   },
   "outputs": [],
   "source": [
    "# Load the full dataset (train, validation, test splits)\n",
    "dataset = load_dataset(\"abdoelsayed/ArabicaQA\")\n",
    "print(\"ArabicaQA Loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOHNQcQdcZLz"
   },
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 605,
     "status": "ok",
     "timestamp": 1743883992487,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "voUAmWx2H1Km",
    "outputId": "9e21b743-b092-4071-834f-60e195e08b5a"
   },
   "outputs": [],
   "source": [
    "# General info about the dataset\n",
    "print(dataset)\n",
    "\n",
    "# Print a sample from the training set\n",
    "print(\"\\nTraining Sample:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1743466092854,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "BK546OjOH_v5",
    "outputId": "ba20d3dd-048b-4ffb-c65b-d4428db11b1d"
   },
   "outputs": [],
   "source": [
    "# Show 3 random samples from the training set\n",
    "from random import sample\n",
    "\n",
    "for i in sample(range(len(dataset['train'])), 3):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(dataset['train'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eGpQaHT4vTl"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3529,
     "status": "ok",
     "timestamp": 1743951709425,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "DKM1tp8h1OOq",
    "outputId": "4d28ddaf-0bd7-4784-bafb-d203417aac34"
   },
   "outputs": [],
   "source": [
    "# Convert data to DataFrames\n",
    "df_train = pd.DataFrame(dataset['train'])\n",
    "df_val = pd.DataFrame(dataset['validation'])\n",
    "df_test = pd.DataFrame(dataset['test'])\n",
    "\n",
    "print(\"Converted data to DataFrames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1743446131083,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "VBf_ZUQ61kJx",
    "outputId": "4ca094de-5b16-4933-9aba-a60ef21b598c"
   },
   "outputs": [],
   "source": [
    "# Visualise train dataframe structure\n",
    "df_train.describe()\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2168,
     "status": "ok",
     "timestamp": 1743951711666,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "7WMdUwOTNHFJ",
    "outputId": "45b4d4af-829a-4e50-8328-9f7f12c405f7"
   },
   "outputs": [],
   "source": [
    "# Flatten and expand dataframes\n",
    "def flatten_and_expand(df_expanded):\n",
    "    rows = []\n",
    "    for i, row in df_expanded.iterrows():\n",
    "        for para in row['paragraphs']:\n",
    "            context = para['context']\n",
    "            for qa in para['qas']:\n",
    "                question = qa['question']\n",
    "                q_id = qa['id']\n",
    "                is_impossible = qa.get('is_impossible', False)\n",
    "                answers = qa.get('answers', [])\n",
    "\n",
    "                for ans in answers:\n",
    "                    rows.append({\n",
    "                        'id': q_id,\n",
    "                        'question': question,\n",
    "                        'context': context,\n",
    "                        'answer': ans['text'],\n",
    "                        'answer_start': ans.get('answer_start', None),\n",
    "                        'is_impossible': is_impossible\n",
    "                    })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Expand the 'data' column into multiple columns for each dataframe\n",
    "df_train_expanded = pd.json_normalize(df_train['data'])\n",
    "df_val_expanded = pd.json_normalize(df_val['data'])\n",
    "df_test_expanded = pd.json_normalize(df_test['data'])\n",
    "\n",
    "# Process the training data\n",
    "df_train_qa = flatten_and_expand(df_train_expanded)\n",
    "print(\"Training Data Processed:\")\n",
    "df_train_qa.info()\n",
    "display(df_train_qa.head())\n",
    "display(df_train_qa['question'].value_counts().head(10))\n",
    "display(df_train_qa['answer'].value_counts().head(10))\n",
    "\n",
    "# Process the validation data\n",
    "df_val_qa = flatten_and_expand(df_val_expanded)\n",
    "print(\"\\nValidation Data Processed:\")\n",
    "df_val_qa.info()\n",
    "display(df_val_qa.head())\n",
    "display(df_val_qa['question'].value_counts().head(10))\n",
    "display(df_val_qa['answer'].value_counts().head(10))\n",
    "\n",
    "# Process the test data\n",
    "df_test_qa = flatten_and_expand(df_test_expanded)\n",
    "print(\"\\nTest Data Processed:\")\n",
    "df_test_qa.info()\n",
    "display(df_test_qa.head())\n",
    "display(df_test_qa['question'].value_counts().head(10))\n",
    "display(df_test_qa['answer'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1743951711709,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "6HN7BWuQWhSd",
    "outputId": "50522fcd-c225-4134-f5ee-0044343f45d1"
   },
   "outputs": [],
   "source": [
    "# Filter out NA answers\n",
    "def count_na_answers(df):\n",
    "    na_answers = df['answer'].isin(['.', '. ', '', None]).sum()\n",
    "    return na_answers\n",
    "\n",
    "# Training data\n",
    "na_count_before_train = count_na_answers(df_train_qa)\n",
    "df_train_qa = df_train_qa[~df_train_qa['answer'].isin(['.', '. ', '', None])]\n",
    "na_count_after_train = count_na_answers(df_train_qa)\n",
    "\n",
    "print(\"Training Data NA Answer Counts:\")\n",
    "print(f\"  NA answers before filtering: {na_count_before_train}\")\n",
    "print(f\"  NA answers after filtering: {na_count_after_train}\")\n",
    "\n",
    "# Validation data\n",
    "na_count_before_val = count_na_answers(df_val_qa)\n",
    "df_val_qa = df_val_qa[~df_val_qa['answer'].isin(['.', '. ', '', None])]\n",
    "na_count_after_val = count_na_answers(df_val_qa)\n",
    "\n",
    "print(\"\\nValidation Data NA Answer Counts:\")\n",
    "print(f\"  NA answers before filtering: {na_count_before_val}\")\n",
    "print(f\"  NA answers after filtering: {na_count_after_val}\")\n",
    "\n",
    "# Test data\n",
    "na_count_before_test = count_na_answers(df_test_qa)\n",
    "df_test_qa = df_test_qa[~df_test_qa['answer'].isin(['.', '. ', '', None])]\n",
    "na_count_after_test = count_na_answers(df_test_qa)\n",
    "\n",
    "print(\"\\nTest Data NA Answer Counts:\")\n",
    "print(f\"  NA answers before filtering: {na_count_before_test}\")\n",
    "print(f\"  NA answers after filtering: {na_count_after_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1743951711724,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "OIocBQYqcg-S",
    "outputId": "a7fdd8bd-9027-4f0b-eed8-ddad146bba64"
   },
   "outputs": [],
   "source": [
    "# Checks for training data\n",
    "print(\"Training Data Checks:\")\n",
    "print(f\"  Duplicate IDs found: {df_train_qa['id'].duplicated().sum()}\")\n",
    "print(f\"  Unique contexts: {df_train_qa['context'].nunique()}\")\n",
    "print(\"  Null values per column:\")\n",
    "print(df_train_qa.isnull().sum())\n",
    "\n",
    "# Checks for validation data\n",
    "print(\"\\nValidation Data Checks:\")\n",
    "print(f\"  Duplicate IDs found: {df_val_qa['id'].duplicated().sum()}\")\n",
    "print(f\"  Unique contexts: {df_val_qa['context'].nunique()}\")\n",
    "print(\"  Null values per column:\")\n",
    "print(df_val_qa.isnull().sum())\n",
    "\n",
    "# Checks for test data\n",
    "print(\"\\nTest Data Checks:\")\n",
    "print(f\"  Duplicate IDs found: {df_test_qa['id'].duplicated().sum()}\")\n",
    "print(f\"  Unique contexts: {df_test_qa['context'].nunique()}\")\n",
    "print(\"  Null values per column:\")\n",
    "print(df_test_qa.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1743951711734,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "ir69F2ZVPR7v",
    "outputId": "e5523bdf-0035-47a1-c1d2-6160815b251b"
   },
   "outputs": [],
   "source": [
    "# Find duplicate IDs in the test set\n",
    "duplicate_ids = df_test_qa[df_test_qa['id'].duplicated()]['id'].unique()\n",
    "\n",
    "# Display the full rows with duplicate IDs\n",
    "for id_ in duplicate_ids:\n",
    "    print(f\"--- ID: {id_} ---\")\n",
    "    print(df_test_qa[df_test_qa['id'] == id_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1743951711780,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "b5ryd4gmPeS2",
    "outputId": "313bd786-5aba-4fe0-eab3-85750523cfd3"
   },
   "outputs": [],
   "source": [
    "# Drop duplicate IDs\n",
    "duplicate_count_before_test = df_test_qa['id'].duplicated().sum()\n",
    "\n",
    "# Find duplicate IDs\n",
    "duplicate_ids = df_test_qa[df_test_qa['id'].duplicated()]['id'].unique()\n",
    "\n",
    "# Drop duplicate rows based on 'id', keeping the first occurrence\n",
    "df_test_qa = df_test_qa.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "# Re-check for duplicate IDs\n",
    "duplicate_count_after_test = df_test_qa['id'].duplicated().sum()\n",
    "\n",
    "print(\"Test Data Duplicate ID Counts:\")\n",
    "print(f\"  Duplicate IDs found before dropping: {duplicate_count_before_test}\")\n",
    "print(f\"  Duplicate IDs found after dropping: {duplicate_count_after_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1743951711820,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "zpLbGXAUddFq",
    "outputId": "3f221025-7914-4df3-f0f3-6b476ae54475"
   },
   "outputs": [],
   "source": [
    "# Calculate length features for training data\n",
    "df_train_qa['context_len'] = df_train_qa['context'].apply(len)\n",
    "df_train_qa['question_len'] = df_train_qa['question'].apply(len)\n",
    "df_train_qa['answer_len'] = df_train_qa['answer'].apply(len)\n",
    "\n",
    "print(\"Training Data Length Features:\")\n",
    "print(df_train_qa[['context_len', 'question_len', 'answer_len']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1743951711826,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "zvNxAevxdi4D",
    "outputId": "2f3670e2-cc33-400d-c85b-e07d3ea73380"
   },
   "outputs": [],
   "source": [
    "# Calculate 'is_impossible' value counts for all 3 dataframes\n",
    "def check_impossible_values(df, df_name):\n",
    "    value_counts = df['is_impossible'].value_counts(normalize=True)\n",
    "    print(f\"{df_name} Data 'is_impossible' Value Counts (Normalized):\")\n",
    "    print(value_counts)\n",
    "\n",
    "    if len(value_counts) == 1 and value_counts.index[0] == False and value_counts[False] == 1.0:\n",
    "        print(f\"No impossible questions in {df_name} DataFrame.\")\n",
    "\n",
    "# Training data\n",
    "check_impossible_values(df_train_qa, \"Training\")\n",
    "\n",
    "# Validation data\n",
    "print() #add an extra line for readability\n",
    "check_impossible_values(df_val_qa, \"Validation\")\n",
    "\n",
    "# Test data\n",
    "print() #add an extra line for readability\n",
    "check_impossible_values(df_test_qa, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1743951711851,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "38CpuLqQyYsZ",
    "outputId": "0277e5c2-b5cd-45ae-bc09-95a8e9d9c802"
   },
   "outputs": [],
   "source": [
    "# Display random examples from test set\n",
    "num_examples = 4\n",
    "top_answers = df_test_qa['answer'].value_counts().head(num_examples).index.tolist()\n",
    "\n",
    "print(\"--- Random Examples from Test Data ---\\n\")\n",
    "for ans in top_answers:\n",
    "    subset = df_test_qa[df_test_qa['answer'] == ans]\n",
    "\n",
    "    if len(subset) > 0:\n",
    "        random_index = random.randint(0, len(subset) - 1)\n",
    "        example = subset.iloc[random_index]\n",
    "\n",
    "        print(f\"Question: {example['question']}\")\n",
    "        print(f\"Context (first 150 chars): {example['context'][:150]}...\")\n",
    "        print(f\"Answer: {example['answer']}\")\n",
    "        print(f\"Is Impossible: {example['is_impossible']}\")\n",
    "        print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmxtGM8L_9t0"
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10362,
     "status": "ok",
     "timestamp": 1743951722215,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "FlJdzrTxZzwt",
    "outputId": "67af6998-0ce1-4eb2-dcf4-7b72a45d0e50"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login\n",
    "\n",
    "\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424,
     "referenced_widgets": [
      "acf15c21babb4ab7b88f02b7990d9894",
      "e5f40fea82ec46a3a71db04f2c85d876",
      "be4ca8ed178d457b8ffc157c9318a4d6",
      "5671a373ae334069bb7368f811ae4056",
      "a8b20d8103424fc996eef05296f6b2be",
      "8b9bd5e8f40d4dd29766d8b38a6a3c44",
      "a1191f66cd9b402987c02c8f1c96f980",
      "32392be3734e45028affa77f6db6fa02",
      "dbec7433450142d9ace8e613ddebc73d",
      "0a0af363d9014bd2989b3df281862b39",
      "ac8b3340b3b34559afde2fb7bfab8be1",
      "a160d5368598492c9134c6b97c5e3397",
      "9ae4dbfd79d14881bf51ed3e02c7093d",
      "09d1264acc4b4bf1a294a73808bd746d",
      "5cc11619295844b2abed5330833154ca",
      "58504858d7e242878d9f3cb5185199b0",
      "a6183496edbc4b758230da5972926fac",
      "b9122f85cced45a9a302d869c3b64f2e",
      "37974236517c478c970d8daa070cd4e0",
      "f1e2650f3e8f4a29a1b10ca67a5edbae",
      "f07f10a8a53e4bd8b07b20b08647295d",
      "eaa420139192463d98961dab00dfc66f",
      "2544473bffc84443971e4d2e038216ef",
      "4928a34ebdfe47cc9ccbfff376b656f2",
      "b0c08589059144e189ba8848d21b3499",
      "203b8192701f4ceca91b1efe4f276aa6",
      "c2441c0174f04aa1bbf1a81b6558a88f",
      "6fe5d70a08f740038791f1bcee4854ae",
      "7cf52e09926e4596ad60d40d2a66abfb",
      "649f8560b790477a9c0fa468ae7ec2b6",
      "73da728a059c4964bdd0641c29404447",
      "746c6f78b42e4134bb445793c5de1ea4",
      "d6cc5a5c951b4133b7dd7b519d0efc49",
      "f20e7175b1514dc8af75a403f397a65e",
      "34e5717de60a4573866a0ed3c91deff0",
      "1e56780fc7cc4b22b3a9a3d6a3fae732",
      "f133cd7f579e4ae3a42bd3a30e26ad4e",
      "c375d8de854c4e27ae5544d3f0e31988",
      "5deb24415b37419faf34952a2cd2c4fa",
      "2e4e1b3ea32148b7ba0f2ebc2ee113f5",
      "7c6fad6fc5434007a4d1cda5e62b5800",
      "7bcb5fe53b17416d85db87dcbe6a32c9",
      "627bdeb147564d708b5dda7a6f7d594b",
      "d0f1f7fae5424c60ba94d3dcd87a00c6",
      "0f45194355f2445b974aebce44dcbdb7",
      "9dbd5ec4a76946f3aee43fa115245017",
      "f93e309050344ad3ba3b2ef573d8d41d",
      "c8f33576982a4b339a8ca2bdf8f16709",
      "20129db7328840e8ba2b5c5ad858101a",
      "decad4949436495b9f2d699f1bb7bcac",
      "c0071cde08a74ecb9f03e6eabc359c9d",
      "7105227c070c487e83ae0861da078eb8",
      "8f9955dd264048f9875668e82e2b9275",
      "cab46faf807e4b58af8c82be6736e099",
      "a6a80d3535a2444789693806de19a122",
      "e4bc2e88bf5d4dea8d6ad6607dd3a637",
      "04440c78d0e6474fb44c068d66d85f61",
      "afb12f7074ec4cce899deb4e618aa107",
      "3188d0db02294ebcb5d5a8f7ab4324ce",
      "b7e2b262d92644408ecaa13f4e96ca66",
      "780e9c84e01e4490963c5017fce88392",
      "bedb0c83670d4c349156dde861c03a60",
      "ec884d9192bb44c9a598f8b03ec2b9b5",
      "4785348aa0074501803fd20cd455d965",
      "1329e92cb04b4fab9a1c8fde69bc1378",
      "673bb367e07d4501aeac1d3461023e60",
      "6db6b895ebf745199954c6461acd8cd0",
      "d2afc03a3ce84fd297a8c2d7ec439eba",
      "9ae812a234ff48d383f29f59cd323c6e",
      "c0dc1141bd9a46a3864fd06fdf0a1cdd",
      "d06a18fba5fe4e31b4f20914e7a7b51c",
      "b8e65e34642d413b97153a30bf780f18",
      "10eabd9388b5454b9fe97d10df3a5c06",
      "cac8f8138978468f96b8b3e6ff1dfff5",
      "0652c1d133a64c64af54c4bc593e4eed",
      "5541ebc02e3945a7851625738d9d373e",
      "eacfcb43f23048bbbe3bcc7d9505c034",
      "7e11f2928d244b8ea3eb8b456b4a14a0",
      "3bd5da221c924bc1a54f1a832d5bb903",
      "a90c576fc89041a6a39649be90ad62f0",
      "f431f6fe7a4b4223b12da2f2b0f5ed20",
      "73a0cac8929742af851f908468d07607",
      "504cf3a06e4c44c8abae3c017fe0be9b",
      "01742df61965483d81cfdea68a6984af",
      "0a1877394e4946d9ae069d773eefdf9e",
      "703c144a1d254df3bee9bb0c23e9f968",
      "0336ebccf81b48feb834b635cbbcf41e",
      "bbba8eaa0a30465b917001669a45e24e",
      "de5fb5962d2649a79ad4cb0d2f10cfe3",
      "30868ba6944a4c4c8144470952a89275",
      "e553b20914dd49f8879ea731f65f29e1",
      "dde9255775e9464a94af1eb76d20ec32",
      "7cd38c3de58143c289f58c6489fe25b2",
      "1857256de17247a688b64f1d21c05558",
      "613922cdaa574b48affc7761b78a9a16",
      "95fbb619178c43eeb01ebdfec607ffb0",
      "24fe826d00db4a29ae2e6435d5b8875b",
      "53e1f5a9aa7b4564a92e525d48fc7135",
      "9562a392c0a04411a02fc74125fde9cd",
      "3957b42eaadc4f28acd58c83c68e44a1",
      "eda06a79a1ce4873bd806542123811df",
      "9f06197279b54e53ac756c6b39af0e55",
      "ddd45d1ec57643d8bc3998a000897d91",
      "e985384e02a94720ae6adbc1d9e3c881",
      "b00aa534751c444680b5dc8af1f00644",
      "950f117279824e3ea6ee46059b2df927",
      "aa5285c92e2e4fce9c84d99580407022",
      "76844d6ac8fd4890b2eedbd145c8a41d",
      "8c53391545384888a1fe4b211d777680",
      "76aaa70b8efc47e8b4dd6e3148f5daab",
      "b2d1cb27955840e38fffb01afafc2362",
      "d58061dd83914a8baec93812b8343c17",
      "5f37dd794c904418a22acfe6a8a0ec1d",
      "0ad227c50d55402aa53f5bc476928f3b",
      "aed10154bda44c2b8efd1c8df0c232ab",
      "fb888e5ee77e439ea54cbe9bfcf2b473",
      "0d6854402e064537bc3421c1df992027",
      "a5c1073f8ec8430c8a83abe22da7ee28",
      "3c83301d443a4846b7e2127077713d22",
      "17f03b794ad14f9a8a251b3eaa967a53",
      "f9c083b11e294f07aed251d2ac1624d9"
     ]
    },
    "executionInfo": {
     "elapsed": 13148,
     "status": "ok",
     "timestamp": 1743953352854,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "rtZJnrhdIYdq",
    "outputId": "2150ceb8-dc10-4a2f-f304-2ff3cc9827d8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import nltk # Needed for OOV calculation word tokenization\n",
    "from datasets import Dataset, DatasetDict # Added DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from evaluate import load # Used for SQuAD metric\n",
    "\n",
    "# --- Configuration ---\n",
    "# Model Llama 3.1 8B (Using Meta-Llama-3-8B identifier)\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "# LoRA Rank (r): 16\n",
    "# LoRA Alpha: 32\n",
    "# Batch Size: 1 (with gradient accumulation to 16)\n",
    "# Learning Rate: 2e-5\n",
    "# Training Epochs: 10\n",
    "# Early Stopping: Based on validation F1\n",
    "# Precision: bfloat16\n",
    "\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- Quantization Config ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# --- Load QA Model ---\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# --- Prepare Model for LoRA Training ---\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E853DQ8A2RoW"
   },
   "source": [
    "---\n",
    "\n",
    "v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5b362cbf45d4494dafe5aaef8c3efa35",
      "7efe0d3afa88407da4dd2002f85cf863",
      "6e40ada8dbb64670ad2e5eb218631483",
      "2abd08a7932d4c0b8d440010d38e0e62",
      "207475a8045c45519958db6228f03fe4",
      "10dfafdffea94e468a8bfc16deffcaae",
      "afb4326263c3481fb1bb70395497ff60",
      "12b7b84785c14be68e3d93817e5a36ba",
      "3d1a320c3d4441989c39f3b2d72f25a3",
      "acda125e4a1d42f49891b7ed14cd872c",
      "4302f10fc4564fa08c429d760a5c3809",
      "fc459bf522a3407a9078ee65841b0c06",
      "32c97c22f72243e79272d0bb9b37285e",
      "7fccfcbd894140e2ad01d255944cc6af",
      "b41b23b5ff5b454e976435174fe94fa2",
      "bbec9951d59949c5a98018655540e519",
      "fca851ee420d475c9bc4cfecf8d686a7",
      "7567d04a940e4152946b6b03610a2428",
      "fb75a1c9ea8347cdafc7e185f04e233e",
      "9bd47acc1c1c40d789052e9b076f3352",
      "7587d4529bf44252871f8e5c69a1f2ba",
      "235e8a3e613d46398259ed286ad7e8ce",
      "ca8e0054a74e4ae894df3a380b1c534d",
      "28aae35f21f3470f8be15a33b2a4a93a",
      "8578797ea1bc441e8009247e1cdb9443",
      "3b293482bcdc4bb7855cc7a7c8c1d9d6",
      "cd75c2a6372648f295587d30b274eff2",
      "013ed3887cd5477595d0c85a47b5c2fc",
      "28e0e3a5308c46a3b49336e1fc306244",
      "be269e850a8940a6a872ac74e656cd86",
      "85beaa48fade495eaf4fea16875ba6ab",
      "2f66333fdf0a46c3969409323b1c0b03",
      "f208d9a04a1a4b149a6d359366178a77"
     ]
    },
    "executionInfo": {
     "elapsed": 14390,
     "status": "ok",
     "timestamp": 1743956493056,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "ggfEx4bF3jZP",
    "outputId": "c2498b65-107c-40ba-a27e-5b154ae7c308"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# --- LoRA Configuration ---\n",
    "use_lora = True\n",
    "if use_lora:\n",
    "    peft_config = LoraConfig(\n",
    "        task_type='QUESTION_ANS',\n",
    "        inference_mode=False,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "# --- Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-8b-arabicaqa\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    gradient_checkpointing=True,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs-llama3-8b-arabicaqa\",\n",
    "    logging_steps=10,\n",
    "    bf16=True, # Use bfloat16 precision\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\", # Use F1 score for early stopping/best model\n",
    "    report_to=\"none\",\n",
    "    optim=\"adamw_torch\",\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\"\n",
    ")\n",
    "\n",
    "# Reduce dataset size to 100 samples\n",
    "df_train_qa = df_train_qa.sample(n=10, random_state=42)\n",
    "df_val_qa = df_val_qa.sample(n=10, random_state=42)\n",
    "df_test_qa = df_test_qa.sample(n=10, random_state=42)\n",
    "\n",
    "# --- Convert DataFrames to Datasets ---\n",
    "# Keep raw datasets for postprocessing steps that need original text/structure\n",
    "raw_datasets = DatasetDict({\n",
    "    'train': Dataset.from_pandas(df_train_qa),\n",
    "    'validation': Dataset.from_pandas(df_val_qa),\n",
    "    'test': Dataset.from_pandas(df_test_qa)\n",
    "})\n",
    "print(\"\\nConverted preprocessed DataFrames to Hugging Face Datasets:\")\n",
    "print(raw_datasets)\n",
    "\n",
    "# --- QA Data Preprocessing (Tokenization) ---\n",
    "max_length = 512\n",
    "doc_stride = 128\n",
    "\n",
    "# MODIFIED: Reads 'answer' and 'answer_start' directly from the input example\n",
    "def prepare_train_features(examples):\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=False,\n",
    "        padding=False,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_token_type_ids=True,\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id else 0\n",
    "\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "\n",
    "        start_char = examples[\"answer_start\"][sample_index]\n",
    "        answer_text = examples[\"answer\"][sample_index]\n",
    "        end_char = start_char + len(answer_text) if start_char is not None and answer_text else None\n",
    "\n",
    "        context = examples[\"context\"][sample_index]\n",
    "        question = examples[\"question\"][sample_index]\n",
    "\n",
    "        print(f\"\\nExample {i} — ID: {examples['id'][sample_index]}\")\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"A: {answer_text} | Start char: {start_char}, End char: {end_char}\")\n",
    "        print(f\"Context excerpt: ...{context[max(start_char - 30, 0):start_char + len(answer_text) + 30]}...\")\n",
    "\n",
    "        if start_char is None or end_char is None:\n",
    "            print(\"Skipping due to missing start_char or answer_text.\")\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            continue\n",
    "\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "\n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "\n",
    "        # Check if the answer is inside this span\n",
    "        if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "            print(\"Answer span not in current chunk → fallback to CLS\")\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Fine-tune token indices\n",
    "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                token_start_index += 1\n",
    "            final_start = token_start_index - 1\n",
    "\n",
    "            while offsets[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            final_end = token_end_index + 1\n",
    "\n",
    "            span = tokenizer.decode(input_ids[final_start:final_end])\n",
    "            print(f\"Token span found — Decoded answer: {span}\")\n",
    "            print(f\"Token Start Index: {final_start}, Token End Index: {final_end}\")\n",
    "\n",
    "            tokenized_examples[\"start_positions\"].append(final_start)\n",
    "            tokenized_examples[\"end_positions\"].append(final_end)\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "data_collator = default_data_collator # Use the default\n",
    "\n",
    "\n",
    "# This function doesn't rely on answer columns, should be fine\n",
    "def prepare_validation_features(examples):\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\", # Truncate context if needed\n",
    "        padding=\"max_length\",    # Pad to max_length\n",
    "        max_length=max_length,   # Use the defined max_length\n",
    "        stride=doc_stride,       # Use the defined stride\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_token_type_ids=True,\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    dummy_label = 0  # Hack: Add dummy label to trigger Trainer metric logging\n",
    "    tokenized_examples[\"start_positions\"] = [dummy_label] * len(tokenized_examples[\"input_ids\"])\n",
    "    tokenized_examples[\"end_positions\"] = [dummy_label] * len(tokenized_examples[\"input_ids\"])\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "    return tokenized_examples\n",
    "\n",
    "\n",
    "# --- Process Datasets (Tokenization) ---\n",
    "tokenized_datasets = DatasetDict()\n",
    "\n",
    "print(\"\\nTokenizing datasets...\")\n",
    "tokenized_datasets['train'] = raw_datasets['train'].map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets['train'].column_names\n",
    ")\n",
    "tokenized_datasets['validation'] = raw_datasets['validation'].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets['validation'].column_names\n",
    ")\n",
    "tokenized_datasets['test'] = raw_datasets['test'].map(\n",
    "    prepare_validation_features,\n",
    "     batched=True,\n",
    "     remove_columns=raw_datasets['test'].column_names\n",
    ")\n",
    "print(\"Tokenization complete.\")\n",
    "print(tokenized_datasets)\n",
    "\n",
    "\n",
    "# --- Data Collator ---\n",
    "data_collator = default_data_collator\n",
    "\n",
    "# --- Metrics Calculation ---\n",
    "squad_metric = load(\"squad\")\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size=20, max_answer_length=30):\n",
    "    # This function takes raw examples (Dataset object), features (tokenized Dataset object), and predictions\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])} # Map example ID to its index in the examples dataset\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    # Map feature index back to its original example ID\n",
    "    for i, feature in enumerate(features):\n",
    "        # example_id might not be directly in feature if remove_columns was used after mapping\n",
    "        # We need to rely on the order or add example_id during mapping if needed\n",
    "         features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "\n",
    "    predictions = collections.OrderedDict()\n",
    "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "    for example_index, example in enumerate(examples):\n",
    "        feature_indices = features_per_example[example_index]\n",
    "        min_null_score = None\n",
    "        valid_answers = []\n",
    "        context = example[\"context\"]\n",
    "\n",
    "        for feature_index in feature_indices:\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id) if tokenizer.cls_token_id else 0\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                        or not isinstance(offset_mapping[start_index], tuple)\n",
    "                        or not isinstance(offset_mapping[end_index], tuple)\n",
    "                    ):\n",
    "                        continue\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "\n",
    "        predictions[str(example[\"id\"])] = best_answer[\"text\"]\n",
    "\n",
    "\n",
    "    predictions_list = [\n",
    "        {\"id\": str(k), \"prediction_text\": v}\n",
    "        for k, v in predictions.items()\n",
    "    ]\n",
    "    return predictions_list\n",
    "\n",
    "\n",
    "# MODIFIED: Creates references based on the user's preprocessed DataFrame structure\n",
    "#def compute_metrics(p):\n",
    "    # Use the raw validation dataset (derived from df_val_qa) for ground truth\n",
    "    # The 'examples' argument to postprocess_qa_predictions should be the raw data\n",
    "    #predictions = postprocess_qa_predictions(raw_datasets['validation'], tokenized_datasets['validation'], p.predictions)\n",
    "\n",
    "squad_metric = load(\"squad\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    print(\">>> compute_metrics() was called <<<\")\n",
    "\n",
    "    predictions = postprocess_qa_predictions(\n",
    "        raw_datasets['validation'],\n",
    "        tokenized_datasets['validation'],\n",
    "        p.predictions\n",
    "    )\n",
    "\n",
    "    df_val = raw_datasets[\"validation\"].to_pandas()\n",
    "    formatted_references = [\n",
    "        {\n",
    "            \"id\": str(row[\"id\"]),\n",
    "            \"answers\": {\n",
    "                \"text\": [str(row[\"answer\"])],\n",
    "                \"answer_start\": [int(row[\"answer_start\"])]\n",
    "            }\n",
    "        }\n",
    "        for _, row in df_val.iterrows()\n",
    "    ]\n",
    "\n",
    "    print(\"Sample prediction:\", predictions[0])\n",
    "    print(\"Sample reference:\", formatted_references[0])\n",
    "\n",
    "    results = squad_metric.compute(predictions=predictions, references=formatted_references)\n",
    "\n",
    "    return {\n",
    "        \"eval_em\": results[\"exact_match\"],\n",
    "        \"eval_f1\": results[\"f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "15bc778b12b2471bb21a0987e89adb6a",
      "642d4b9bae4e4c28b7b73c183ac30f25",
      "c9ca0e7108e04fe089a2eb6c1c89084f",
      "0f752951d22846e09297ca7d3f8db526",
      "431dc8c89a724b43bc334c1818b841b3",
      "dc3ca179bcf04bdebb0e1872a0da074d",
      "ef39482dfc454b32b270e6ba74d775bb",
      "dadf279db7fb4f5aadfb9d5cd17d3ba2",
      "6ec7364780ab45fcba0cebfe1432fa90",
      "e3503bfd6aa343fead74fc8a52e04eee",
      "5655a5a0dbf74011bdf238e33a809b15",
      "20b1c0d33ea9481ba5585ba9c5f4f123",
      "e1c19a4b87d44e82868a5d83c7c825e8",
      "aa66341791cd4b469211965fae4aebf0",
      "a78bc53580d74886a4aed93663f8772b",
      "41721898364e44bf859035d659d262d4",
      "8dfcd9214cd84dfba86843da3177bff5",
      "028a3b14585a4ebea4880f3674261a68",
      "e172d5d60d3546279b0df2a2d46dc685",
      "d9b11ece8e2747b0a89009ebe53717e8",
      "b56a9364a5634e2c9bef16e99d9f9ca7",
      "6d6d656954694710865cbde3f75f6ddd",
      "b6a3ff34f8004185a86b7c607fa05f6d",
      "3667eb216bfe41d0b260d0d3dfb576f6",
      "229b73ccda5b45be899d8ff0064a757f",
      "4c42b409d2794987b231c71445f4802c",
      "e0a2564929aa40e5904d575abe4e66c0",
      "84c87a59c1b9401c8c5484d3fc469351",
      "593c585a75a74ea5ab62115150e77e56",
      "5d1e8e41f1c547cfb513c790e00328bb",
      "ba114b8b685f4899aac4d0716ee3eaed",
      "2b9ef04da5034dbcb6775cec0df843f6",
      "d7802cff5e7d4dc4ac0e37d7c3c18d8e"
     ]
    },
    "executionInfo": {
     "elapsed": 19055,
     "status": "ok",
     "timestamp": 1743957094718,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "EAXCHywT613w",
    "outputId": "3f2cf7ce-eae6-4789-8ce1-12986e0f7fa0"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "import traceback\n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"], examples[\"context\"], truncation=\"only_second\",\n",
    "        max_length=max_length, stride=doc_stride, return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True, padding=\"max_length\"\n",
    "    )\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping_list = tokenized_examples[\"offset_mapping\"] # Keep offsets\n",
    "\n",
    "    start_positions = [] ; end_positions = [] ; all_sequence_ids = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping_list):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token is not None and tokenizer.cls_token_id in input_ids else 0\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        all_sequence_ids.append(sequence_ids) # Store sequence_ids\n",
    "\n",
    "        sample_index = sample_mapping[i]\n",
    "        start_char = examples[\"answer_start\"][sample_index]\n",
    "        answer_text = examples[\"answer\"][sample_index]\n",
    "\n",
    "        try: start_char_int = int(start_char); end_char = start_char_int + len(str(answer_text))\n",
    "        except (TypeError, ValueError): start_char_int = None; end_char = None\n",
    "\n",
    "        if start_char_int is None:\n",
    "            start_positions.append(cls_index); end_positions.append(cls_index)\n",
    "        else:\n",
    "            token_start_index = 0\n",
    "            while sequence_ids is not None and token_start_index < len(sequence_ids) and sequence_ids[token_start_index] != 1: token_start_index += 1\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids is not None and token_end_index >= 0 and sequence_ids[token_end_index] != 1: token_end_index -= 1\n",
    "\n",
    "            if not isinstance(offsets, (list, tuple)):\n",
    "                 start_positions.append(cls_index); end_positions.append(cls_index); continue\n",
    "\n",
    "            start_offset_valid = token_start_index < len(offsets) and offsets[token_start_index] is not None\n",
    "            end_offset_valid = token_end_index >= 0 and token_end_index < len(offsets) and offsets[token_end_index] is not None\n",
    "\n",
    "            if not (start_offset_valid and end_offset_valid and offsets[token_start_index][0] <= start_char_int and offsets[token_end_index][1] >= end_char):\n",
    "                start_positions.append(cls_index); end_positions.append(cls_index)\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index] is not None and offsets[token_start_index][0] <= start_char_int: token_start_index += 1\n",
    "                calculated_start = token_start_index - 1\n",
    "                start_positions.append(max(0, calculated_start))\n",
    "\n",
    "                while token_end_index >= 0 and offsets[token_end_index] is not None and offsets[token_end_index][1] >= end_char: token_end_index -= 1\n",
    "                calculated_end = token_end_index + 1\n",
    "                final_end = min(len(input_ids) - 1, calculated_end)\n",
    "                final_start = max(0, calculated_start)\n",
    "                end_positions.append(max(final_start, final_end))\n",
    "\n",
    "    tokenized_examples[\"start_positions\"] = start_positions\n",
    "    tokenized_examples[\"end_positions\"] = end_positions\n",
    "    tokenized_examples[\"sequence_ids\"] = all_sequence_ids\n",
    "    # offset_mapping is already present\n",
    "\n",
    "    # Sanity check lengths\n",
    "    if len(tokenized_examples[\"start_positions\"]) != len(tokenized_examples[\"input_ids\"]): raise ValueError(\"Length mismatch for start_positions\")\n",
    "    if len(tokenized_examples[\"end_positions\"]) != len(tokenized_examples[\"input_ids\"]): raise ValueError(\"Length mismatch for end_positions\")\n",
    "    if len(tokenized_examples[\"sequence_ids\"]) != len(tokenized_examples[\"input_ids\"]): raise ValueError(\"Length mismatch for sequence_ids\")\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "# ** FIXED prepare_validation_features ** (Ensures required columns are returned)\n",
    "def prepare_validation_features(examples):\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"], examples[\"context\"], truncation=\"only_second\",\n",
    "        max_length=max_length, stride=doc_stride, return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True, padding=\"max_length\"\n",
    "    )\n",
    "    # Need overflow_to_sample_mapping to get original example ID\n",
    "    if \"overflow_to_sample_mapping\" not in tokenized_examples:\n",
    "        print(\"Warning: overflow_to_sample_mapping not found in tokenizer output. Cannot map features to original IDs reliably.\")\n",
    "        # Handle this case, maybe by returning or raising error, depending on requirements\n",
    "        # For now, try to proceed assuming it might be empty but present? Unlikely.\n",
    "        sample_mapping = [] # Placeholder\n",
    "    else:\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    offset_mapping_list = tokenized_examples[\"offset_mapping\"] # Keep reference\n",
    "\n",
    "    example_ids = [] ; processed_offset_maps = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1\n",
    "        # Check if sample_mapping is valid for index i\n",
    "        if i < len(sample_mapping):\n",
    "            sample_index = sample_mapping[i]\n",
    "            # Check if sample_index is valid for examples['id']\n",
    "            if sample_index < len(examples[\"id\"]):\n",
    "                 example_ids.append(examples[\"id\"][sample_index])\n",
    "            else:\n",
    "                 example_ids.append(None) # Indicate mapping failure\n",
    "                 print(f\"Warning: sample_index {sample_index} out of bounds for examples['id'] (len={len(examples['id'])}).\")\n",
    "        else:\n",
    "            example_ids.append(None) # Indicate mapping failure\n",
    "            print(f\"Warning: index {i} out of bounds for sample_mapping (len={len(sample_mapping)}).\")\n",
    "\n",
    "\n",
    "        current_offsets = offset_mapping_list[i]\n",
    "        processed_offset_maps.append([\n",
    "            (o if sequence_ids is not None and k < len(sequence_ids) and sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(current_offsets)\n",
    "        ])\n",
    "\n",
    "    tokenized_examples[\"example_id\"] = example_ids\n",
    "    tokenized_examples[\"offset_mapping\"] = processed_offset_maps\n",
    "\n",
    "    # Clean up remaining popped key if needed, though usually handled by remove_columns\n",
    "    # if \"overflow_to_sample_mapping\" in tokenized_examples:\n",
    "    #      del tokenized_examples[\"overflow_to_sample_mapping\"]\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "# --- Process Datasets (Tokenization) ---\n",
    "print(\"\\n--- Tokenizing Datasets ---\")\n",
    "tokenized_datasets = DatasetDict()\n",
    "\n",
    "try:\n",
    "    # Apply the FIXED preprocessing functions\n",
    "    tokenized_datasets['train'] = raw_datasets['train'].map(\n",
    "        prepare_train_features, batched=True,\n",
    "        remove_columns=raw_datasets['train'].column_names\n",
    "    )\n",
    "    tokenized_datasets['validation'] = raw_datasets['validation'].map(\n",
    "        prepare_validation_features, batched=True,\n",
    "        remove_columns=raw_datasets['validation'].column_names\n",
    "    )\n",
    "    tokenized_datasets['test'] = raw_datasets['test'].map(\n",
    "        prepare_validation_features, batched=True,\n",
    "        remove_columns=raw_datasets['test'].column_names\n",
    "    )\n",
    "    print(\"Tokenization complete.\")\n",
    "    print(\"Columns in tokenized train dataset:\", tokenized_datasets['train'].column_names)\n",
    "    print(\"Columns in tokenized validation dataset:\", tokenized_datasets['validation'].column_names)\n",
    "    print(\"Columns in tokenized test dataset:\", tokenized_datasets['test'].column_names)\n",
    "    # Verify required columns are present\n",
    "    assert 'sequence_ids' in tokenized_datasets['train'].column_names, \"'sequence_ids' missing from train set!\"\n",
    "    assert 'offset_mapping' in tokenized_datasets['train'].column_names, \"'offset_mapping' missing from train set!\"\n",
    "    assert 'start_positions' in tokenized_datasets['train'].column_names, \"'start_positions' missing from train set!\"\n",
    "    assert 'end_positions' in tokenized_datasets['train'].column_names, \"'end_positions' missing from train set!\"\n",
    "    assert 'example_id' in tokenized_datasets['validation'].column_names, \"'example_id' missing from validation set!\"\n",
    "    assert 'offset_mapping' in tokenized_datasets['validation'].column_names, \"'offset_mapping' missing from validation set!\"\n",
    "\n",
    "except Exception as map_error:\n",
    "    print(f\"!!! ERROR during dataset mapping/tokenization: {map_error}\")\n",
    "    traceback.print_exc()\n",
    "    # Handle error - perhaps exit or raise? Skipping subsequent steps if tokenization fails.\n",
    "    raise SystemExit(\"Tokenization failed, cannot continue.\")\n",
    "\n",
    "\n",
    "# --- Data Collator ---\n",
    "data_collator = default_data_collator\n",
    "\n",
    "# --- Metrics Calculation ---\n",
    "print(\"\\n--- Setting up Metrics ---\")\n",
    "squad_metric = load(\"squad\")\n",
    "\n",
    "# Define postprocess_qa_predictions (ensure it's defined correctly)\n",
    "# (Using the robust version provided before)\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size=20, max_answer_length=30):\n",
    "    # ... (Keep the robust postprocess_qa_predictions function from previous answers) ...\n",
    "    # ... (It includes checks for required columns and other edge cases) ...\n",
    "    if not isinstance(raw_predictions, (tuple, list)) or len(raw_predictions) != 2:\n",
    "         print(f\"Error: Unexpected raw_predictions format: {type(raw_predictions)}\")\n",
    "         return collections.OrderedDict()\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "\n",
    "    required_feature_cols = [\"example_id\", \"offset_mapping\", \"input_ids\"]\n",
    "    if not all(col in features.column_names for col in required_feature_cols):\n",
    "        missing_cols = [col for col in required_feature_cols if col not in features.column_names]\n",
    "        print(f\"Error: Features dataset is missing required columns: {missing_cols}\")\n",
    "        return collections.OrderedDict()\n",
    "\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        feature_example_id = feature[\"example_id\"]\n",
    "        if feature_example_id is not None and feature_example_id in example_id_to_index:\n",
    "            features_per_example[example_id_to_index[feature_example_id]].append(i)\n",
    "\n",
    "    predictions = collections.OrderedDict()\n",
    "    # print(f\"Post-processing {len(examples)} examples split into {len(features)} features.\") # Less verbose\n",
    "\n",
    "    for example_index, example in enumerate(examples):\n",
    "        feature_indices = features_per_example.get(example_index, [])\n",
    "        if not feature_indices: continue\n",
    "\n",
    "        min_null_score = None ; valid_answers = []\n",
    "        context = str(example[\"context\"])\n",
    "\n",
    "        for feature_index in feature_indices:\n",
    "             if feature_index >= len(all_start_logits): continue\n",
    "             start_logits = all_start_logits[feature_index]\n",
    "             end_logits = all_end_logits[feature_index]\n",
    "             offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "             input_ids = features[feature_index][\"input_ids\"]\n",
    "\n",
    "             if not isinstance(offset_mapping, (list, tuple)): continue\n",
    "\n",
    "             cls_index = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token is not None and tokenizer.cls_token_id in input_ids else 0\n",
    "             feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "             if min_null_score is None or min_null_score < feature_null_score: min_null_score = feature_null_score\n",
    "\n",
    "             start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "             end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "\n",
    "             for start_index in start_indexes:\n",
    "                 for end_index in end_indexes:\n",
    "                     if start_index >= len(offset_mapping) or end_index >= len(offset_mapping) or \\\n",
    "                        offset_mapping[start_index] is None or offset_mapping[end_index] is None: continue\n",
    "                     if end_index < start_index or end_index - start_index + 1 > max_answer_length: continue\n",
    "                     try:\n",
    "                        start_char = offset_mapping[start_index][0]\n",
    "                        end_char = offset_mapping[end_index][1]\n",
    "                        if start_char < len(context) and end_char <= len(context):\n",
    "                             valid_answers.append({\"score\": start_logits[start_index] + end_logits[end_index], \"text\": context[start_char: end_char]})\n",
    "                     except IndexError: continue # Ignore invalid offset indices\n",
    "\n",
    "        best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        if valid_answers: best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "    # print(f\"Post-processing finished. Generated {len(predictions)} predictions.\") # Less verbose\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Define compute_metrics (Cleaned version)\n",
    "def compute_metrics(p):\n",
    "    if 'validation' not in raw_datasets or not raw_datasets['validation'] or \\\n",
    "       'validation' not in tokenized_datasets or not tokenized_datasets['validation']:\n",
    "         print(\"Error: Validation datasets missing/empty in compute_metrics.\")\n",
    "         return {\"em\": 0.0, \"f1\": 0.0}\n",
    "    try:\n",
    "        predictions_dict = postprocess_qa_predictions(raw_datasets['validation'], tokenized_datasets['validation'], p.predictions)\n",
    "        formatted_references = []\n",
    "        for i in range(len(raw_datasets['validation'])):\n",
    "            example = raw_datasets['validation'][i]\n",
    "            try: start_pos = int(example[\"answer_start\"])\n",
    "            except (TypeError, ValueError): start_pos = -1\n",
    "            formatted_references.append({\"id\": example[\"id\"], \"answers\": {\"text\": [str(example[\"answer\"])], \"answer_start\": [start_pos]}})\n",
    "\n",
    "        if not predictions_dict or not formatted_references: return {\"em\": 0.0, \"f1\": 0.0} # Handle empty case\n",
    "\n",
    "        # Ensure keys are strings for comparison if IDs are strings\n",
    "        predictions_dict_str_keys = {str(k): v for k, v in predictions_dict.items()}\n",
    "        formatted_references_str_keys = []\n",
    "        for ref in formatted_references:\n",
    "            ref['id'] = str(ref['id'])\n",
    "            formatted_references_str_keys.append(ref)\n",
    "\n",
    "        results = squad_metric.compute(predictions=predictions_dict_str_keys, references=formatted_references_str_keys)\n",
    "\n",
    "        if not results or \"exact_match\" not in results or \"f1\" not in results: return {\"em\": 0.0, \"f1\": 0.0} # Handle metric failure\n",
    "        return {\"em\": results[\"exact_match\"], \"f1\": results[\"f1\"]}\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"!!! UNEXPECTED ERROR in compute_metrics: {e}\")\n",
    "         traceback.print_exc()\n",
    "         return {\"em\": 0.0, \"f1\": 0.0}\n",
    "\n",
    "# --- Trainer Initialization ---\n",
    "print(\"\\n--- Initializing Trainer ---\")\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer, data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "print(\"Trainer initialized.\")\n",
    "\n",
    "# --- Training ---\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    end_time = time.time()\n",
    "    print(f\"Training finished. Time: {end_time - start_time:.2f} seconds\")\n",
    "    trainer.log_metrics(\"train\", train_result.metrics)\n",
    "    trainer.save_metrics(\"train\", train_result.metrics)\n",
    "    trainer.save_state()\n",
    "    trainer.save_model(f\"{training_args.output_dir}/final_model\")\n",
    "    print(f\"Model saved to {training_args.output_dir}/final_model\")\n",
    "except Exception as training_error:\n",
    "    print(f\"!!! ERROR during training: {training_error}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "# --- Evaluation ---\n",
    "print(\"\\n--- Starting Evaluation on Validation Set ---\")\n",
    "start_eval = time.time(); eval_results = {}\n",
    "try:\n",
    "    eval_results = trainer.evaluate()\n",
    "    end_eval = time.time()\n",
    "    print(f\"Evaluation finished. Time: {end_eval - start_eval:.2f} seconds\")\n",
    "    print(\"Validation Set Evaluation Results:\")\n",
    "    print(eval_results)\n",
    "    trainer.log_metrics(\"eval\", eval_results)\n",
    "    trainer.save_metrics(\"eval\", eval_results)\n",
    "    if 'eval_loss' in eval_results and np.isnan(eval_results['eval_loss']): print(\"\\nWARNING: eval_loss is NaN.\\n\")\n",
    "except Exception as eval_error: print(f\"!!! ERROR during evaluation: {eval_error}\"); traceback.print_exc()\n",
    "\n",
    "# --- Prediction on Test Set ---\n",
    "print(\"\\n--- Starting Prediction on Test Set ---\")\n",
    "start_test = time.time(); final_test_predictions = {}; test_metrics_log = {}\n",
    "try:\n",
    "    test_predictions_output = trainer.predict(tokenized_datasets['test'])\n",
    "    if test_predictions_output.predictions is not None:\n",
    "        final_test_predictions = postprocess_qa_predictions(raw_datasets['test'], tokenized_datasets['test'], test_predictions_output.predictions)\n",
    "    end_test = time.time(); print(f\"Test Inference finished. Time: {end_test - start_test:.2f} seconds\")\n",
    "\n",
    "    print(\"\\nComputing metrics on test set...\")\n",
    "    test_references = []\n",
    "    for i in range(len(raw_datasets['test'])):\n",
    "        example = raw_datasets['test'][i]\n",
    "        try: start_pos = int(example[\"answer_start\"])\n",
    "        except (TypeError, ValueError): start_pos = -1\n",
    "        test_references.append({\"id\": str(example[\"id\"]), \"answers\": {\"text\": [str(example[\"answer\"])], \"answer_start\": [start_pos]}})\n",
    "\n",
    "    if final_test_predictions and test_references:\n",
    "        final_test_predictions_str_keys = {str(k): v for k, v in final_test_predictions.items()}\n",
    "        test_metrics = squad_metric.compute(predictions=final_test_predictions_str_keys, references=test_references)\n",
    "        print(\"\\nTest Set Performance (EM, F1):\"); print(test_metrics)\n",
    "        test_metrics_log = {f\"test_{k}\": v for k,v in test_metrics.items()}\n",
    "        if test_predictions_output.metrics is not None and 'test_loss' in test_predictions_output.metrics:\n",
    "             test_metrics_log['test_loss'] = test_predictions_output.metrics['test_loss']\n",
    "             if np.isnan(test_metrics_log['test_loss']): print(\"\\nWARNING: test_loss is NaN.\\n\")\n",
    "        print(\"Metrics logged for test set:\"); print(test_metrics_log)\n",
    "        trainer.log_metrics(\"test\", test_metrics_log)\n",
    "        trainer.save_metrics(\"test\", test_metrics_log)\n",
    "    else: print(\"Skipping test metric calculation due to empty predictions or references.\")\n",
    "except Exception as predict_error: print(f\"!!! ERROR during prediction/test evaluation: {predict_error}\"); traceback.print_exc()\n",
    "\n",
    "\n",
    "# --- Fertility and Token Metrics ---\n",
    "print(\"\\n--- Calculating Fertility & Token Metrics ---\")\n",
    "# Import necessary libraries (ensure pandas/collections/statistics are available)\n",
    "import numpy as np\n",
    "import time # Already imported earlier\n",
    "import collections # Added for Counter fallback\n",
    "import statistics # Added for median fallback\n",
    "try:\n",
    "    import pandas as pd\n",
    "    PANDAS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Pandas not found. Using collections.Counter for distribution.\")\n",
    "    PANDAS_AVAILABLE = False\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    NLTK_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"NLTK not found. Skipping OOV calculation.\")\n",
    "    NLTK_AVAILABLE = False\n",
    "\n",
    "\n",
    "# Check if datasets are valid before proceeding\n",
    "if 'train' not in tokenized_datasets or not tokenized_datasets['train'] or \\\n",
    "   'train' not in raw_datasets or not raw_datasets['train']:\n",
    "    print(\"Error: Training datasets are empty or missing. Cannot calculate fertility/token metrics.\")\n",
    "else:\n",
    "    # --- Existing Token Length Distribution (Overall Features) ---\n",
    "    all_input_ids = tokenized_datasets['train'][\"input_ids\"]\n",
    "    token_lengths = [len(ids) for ids in all_input_ids]\n",
    "    print(\"\\n--- Overall Token Length Distribution (Training Set Features) ---\")\n",
    "    if token_lengths:\n",
    "        min_len, max_len, avg_token_count, median_len = np.min(token_lengths), np.max(token_lengths), np.mean(token_lengths), np.median(token_lengths)\n",
    "        total_tokens_overall = sum(token_lengths) # Renamed to avoid confusion\n",
    "        print(f\"Min Token Length per Feature: {min_len}\")\n",
    "        print(f\"Max Token Length per Feature: {max_len}\")\n",
    "        print(f\"Mean Token Length per Feature: {avg_token_count:.2f}\")\n",
    "        print(f\"Median Token Length per Feature: {median_len}\")\n",
    "        print(f\"Total Tokens Generated (all features): {total_tokens_overall}\") # Use renamed variable\n",
    "    else:\n",
    "        avg_token_count, total_tokens_overall = 0, 0\n",
    "        print(\"No token lengths found for overall features.\")\n",
    "\n",
    "    # --- *** NEW: Answer Token Length Distribution *** ---\n",
    "    print(\"\\n--- Answer Token Length Distribution (Training Set Features) ---\")\n",
    "    answer_token_lengths = []\n",
    "    # Ensure required columns exist\n",
    "    required_cols = ['start_positions', 'end_positions', 'input_ids']\n",
    "    if not all(col in tokenized_datasets['train'].column_names for col in required_cols):\n",
    "        print(f\"Error: Missing required columns for answer token length calculation: \"\n",
    "              f\"{[col for col in required_cols if col not in tokenized_datasets['train'].column_names]}\")\n",
    "    else:\n",
    "        start_positions = tokenized_datasets['train']['start_positions']\n",
    "        end_positions = tokenized_datasets['train']['end_positions']\n",
    "        input_ids_list = tokenized_datasets['train']['input_ids'] # Get all input_ids\n",
    "        cls_token_id = tokenizer.cls_token_id if tokenizer.cls_token is not None else None\n",
    "\n",
    "        skipped_features = 0\n",
    "        for i in range(len(start_positions)):\n",
    "            start_tok = start_positions[i]\n",
    "            end_tok = end_positions[i]\n",
    "\n",
    "            # Determine the CLS index for *this specific feature*\n",
    "            # This handles cases where CLS might not be at index 0 if tokenizer behaves differently\n",
    "            # or if input_ids were manipulated (unlikely here but safer)\n",
    "            current_cls_index = -1 # Default to invalid index\n",
    "            if cls_token_id is not None:\n",
    "                try:\n",
    "                    current_cls_index = input_ids_list[i].index(cls_token_id)\n",
    "                except ValueError:\n",
    "                    # This feature doesn't contain the CLS token? Very unlikely for QA tasks.\n",
    "                    # If this happens, we can't reliably tell if start/end == cls_index means \"no answer\".\n",
    "                    # Let's assume 0 is the fallback if cls_token_id is None or not found.\n",
    "                    current_cls_index = 0 # Fallback assumption\n",
    "\n",
    "            # Check if start/end positions point to the CLS index (meaning no answer span in this feature)\n",
    "            # OR if end < start (invalid span)\n",
    "            if (start_tok == current_cls_index and end_tok == current_cls_index) or end_tok < start_tok:\n",
    "                skipped_features += 1\n",
    "                continue # Skip features where the answer isn't present or span is invalid\n",
    "\n",
    "            # Calculate length: inclusive count of tokens\n",
    "            length = end_tok - start_tok + 1\n",
    "            answer_token_lengths.append(length)\n",
    "\n",
    "        if answer_token_lengths:\n",
    "            print(f\"Calculated token lengths for {len(answer_token_lengths)} answer spans \"\n",
    "                  f\"(skipped {skipped_features} features where answer wasn't fully contained).\")\n",
    "\n",
    "            if PANDAS_AVAILABLE:\n",
    "                s = pd.Series(answer_token_lengths)\n",
    "                print(\"Answer Token Length Counts (Count: Length):\")\n",
    "                # Get value counts, sort by index (length), and print in a format similar to the example\n",
    "                counts_df = s.value_counts().sort_index().reset_index()\n",
    "                counts_df.columns = ['Length', 'Count']\n",
    "                print(counts_df.to_string(index=False)) # Print DataFrame without index\n",
    "\n",
    "                print(f\"\\nMin Answer Token Length: {s.min()}\")\n",
    "                print(f\"Max Answer Token Length: {s.max()}\")\n",
    "                print(f\"Mean Answer Token Length: {s.mean():.2f}\")\n",
    "                print(f\"Median Answer Token Length: {s.median()}\")\n",
    "            else:\n",
    "                # Fallback using collections.Counter and statistics\n",
    "                counts = collections.Counter(answer_token_lengths)\n",
    "                print(\"Answer Token Length Counts (Length: Count):\")\n",
    "                total_answers = len(answer_token_lengths)\n",
    "                print(f\"{'Length':<8} {'Count':<8} {'Percentage':<10}\")\n",
    "                print(\"-\" * 28)\n",
    "                for length, count in sorted(counts.items()):\n",
    "                     percentage = (count / total_answers) * 100\n",
    "                     print(f\"{length:<8} {count:<8} {percentage:>9.2f}%\")\n",
    "                print(\"-\" * 28)\n",
    "\n",
    "                min_len = min(answer_token_lengths)\n",
    "                max_len = max(answer_token_lengths)\n",
    "                mean_len = statistics.mean(answer_token_lengths)\n",
    "                median_len = statistics.median(answer_token_lengths)\n",
    "\n",
    "                print(f\"\\nMin Answer Token Length: {min_len}\")\n",
    "                print(f\"Max Answer Token Length: {max_len}\")\n",
    "                print(f\"Mean Answer Token Length: {mean_len:.2f}\")\n",
    "                print(f\"Median Answer Token Length: {median_len}\")\n",
    "\n",
    "        else:\n",
    "            print(\"No valid answer spans found in the training features to calculate token lengths.\")\n",
    "            if skipped_features > 0:\n",
    "                 print(f\"(Skipped {skipped_features} features where answer wasn't fully contained).\")\n",
    "\n",
    "\n",
    "    # --- Existing Compression Ratio, Vocab Size, OOV ---\n",
    "    # (Keep the rest of the metrics calculations as they were)\n",
    "    # Make sure to use 'total_tokens_overall' if needed here for compression ratio\n",
    "\n",
    "    total_question_chars = sum(len(str(q)) for q in raw_datasets['train']['question'])\n",
    "    total_context_chars = sum(len(str(c)) for c in raw_datasets['train']['context'])\n",
    "    total_chars = total_question_chars + total_context_chars\n",
    "\n",
    "    # Use total_tokens_overall for compression ratio based on *all* generated tokens\n",
    "    compression_ratio = total_chars / total_tokens_overall if total_tokens_overall > 0 else 0\n",
    "    print(f\"\\n--- Compression Ratio (Training Set) ---\")\n",
    "    print(f\"Total Characters (Q+C from raw_datasets): {total_chars}\")\n",
    "    print(f\"Total Tokens Generated (Mapped Features): {total_tokens_overall}\") # Clarified variable name\n",
    "    print(f\"Compression Ratio (Input Chars / Output Tokens): {compression_ratio:.2f}\")\n",
    "\n",
    "    # Vocabulary Size\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    print(f\"\\n--- Vocabulary Size ---\")\n",
    "    print(f\"Tokenizer Vocabulary Size: {vocab_size}\")\n",
    "\n",
    "    # Out-of-Vocabulary (OOV) Rate\n",
    "    if NLTK_AVAILABLE:\n",
    "        try: nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError: print(\"Downloading NLTK punkt tokenizer...\"); nltk.download('punkt', quiet=True)\n",
    "\n",
    "        unk_token_count, total_word_tokens = 0, 0\n",
    "        all_contexts = raw_datasets['train'][\"context\"]\n",
    "        print(f\"\\nChecking OOV rate on {len(all_contexts)} training contexts...\")\n",
    "        for context in all_contexts:\n",
    "            words = word_tokenize(str(context).lower())\n",
    "            total_word_tokens += len(words)\n",
    "            for word in words:\n",
    "                token_ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "                if tokenizer.unk_token_id is not None and tokenizer.unk_token_id in token_ids:\n",
    "                    unk_token_count += token_ids.count(tokenizer.unk_token_id)\n",
    "\n",
    "        oov_rate = (unk_token_count / total_word_tokens) * 100 if total_word_tokens > 0 else 0\n",
    "        print(f\"\\n--- Out-of-Vocabulary (OOV) Rate (Basic Check on Training Context Words) ---\")\n",
    "        print(f\"Total Word Tokens (NLTK): {total_word_tokens}\")\n",
    "        print(f\"Unknown Tokens Found ({tokenizer.unk_token}): {unk_token_count}\")\n",
    "        print(f\"OOV Rate: {oov_rate:.4f}%\")\n",
    "    else:\n",
    "        print(\"\\n--- Out-of-Vocabulary (OOV) Rate ---\")\n",
    "        print(\"Skipped due to NLTK not being available.\")\n",
    "\n",
    "\n",
    "    # --- Existing Token Fertility ---\n",
    "    # (Keep this section as is, using sequence_ids)\n",
    "    print(\"\\n--- Token Fertility (Training Set - Based on Sequence IDs) ---\")\n",
    "    total_q_tokens_from_seq_ids, total_c_tokens_from_seq_ids = 0, 0\n",
    "    print(\"Counting tokens from sequence IDs...\")\n",
    "    if 'sequence_ids' in tokenized_datasets['train'].column_names:\n",
    "        for feature in tokenized_datasets['train']:\n",
    "            sequence_ids = feature.get('sequence_ids') # Use .get for safety\n",
    "            if sequence_ids: # Check if sequence_ids is not None or empty\n",
    "                 # Count tokens based on their sequence ID (0 for question, 1 for context)\n",
    "                 total_q_tokens_from_seq_ids += sum(1 for sid in sequence_ids if sid == 0)\n",
    "                 total_c_tokens_from_seq_ids += sum(1 for sid in sequence_ids if sid == 1)\n",
    "    else:\n",
    "        print(\"Error: 'sequence_ids' column not found in tokenized_datasets['train']. Cannot calculate fertility accurately.\")\n",
    "        total_q_tokens_from_seq_ids = float('nan')\n",
    "        total_c_tokens_from_seq_ids = float('nan')\n",
    "\n",
    "    # Calculate fertility using counts from sequence IDs\n",
    "    question_fertility = total_q_tokens_from_seq_ids / total_question_chars if total_question_chars > 0 else 0\n",
    "    context_fertility = total_c_tokens_from_seq_ids / total_context_chars if total_context_chars > 0 else 0\n",
    "\n",
    "    # Avoid calculating overall if components failed\n",
    "    if not np.isnan(total_q_tokens_from_seq_ids) and not np.isnan(total_c_tokens_from_seq_ids):\n",
    "        total_q_c_tokens_seq_ids = total_q_tokens_from_seq_ids + total_c_tokens_from_seq_ids\n",
    "        overall_fertility = total_q_c_tokens_seq_ids / total_chars if total_chars > 0 else 0\n",
    "    else:\n",
    "        total_q_c_tokens_seq_ids = float('nan')\n",
    "        overall_fertility = float('nan')\n",
    "\n",
    "    print(f\"Total Question Tokens (from sequence_ids): {total_q_tokens_from_seq_ids}\")\n",
    "    print(f\"Total Question Chars (from raw_datasets): {total_question_chars}\")\n",
    "    print(f\"  -> Question Fertility (Tokens/Char): {question_fertility:.4f}\")\n",
    "\n",
    "    print(f\"\\nTotal Context Tokens (from sequence_ids): {total_c_tokens_from_seq_ids}\")\n",
    "    print(f\"Total Context Chars (from raw_datasets): {total_context_chars}\")\n",
    "    print(f\"  -> Context Fertility (Tokens/Char): {context_fertility:.4f}\")\n",
    "\n",
    "    print(f\"\\nTotal Q+C Tokens (from sequence_ids): {total_q_c_tokens_seq_ids}\")\n",
    "    print(f\"Total Q+C Chars (from raw_datasets): {total_chars}\")\n",
    "    print(f\"  -> Overall Fertility (Tokens/Char): {overall_fertility:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TE-49nfD3gEH"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "3e061a237c9640bcb9b1c47629509f4e",
      "0acf6292c3e444a882b5d56ff4029ff6",
      "b782595dd4d6480390eddc06b053d1c5",
      "ad330e66a2684084858e23cac899bbf5",
      "1b18445fef43475b91f5dfebd82e2a89",
      "6389cdd811224f6e82526867da7c4156",
      "f1b438d9fcad4676b7bbb14dcb98d75c",
      "d20cc7f181fd46e68317cbee4a9aded0",
      "78e002284d2545e6b18d61d061e836d7",
      "aec16557b064403ea77bdb8406a3ee29",
      "3c64cb24183d49c3bcccdc370bf2e632"
     ]
    },
    "executionInfo": {
     "elapsed": 9096,
     "status": "ok",
     "timestamp": 1743469485101,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "YU8VdFBTaGaY",
    "outputId": "a6c238a2-b799-42e8-813f-27fe11484b49"
   },
   "outputs": [],
   "source": [
    "# Set model name\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# BitsAndBytes quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load QA model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "tvFhjbtxaStd"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# Format and tokenize datasets\n",
    "def format_qa_prompt(row):\n",
    "    prompt = f\"Context: {row['context']} Question: {row['question']} Answer:\"\n",
    "    return {\"text\": prompt, \"labels\": row[\"answer\"]}\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train_qa)\n",
    "dataset_train = dataset_train.map(format_qa_prompt)\n",
    "\n",
    "dataset_val = Dataset.from_pandas(df_val_qa)\n",
    "dataset_val = dataset_val.map(format_qa_prompt)\n",
    "\n",
    "dataset_test = Dataset.from_pandas(df_test_qa)\n",
    "dataset_test = dataset_test.map(format_qa_prompt)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=1024)\n",
    "    tokenized_labels = tokenizer(examples[\"labels\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    return {\"input_ids\": tokenized_inputs[\"input_ids\"], \"labels\": tokenized_labels[\"input_ids\"]}\n",
    "\n",
    "# Set the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenized_datasets_train = dataset_train.map(tokenize_function, batched=True)\n",
    "tokenized_datasets_val = dataset_val.map(tokenize_function, batched=True)\n",
    "tokenized_datasets_test = dataset_test.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"Tokenization Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "datX90P07T2K"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# Format and tokenize datasets\n",
    "def format_qa_prompt(example):\n",
    "    full_text = f\"Context: {example['context']}\\nQuestion: {example['question']}\\nAnswer: {example['answer']}\"\n",
    "    return {\"text\": full_text}\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train_qa).map(format_qa_prompt)\n",
    "dataset_val = Dataset.from_pandas(df_val_qa).map(format_qa_prompt)\n",
    "dataset_test = Dataset.from_pandas(df_test_qa).map(format_qa_prompt)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets_train = dataset_train.map(tokenize_function, batched=True, remove_columns=dataset_train.column_names)\n",
    "tokenized_datasets_val = dataset_val.map(tokenize_function, batched=True, remove_columns=dataset_val.column_names)\n",
    "tokenized_datasets_test = dataset_test.map(tokenize_function, batched=True, remove_columns=dataset_test.column_names)\n",
    "\n",
    "print(\"Tokenization Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "DLWig1iVp_Cr"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# Token fertility (tokens per character)\n",
    "def calculate_token_fertility(text, tokens):\n",
    "    return len(tokens) / len(text) if len(text) > 0 else 0\n",
    "\n",
    "# Token metrics (token count, token fertility by context and question)\n",
    "def compute_token_metrics(encoded_texts, decoded_texts, original_contexts, original_questions):\n",
    "    encoded_lengths = [len(tokens) for tokens in encoded_texts[\"input_ids\"]]\n",
    "    decoded_lengths = [len(text) for text in decoded_texts]\n",
    "\n",
    "    context_fertilities = []\n",
    "    question_fertilities = []\n",
    "    answer_fertilities = []\n",
    "\n",
    "    for context, question, answer, encoded_length in zip(original_contexts, original_questions, decoded_texts, encoded_lengths):\n",
    "        # Approximate context and question lengths\n",
    "        context_length = len(context)\n",
    "        question_length = len(question)\n",
    "        answer_length = len(answer)\n",
    "\n",
    "        # Calculate approximate token lengths\n",
    "        context_tokens = int(encoded_length * (context_length / (context_length + question_length + answer_length))) if (context_length + question_length + answer_length) > 0 else 0\n",
    "        question_tokens = int(encoded_length * (question_length / (context_length + question_length + answer_length))) if (context_length + question_length + answer_length) > 0 else 0\n",
    "        answer_tokens = encoded_length - context_tokens - question_tokens\n",
    "\n",
    "        context_fertility = calculate_token_fertility(context, range(context_tokens))\n",
    "        question_fertility = calculate_token_fertility(question, range(question_tokens))\n",
    "        answer_fertility = calculate_token_fertility(answer, range(answer_tokens))\n",
    "\n",
    "        context_fertilities.append(context_fertility)\n",
    "        question_fertilities.append(question_fertility)\n",
    "        answer_fertilities.append(answer_fertility)\n",
    "\n",
    "    avg_context_fertility = np.mean(context_fertilities) if context_fertilities else 0\n",
    "    avg_question_fertility = np.mean(question_fertilities) if question_fertilities else 0\n",
    "    avg_answer_fertility = np.mean(answer_fertilities) if answer_fertilities else 0\n",
    "\n",
    "    return {\n",
    "        \"token_count\": np.mean(encoded_lengths),\n",
    "        \"avg_context_fertility\": avg_context_fertility,\n",
    "        \"avg_question_fertility\": avg_question_fertility,\n",
    "        \"avg_answer_fertility\": avg_answer_fertility,\n",
    "        \"fertility\": np.mean([avg_context_fertility, avg_question_fertility, avg_answer_fertility])\n",
    "    }\n",
    "\n",
    "def compute_time_metrics(start_time, end_time):\n",
    "    return {\"time\": end_time - start_time}\n",
    "\n",
    "# Rouge and Bleu metrics\n",
    "rouge = load(\"rouge\")\n",
    "bleu = load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(decoded_labels, skip_special_tokens=True)\n",
    "\n",
    "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bleu_result = bleu.compute(predictions=[decoded_preds], references=[decoded_labels])\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": rouge_result[\"rouge1\"],\n",
    "        \"rouge2\": rouge_result[\"rouge2\"],\n",
    "        \"rougeL\": rouge_result[\"rougeL\"],\n",
    "        \"bleu\": bleu_result[\"score\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 859,
     "referenced_widgets": [
      "f7b98c994a9248a1ac9fe2df47434c5b",
      "d008ce8006c647b3b7e42f0b534519fe",
      "741eaaac546d48b0b6d454aacd1f14f6",
      "411d1ebd329d48c1b872343318332e26",
      "ebdd17b59ced4818a91d4ed7ac8aa4d3",
      "969a48ec64614d4ebf72c1db67eb527a",
      "d420ac52f9bd45038464d6793c2220c0",
      "705d8b7c8b714c4e8ec71276ea8dcc2e",
      "b5f3b19a210f49cfadd7c0f49e2081fa",
      "5ca85b6a4b55427b8084a4dae7fd8769",
      "e7b317bd3ab241899320af6e960d3265",
      "6053d0eb1cf7401e8ac6c136e2f01f76",
      "c8ab0a618cf24dfe8015735aaefe1f46",
      "18acb3cd894b4cfc9be5ae3b6068f74b",
      "29ee08681db64da9978928cc96aea2c6",
      "ca2b5f2db14443dda906e9b983a24a0e",
      "e7388faabbfc45ab8e75cd3c55ac4c17",
      "e3cf7398b2954b78b71d277bb6862576",
      "595808c3805441b8874d0fbe6eb844a2",
      "fdbd56a41ddf4bfdbf7f97a59dd23574",
      "1c3c81441cfd48279d57d20757f17e3e",
      "9b40756cd63f4b75b4edecc69ac5636d",
      "45d2d13841c14891b071c0789a392609",
      "cf4bd20a0078432287c059947ffe7953",
      "dd856d8391394af780fc1e9051d9e983",
      "328b337dac624bbbb8fbc9ff43599bb0",
      "52ec209bdf5c4f3a98636e7b09ac4184",
      "2d2e03bb7feb43e6bf557bdd5d779bf1",
      "3784e29421fc48a5a13226db8c82302e",
      "97c6d588f4cf4f0c83768284c735fe8b",
      "c5435e76f91b4a368e0a443104e272b8",
      "abb0123f43374cc781b581c7c88702fd",
      "6bd9ecf73277483aba00f3db05479553",
      "85e2031f42a04be0b6a06629dd35f54e",
      "b9cd62829da7496aae3ffb0567052024",
      "3e80ca08df0c484da3d7dbaeaf274eae",
      "369bd569ec4d4b8cbbd1ea207db8bf1f",
      "a94906769fe64b2bbde2e4ec53cb16ef",
      "aaa345ceec7644ab81eaacd3d3ee0fb5",
      "8dd15d826e744910b74336e410002df3",
      "0a6e0f38424445fe8026792755420d55",
      "c62aded213f14b0dbcfc4a4b729dc7ad",
      "b53d93a72b4f48dcb4793865723b836e",
      "4e8aedeaf5304402b3a60db9bbd362f9"
     ]
    },
    "executionInfo": {
     "elapsed": 1501044,
     "status": "ok",
     "timestamp": 1743476231474,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "VSBzT0ZcJNue",
    "outputId": "6f2300ea-59e4-416c-e8c0-4c10fef4bf1d"
   },
   "outputs": [],
   "source": [
    "# Prepare model for LoRA training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Optionally add LoRA\n",
    "use_lora = True\n",
    "if use_lora:\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.Question_Answering,\n",
    "        inference_mode=False,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    gradient_checkpointing=True,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=None,\n",
    "    optim=\"adamw_torch\",\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\"\n",
    ")\n",
    "\n",
    "# Reduce dataset size to 100 samples\n",
    "df_train_qa = df_train_qa.sample(n=100, random_state=42)\n",
    "df_val_qa = df_val_qa.sample(n=100, random_state=42)\n",
    "df_test_qa = df_test_qa.sample(n=100, random_state=42)\n",
    "\n",
    "# QA data formatting (SQuAD-style)\n",
    "def prepare_features(example):\n",
    "    tokenized = tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=512,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    start_char = example[\"answer_start\"]\n",
    "    end_char = start_char + len(example[\"answer\"])\n",
    "\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "    sequence_ids = tokenized.sequence_ids()\n",
    "\n",
    "    start_token = 0\n",
    "    end_token = 0\n",
    "    for idx, (start, end) in enumerate(offset_mapping):\n",
    "        if sequence_ids[idx] != 1:\n",
    "            continue\n",
    "        if start <= start_char < end:\n",
    "            start_token = idx\n",
    "        if start < end_char <= end:\n",
    "            end_token = idx\n",
    "            break\n",
    "\n",
    "    tokenized[\"start_positions\"] = start_token\n",
    "    tokenized[\"end_positions\"] = end_token\n",
    "    return tokenized\n",
    "\n",
    "# Map preprocessing\n",
    "train_dataset = Dataset.from_pandas(df_train_qa).map(prepare_features)\n",
    "val_dataset = Dataset.from_pandas(df_val_qa).map(prepare_features)\n",
    "test_dataset = Dataset.from_pandas(df_test_qa).map(prepare_features)\n",
    "\n",
    "# Data collator\n",
    "data_collator = default_data_collator\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "total_steps = len(train_dataset) * training_args.num_train_epochs // training_args.gradient_accumulation_steps\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0, total_iters=total_steps)\n",
    "\n",
    "# Metrics\n",
    "rouge = load(\"rouge\")\n",
    "bleu = load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions[0], skip_special_tokens=True)\n",
    "    print(\"Sample decoded prediction:\", decoded_preds[0])\n",
    "    decoded_labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(decoded_labels, skip_special_tokens=True)\n",
    "\n",
    "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bleu_result = bleu.compute(predictions=[decoded_preds], references=[decoded_labels])\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": rouge_result[\"rouge1\"],\n",
    "        \"rouge2\": rouge_result[\"rouge2\"],\n",
    "        \"rougeL\": rouge_result[\"rougeL\"],\n",
    "        \"bleu\": bleu_result[\"score\"],\n",
    "    }\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, scheduler)\n",
    ")\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "end_time = time.time()\n",
    "print(f\"Training Time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Evaluation\n",
    "start_eval = time.time()\n",
    "eval_results = trainer.evaluate()\n",
    "end_eval = time.time()\n",
    "print(f\"Evaluation Time: {end_eval - start_eval:.2f} seconds\")\n",
    "print(eval_results)\n",
    "\n",
    "# Predict on test set\n",
    "start_test = time.time()\n",
    "test_results = trainer.predict(test_dataset)\n",
    "end_test = time.time()\n",
    "print(f\"Test Inference Time: {end_test - start_test:.2f} seconds\")\n",
    "print(test_results)\n",
    "\n",
    "# Fertility Metrics\n",
    "encoded = train_dataset[\"input_ids\"]\n",
    "decoded = tokenizer.batch_decode(encoded, skip_special_tokens=True)\n",
    "\n",
    "def calculate_token_fertility(text, tokens):\n",
    "    return len(tokens) / len(text) if len(text) > 0 else 0\n",
    "\n",
    "def compute_token_metrics(encoded_texts, decoded_texts, original_contexts, original_questions):\n",
    "    encoded_lengths = [len(tokens) for tokens in encoded_texts]\n",
    "    decoded_lengths = [len(text) for text in decoded_texts]\n",
    "\n",
    "    context_fertilities = []\n",
    "    question_fertilities = []\n",
    "    answer_fertilities = []\n",
    "\n",
    "    for context, question, answer, encoded_length in zip(original_contexts, original_questions, decoded_texts, encoded_lengths):\n",
    "        context_length = len(context)\n",
    "        question_length = len(question)\n",
    "        answer_length = len(answer)\n",
    "        total_length = context_length + question_length + answer_length\n",
    "\n",
    "        if total_length == 0:\n",
    "            context_tokens = question_tokens = answer_tokens = 0\n",
    "        else:\n",
    "            context_tokens = int(encoded_length * (context_length / total_length))\n",
    "            question_tokens = int(encoded_length * (question_length / total_length))\n",
    "            answer_tokens = encoded_length - context_tokens - question_tokens\n",
    "\n",
    "        context_fertilities.append(calculate_token_fertility(context, range(context_tokens)))\n",
    "        question_fertilities.append(calculate_token_fertility(question, range(question_tokens)))\n",
    "        answer_fertilities.append(calculate_token_fertility(answer, range(answer_tokens)))\n",
    "\n",
    "    return {\n",
    "        \"token_count\": np.mean(encoded_lengths),\n",
    "        \"avg_context_fertility\": np.mean(context_fertilities),\n",
    "        \"avg_question_fertility\": np.mean(question_fertilities),\n",
    "        \"avg_answer_fertility\": np.mean(answer_fertilities),\n",
    "        \"fertility\": np.mean([np.mean(context_fertilities), np.mean(question_fertilities), np.mean(answer_fertilities)])\n",
    "    }\n",
    "\n",
    "metrics = compute_token_metrics(\n",
    "    encoded_texts=train_dataset[\"input_ids\"],\n",
    "    decoded_texts=decoded,\n",
    "    original_contexts=df_train_qa[\"context\"].tolist(),\n",
    "    original_questions=df_train_qa[\"question\"].tolist()\n",
    ")\n",
    "\n",
    "print(\"\\n--- Token Fertility Metrics ---\")\n",
    "print(f\"Avg Token Count: {metrics['token_count']:.2f}\")\n",
    "print(f\"Avg Context Fertility: {metrics['avg_context_fertility']:.4f}\")\n",
    "print(f\"Avg Question Fertility: {metrics['avg_question_fertility']:.4f}\")\n",
    "print(f\"Avg Answer Fertility: {metrics['avg_answer_fertility']:.4f}\")\n",
    "print(f\"Overall Fertility: {metrics['fertility']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d06e03ffd44e438587a22ceed1882641",
      "9b43ab2eddbe4922bf0c5621d012183b",
      "103185a7733c4f83a26cc6777bbccd02",
      "6ec163d3585040c6b21a6cbf87716f67",
      "ba5aaae5916d4209af0a100258048f43",
      "c45f42e5ce7f4a1ea5537ac55285a00b",
      "065a24b6647d492ab94efe914c296931",
      "7551400eb1c54bfcbaeeecfec69a5e16",
      "e0cfb83323b04784b624b6f482be78e6",
      "e8e07c51e0814c2ebb2a455528da2c2c",
      "f34d47828f9e49e8872799189bbd38b2",
      "3611df6a78474df7a0c9deda23b4f0c2",
      "58c88862c0254daab9d97f5e7286155f",
      "00cfa86ad0b146ffb400abe9c4b295bb",
      "f6a3d2bc72744a2a92824a5f7a59f0db",
      "20c870740796481cb7c05b07d119e2e5",
      "6fcaebb2066742399231f17918479007",
      "039081dcb57543c4828c80cf5f51b3ed",
      "35f52b2627e646b085e65a0b0fc1b5f4",
      "638b28d5114a4cb9969b4ac5fcf38c92",
      "ec027e0154804133b159d7c42d87868a",
      "a72d9bae354540199da31b1c6e739a8a",
      "00f3d601482a4a0f8301630272bfcdbe",
      "0387db19c83e465fae928aa40ce0ac55",
      "629e11b69f1940258c459ad8fc04ed55",
      "95f2d005ab9f42989c37094307976dfc",
      "2ed5081d81ac46d39e1593e1f969781c",
      "4b7330d931814ee2b2f61b6d63750670",
      "6efe67edd8c14ce5ade905a02c41d59c",
      "73d031d6fc5b4e17a29701b8607ff7c9",
      "33cbf5e6c9804d5ba732d3e24c6f9407",
      "3bc07d29b054412bb54d3caa38da135f",
      "167e51d1aede4cd6beb508a22a099bbb"
     ]
    },
    "executionInfo": {
     "elapsed": 19005,
     "status": "ok",
     "timestamp": 1743481267685,
     "user": {
      "displayName": "Michelle Anil",
      "userId": "17408398605278605946"
     },
     "user_tz": -60
    },
    "id": "7ImoI26MeDkJ",
    "outputId": "cd9285f8-9595-45e4-d947-e82b96cb9e80"
   },
   "outputs": [],
   "source": [
    "class QuestionAnsweringTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            labels=inputs[\"labels\"]  # This is required for causal LM loss\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Add this line\n",
    "\n",
    "\n",
    "def tokenize(example):\n",
    "    model_inputs = tokenizer(example[\"question\"], example[\"context\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Prepare model for LoRA training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Optionally add LoRA\n",
    "use_lora = True\n",
    "if use_lora:\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        inference_mode=False,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    gradient_checkpointing=True,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=None,\n",
    "    optim=\"adamw_torch\",\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    label_names=[\"start_positions\", \"end_positions\"]\n",
    "\n",
    ")\n",
    "\n",
    "# Reduce dataset size to 100 samples\n",
    "df_train_qa = df_train_qa.sample(n=10, random_state=42)\n",
    "df_val_qa = df_val_qa.sample(n=10, random_state=42)\n",
    "df_test_qa = df_test_qa.sample(n=10, random_state=42)\n",
    "\n",
    "# QA data formatting (SQuAD-style)\n",
    "def prepare_features(example):\n",
    "    prompt = f\"Context: {example['context']}\\nQuestion: {example['question']}\\nAnswer:\"\n",
    "    target = example['answer']\n",
    "\n",
    "    tokenized_input = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    tokenized_label = tokenizer(\n",
    "        target,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    tokenized_input[\"labels\"] = tokenized_label[\"input_ids\"]\n",
    "    return tokenized_input\n",
    "\n",
    "\n",
    "# Map preprocessing\n",
    "train_dataset = Dataset.from_pandas(df_train_qa).map(\n",
    "    prepare_features,\n",
    "    batched=False,\n",
    "    remove_columns=df_train_qa.columns.tolist()\n",
    ")\n",
    "\n",
    "val_dataset = Dataset.from_pandas(df_val_qa).map(\n",
    "    prepare_features,\n",
    "    batched=False,\n",
    "    remove_columns=df_val_qa.columns.tolist()\n",
    ")\n",
    "\n",
    "test_dataset = Dataset.from_pandas(df_test_qa).map(\n",
    "    prepare_features,\n",
    "    batched=False,\n",
    "    remove_columns=df_test_qa.columns.tolist()\n",
    ")\n",
    "\n",
    "\n",
    "# Data collator\n",
    "data_collator = default_data_collator\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "total_steps = len(train_dataset) * training_args.num_train_epochs // training_args.gradient_accumulation_steps\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0, total_iters=total_steps)\n",
    "\n",
    "# Metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "# Metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Clean up whitespaces\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    from evaluate import load\n",
    "    rouge = load(\"rouge\")\n",
    "    bleu = load(\"sacrebleu\")\n",
    "\n",
    "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bleu_result = bleu.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": rouge_result[\"rouge1\"],\n",
    "        \"rouge2\": rouge_result[\"rouge2\"],\n",
    "        \"rougeL\": rouge_result[\"rougeL\"],\n",
    "        \"bleu\": bleu_result[\"score\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Trainer\n",
    "trainer = QuestionAnsweringTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, scheduler)\n",
    ")\n",
    "\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "end_time = time.time()\n",
    "print(f\"Training Time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Evaluation\n",
    "start_eval = time.time()\n",
    "eval_results = trainer.evaluate()\n",
    "end_eval = time.time()\n",
    "print(f\"Evaluation Time: {end_eval - start_eval:.2f} seconds\")\n",
    "\n",
    "print(\"\\n--- Evaluation Metrics ---\")\n",
    "for k, v in eval_results.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# Predict on test set\n",
    "start_test = time.time()\n",
    "test_results = trainer.predict(test_dataset)\n",
    "end_test = time.time()\n",
    "print(f\"Test Inference Time: {end_test - start_test:.2f} seconds\")\n",
    "print(test_results)\n",
    "\n",
    "# Fertility Metrics\n",
    "encoded = train_dataset[\"input_ids\"]\n",
    "decoded = tokenizer.batch_decode(encoded, skip_special_tokens=True)\n",
    "\n",
    "def calculate_token_fertility(text, tokens):\n",
    "    return len(tokens) / len(text) if len(text) > 0 else 0\n",
    "\n",
    "def compute_token_metrics(encoded_texts, decoded_texts, original_contexts, original_questions):\n",
    "    encoded_lengths = [len(tokens) for tokens in encoded_texts]\n",
    "    decoded_lengths = [len(text) for text in decoded_texts]\n",
    "\n",
    "    context_fertilities = []\n",
    "    question_fertilities = []\n",
    "    answer_fertilities = []\n",
    "\n",
    "    for context, question, answer, encoded_length in zip(original_contexts, original_questions, decoded_texts, encoded_lengths):\n",
    "        context_length = len(context)\n",
    "        question_length = len(question)\n",
    "        answer_length = len(answer)\n",
    "        total_length = context_length + question_length + answer_length\n",
    "\n",
    "        if total_length == 0:\n",
    "            context_tokens = question_tokens = answer_tokens = 0\n",
    "        else:\n",
    "            context_tokens = int(encoded_length * (context_length / total_length))\n",
    "            question_tokens = int(encoded_length * (question_length / total_length))\n",
    "            answer_tokens = encoded_length - context_tokens - question_tokens\n",
    "\n",
    "        context_fertilities.append(calculate_token_fertility(context, range(context_tokens)))\n",
    "        question_fertilities.append(calculate_token_fertility(question, range(question_tokens)))\n",
    "        answer_fertilities.append(calculate_token_fertility(answer, range(answer_tokens)))\n",
    "\n",
    "    return {\n",
    "        \"token_count\": np.mean(encoded_lengths),\n",
    "        \"avg_context_fertility\": np.mean(context_fertilities),\n",
    "        \"avg_question_fertility\": np.mean(question_fertilities),\n",
    "        \"avg_answer_fertility\": np.mean(answer_fertilities),\n",
    "        \"fertility\": np.mean([np.mean(context_fertilities), np.mean(question_fertilities), np.mean(answer_fertilities)])\n",
    "    }\n",
    "\n",
    "metrics = compute_token_metrics(\n",
    "    encoded_texts=train_dataset[\"input_ids\"],\n",
    "    decoded_texts=decoded,\n",
    "    original_contexts=df_train_qa[\"context\"].tolist(),\n",
    "    original_questions=df_train_qa[\"question\"].tolist()\n",
    ")\n",
    "\n",
    "print(\"\\n--- Token Fertility Metrics ---\")\n",
    "print(f\"Avg Token Count: {metrics['token_count']:.2f}\")\n",
    "print(f\"Avg Context Fertility: {metrics['avg_context_fertility']:.4f}\")\n",
    "print(f\"Avg Question Fertility: {metrics['avg_question_fertility']:.4f}\")\n",
    "print(f\"Avg Answer Fertility: {metrics['avg_answer_fertility']:.4f}\")\n",
    "print(f\"Overall Fertility: {metrics['fertility']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NYIxAhEaRXa"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "NO2P7VmkAFDC"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "!pip install --upgrade huggingface_hub\n",
    "!pip install transformers==4.49.0\n",
    "!pip install accelerate\n",
    "!pip install peft==0.5.0\n",
    "!pip install datasets\n",
    "!pip install bitsandbytes==0.38.2\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "I5pJzM0AB4KB"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "!huggingface-cli login\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import html\n",
    "import random\n",
    "import time\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    print(\"CUDA is available! Using GPU.\")\n",
    "else:\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXUDxmSR51Qu"
   },
   "outputs": [],
   "source": [
    "class StanceDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Map stance labels to integers\n",
    "        self.label_map = {'Agree': 0, 'Disagree': 1, 'Discuss': 2, 'Unrelated': 3}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        claim = html.unescape(row['claim'])\n",
    "        article = html.unescape(row['article'])\n",
    "\n",
    "        # Truncate article if needed to fit within max_length\n",
    "        if len(article) > 5000:  # Arbitrary limit to avoid very long sequences\n",
    "            article = article[:5000]\n",
    "\n",
    "        # Combine claim and article\n",
    "        text = f\"Claim: {claim} Article: {article}\"\n",
    "\n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Remove batch dimension added by tokenizer\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "\n",
    "        # Add label\n",
    "        encoding['labels'] = torch.tensor(self.label_map[row['stance']])\n",
    "\n",
    "        return encoding\n",
    "\n",
    "# Basic Tokenization Analysis Function\n",
    "def analyze_tokenization(pairs_df, test_pairs_df, sample_size=100):\n",
    "    \"\"\"Analyze tokenization differences for Llama model.\"\"\"\n",
    "    print(\"Loading tokenizer for tokenization analysis...\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\", use_fast=True)\n",
    "\n",
    "    # Sample data for analysis\n",
    "    combined_df = pd.concat([pairs_df, test_pairs_df])\n",
    "    sample_df = combined_df.sample(sample_size, random_state=42)\n",
    "\n",
    "    print(f\"Analyzing tokenization on {sample_size} samples...\")\n",
    "\n",
    "    # Analyze tokenization\n",
    "    results = []\n",
    "    for _, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "        # Clean HTML entities\n",
    "        claim = html.unescape(row['claim'])\n",
    "        article = html.unescape(row['article'][:500])  # Truncate article for speed\n",
    "\n",
    "        # Prepare text (combine claim and article for stance detection)\n",
    "        text = f\"Claim: {claim} Article: {article}\"\n",
    "\n",
    "        # Tokenize with Llama\n",
    "        llama_tokens = tokenizer.encode(text)\n",
    "\n",
    "        results.append({\n",
    "            'text_length': len(text),\n",
    "            'llama_tokens': len(llama_tokens),\n",
    "            'stance': row['stance']\n",
    "        })\n",
    "\n",
    "    token_df = pd.DataFrame(results)\n",
    "\n",
    "    # Visualize tokenization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(token_df['text_length'], token_df['llama_tokens'], alpha=0.7)\n",
    "    plt.xlabel('Text Length (characters)')\n",
    "    plt.ylabel('Llama 3.1 Token Count')\n",
    "    plt.title('Tokenization Analysis: Llama 3.1')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('tokenization_analysis.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Token statistics\n",
    "    print(\"\\nAverage tokens per example:\")\n",
    "    print(f\"Llama 3.1: {token_df['llama_tokens'].mean():.1f} tokens\")\n",
    "\n",
    "    # Tokens by stance\n",
    "    print(\"\\nTokens by stance category:\")\n",
    "    for stance in token_df['stance'].unique():\n",
    "        subset = token_df[token_df['stance'] == stance]\n",
    "        print(f\"\\n{stance}:\")\n",
    "        print(f\"  Llama 3.1: {subset['llama_tokens'].mean():.1f} tokens\")\n",
    "\n",
    "    return token_df\n",
    "\n",
    "# Enhanced Tokenization Analysis with Fertility Metrics\n",
    "def analyze_tokenization_fertility(pairs_df, test_pairs_df, sample_size=100):\n",
    "    \"\"\"Analyze tokenization fertility and OOV rates for Arabic text with Llama.\"\"\"\n",
    "    print(\"Loading tokenizer for detailed tokenization analysis...\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\", use_fast=True)\n",
    "\n",
    "    # Sample data for analysis\n",
    "    combined_df = pd.concat([pairs_df, test_pairs_df])\n",
    "    sample_df = combined_df.sample(sample_size, random_state=42)\n",
    "\n",
    "    # Initialize metrics\n",
    "    results = []\n",
    "\n",
    "    for _, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Analyzing token fertility\"):\n",
    "        # Clean text\n",
    "        claim = html.unescape(row['claim'])\n",
    "        article = html.unescape(row['article'][:500])  # Truncate for speed\n",
    "\n",
    "        # Tokenize\n",
    "        claim_tokens = tokenizer.encode(claim, add_special_tokens=False)\n",
    "        article_tokens = tokenizer.encode(article, add_special_tokens=False)\n",
    "\n",
    "        # Calculate fertility (tokens per character)\n",
    "        claim_fertility = len(claim_tokens) / len(claim) if len(claim) > 0 else 0\n",
    "        article_fertility = len(article_tokens) / len(article) if len(article) > 0 else 0\n",
    "\n",
    "        # Calculate fragmentation (tokens per word) - rough estimate for Arabic\n",
    "        claim_words = len(claim.split())\n",
    "        article_words = len(article.split())\n",
    "        claim_fragmentation = len(claim_tokens) / claim_words if claim_words > 0 else 0\n",
    "        article_fragmentation = len(article_tokens) / article_words if article_words > 0 else 0\n",
    "\n",
    "        # Estimate OOV by checking for multi-token words\n",
    "        # In Arabic, words broken into many tokens often indicate OOV issues\n",
    "        claim_words_list = claim.split()\n",
    "        article_words_list = article.split()\n",
    "\n",
    "        # Track highly fragmented words (potential OOVs)\n",
    "        highly_fragmented_words = []\n",
    "        for word in claim_words_list + article_words_list:\n",
    "            if len(word) >= 3:  # Only check non-trivial words\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                if len(word_tokens) >= 3:  # If a word is broken into 3+ tokens\n",
    "                    highly_fragmented_words.append(word)\n",
    "\n",
    "        results.append({\n",
    "            'claim_tokens': len(claim_tokens),\n",
    "            'article_tokens': len(article_tokens),\n",
    "            'claim_fertility': claim_fertility,\n",
    "            'article_fertility': article_fertility,\n",
    "            'claim_fragmentation': claim_fragmentation,\n",
    "            'article_fragmentation': article_fragmentation,\n",
    "            'highly_fragmented_words': len(highly_fragmented_words),\n",
    "            'highly_fragmented_word_examples': highly_fragmented_words[:5] if highly_fragmented_words else [],\n",
    "            'stance': row['stance']\n",
    "        })\n",
    "\n",
    "    token_df = pd.DataFrame(results)\n",
    "\n",
    "    # Display statistics\n",
    "    print(\"\\n=== Tokenization Fertility Analysis ===\")\n",
    "    print(f\"Average claim fertility: {token_df['claim_fertility'].mean():.4f} tokens/char\")\n",
    "    print(f\"Average article fertility: {token_df['article_fertility'].mean():.4f} tokens/char\")\n",
    "    print(f\"Average claim fragmentation: {token_df['claim_fragmentation'].mean():.2f} tokens/word\")\n",
    "    print(f\"Average article fragmentation: {token_df['article_fragmentation'].mean():.2f} tokens/word\")\n",
    "    print(f\"Average highly fragmented words: {token_df['highly_fragmented_words'].mean():.2f} per sample\")\n",
    "\n",
    "    # Print some example highly fragmented words (potential OOVs)\n",
    "    if len([w for row in token_df['highly_fragmented_word_examples'] for w in row]) > 0:\n",
    "        print(\"\\nExample highly fragmented words (potential OOVs):\")\n",
    "        all_examples = [w for row in token_df['highly_fragmented_word_examples'] for w in row]\n",
    "        for word in random.sample(all_examples, min(10, len(all_examples))):\n",
    "            tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "            print(f\"  '{word}' → {len(tokens)} tokens\")\n",
    "\n",
    "    # Visualize fertility metrics\n",
    "    plt.figure(figsize=(16, 12))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.boxplot(x='stance', y='claim_fertility', data=token_df)\n",
    "    plt.title('Claim Fertility by Stance')\n",
    "    plt.ylabel('Tokens per Character')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.boxplot(x='stance', y='article_fertility', data=token_df)\n",
    "    plt.title('Article Fertility by Stance')\n",
    "    plt.ylabel('Tokens per Character')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.boxplot(x='stance', y='claim_fragmentation', data=token_df)\n",
    "    plt.title('Claim Fragmentation by Stance')\n",
    "    plt.ylabel('Tokens per Word')\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.boxplot(x='stance', y='highly_fragmented_words', data=token_df)\n",
    "    plt.title('Potential OOV Words by Stance')\n",
    "    plt.ylabel('Count of Highly Fragmented Words')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tokenization_fertility_analysis.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Analyze token distribution\n",
    "    claim_token_counts = Counter([t for row in sample_df['claim'] for t in tokenizer.encode(row, add_special_tokens=False)])\n",
    "    article_token_counts = Counter([t for row in sample_df['article'].apply(lambda x: x[:500]) for t in tokenizer.encode(row, add_special_tokens=False)])\n",
    "\n",
    "    print(\"\\nToken frequency analysis:\")\n",
    "    print(f\"Unique tokens in claims: {len(claim_token_counts)}\")\n",
    "    print(f\"Unique tokens in articles: {len(article_token_counts)}\")\n",
    "    print(f\"Token overlap: {len(set(claim_token_counts.keys()) & set(article_token_counts.keys()))}\")\n",
    "\n",
    "    # Save results to file\n",
    "    with open('tokenization_analysis_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            'avg_claim_fertility': token_df['claim_fertility'].mean(),\n",
    "            'avg_article_fertility': token_df['article_fertility'].mean(),\n",
    "            'avg_claim_fragmentation': token_df['claim_fragmentation'].mean(),\n",
    "            'avg_article_fragmentation': token_df['article_fragmentation'].mean(),\n",
    "            'avg_oov_count': token_df['highly_fragmented_words'].mean(),\n",
    "            'unique_claim_tokens': len(claim_token_counts),\n",
    "            'unique_article_tokens': len(article_token_counts),\n",
    "            'token_overlap': len(set(claim_token_counts.keys()) & set(article_token_counts.keys()))\n",
    "        }, f, indent=2)\n",
    "\n",
    "    return token_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8f2efb0772d84f53a07413936fe79ac1",
      "e67bb52fdc79455eac3171e779b291eb",
      "d864f8d29b3944749bf141fc073e6684",
      "df23031ec1724a77988d0edf4d4e2775",
      "59e87bcbd4bd4958a9c26b450d908e9d",
      "6c35a78f334f457b899fa0f342b937bb",
      "c8831e7afbee428e863548c7f9e5eb91",
      "01d8a0e99e214d2784a47426e1a8a88f",
      "bef5888a8f0848ee81ec150227e2e778",
      "ead0eabf00a9480fbc7c8fee129de4b7",
      "9fd7cd517bb1451c87626e2b1f458854",
      "2426b8a6b030433c824781e35e135fd4",
      "b70dc12f9b824421b5bc4cf2812bb3ed",
      "0ed995e4a9524bbcb181280e7c9dc073",
      "5c54f9d3843b4ecc96e21f90bb3e0396",
      "e123d8d9aed44670a8b9e7fd76395ab2",
      "3cae3c2e77354834bfb4aa27755d15c1",
      "93008d918b8942e9b90aa960a4b420c6",
      "d86780d78e8f4ad3994dd3bc86ea4eeb",
      "fbfc13c9185546f797b11732de9d3e48",
      "b5c9ef560c93485e8dc6455d66d31c7e",
      "dcf015a910014e838b108eb55759c0b8",
      "a0dae58844ba49af84d41e6e07f3ce91",
      "35a7a5428604429ca591122f77a83582",
      "2331c8ab2a9b402abf9af80e6e3e7a2a",
      "423c68b8926e4518b1fc47436d69de8a",
      "e68fe164ed1b464fb9cd6ef782320a71",
      "c7312f0bbb9e417dba8123ec16a244b3",
      "2b6570eac10a43fca904085e8add82dc",
      "93dccc5d2c054140b61c4f138305c20f",
      "5a2bf2eedfec4f2aa6ab3e2ca9b766bd",
      "ca5195048a4645538f85b1ea289a28ef",
      "420fc8d9267845418d75f7a1b6e6e152",
      "f775174160964a1ab0ed2becc2e8a078",
      "86ca4a63fd624f79af2e4f9ca29e0b53",
      "b6d3c81c699b4379b055af6c301323a3",
      "fe2d889159304f2fa9167cd9933bc11f",
      "1504dc7969bf47cc8c0a60b6e1d8ca6e",
      "9cf11518bb544b2bb4c7bdbe930a31a5",
      "16800bc68dc8432e8c7367ad51f84785",
      "4d35cac1c6fd4c9dab6ef5b477076dbf",
      "7ab9ac4f4b2f49189bad39b422a5e445",
      "2edbea97d8e2496885afc9072d2be15c",
      "8ac7d3a5af9944f8b43b69c1275865bd",
      "b4c301a450c847218801af78a9a639e4",
      "af84a92fceae4bdabb99f44f69028866",
      "2156a488e7d94b3ba546c8efb60e992f",
      "adec393988b14896a2554afd1a7fc065",
      "a47bbef1b91e4950ad386ebf8d8bdbbf",
      "b7b28760cbfe423babd8c55defbdd338",
      "d3f924843eef475ba516c25573e6c864",
      "fcb8688387c349c99667fd727fe128ab",
      "5fbf692a034a4e658ebc103e912bbe8f",
      "02f40d9228454affa4b031138c83b316",
      "a31bad91136e4c75b2d7dc0fd1af8404",
      "30ef9cbd35d04e01837bc79ec29a87a5",
      "6b67c7ed39f84490a666502e934a6bf8",
      "8b8addff887941be96448977cc6177da",
      "e92e92197d45467c981bd372640070c3",
      "9713a83c48af49b3bf550929cf645b33",
      "73939dcd80b64f3993d7d6c94a92cb1f",
      "d17d421bfc81467caee77e6dfd8d92da",
      "87aec4db46244aa5972c0934c1b9df67",
      "389ab12be6ee419a9ec72dbfdf6a1c53",
      "c534cb86bb0f4be19704c85ec72c2630",
      "8a71b9326c954244a4bf3c0ff2c86f71",
      "f5b548030c0146bb9c07f97ef9dc2ccc",
      "2cb894ce830d4740893acf423dd5cc8e",
      "9eec1ac2794648e8bd16aaaec8558d52",
      "275c53e30ad449589cc2b047f5bb9d8d",
      "667384500a814a2891b84e4f67884c69",
      "0376750b9458478182d2610e8b84e0ab",
      "53b9024c1f674bb5862e92462f1245c6",
      "68c0b0125d334449aba5272ef3bd2e77",
      "d93ce73327474bb482001274beaa0ddb",
      "24084fdc0220413f860a8930f56ec38c",
      "75a00e41bfdf4635916878c59a99eb23",
      "6d9274340be440df8d9314ab98eafb79",
      "4482eebae6824cef858190b68bfd62f0",
      "5ee11a5a205e461bad6f5a55991146cc",
      "3a2af74141ec4b418b727c0c0bf8916d",
      "25fc33888b1b46f6bc9a37710e6d8cc3",
      "8a70fb990f1b4648b33c3e27e5083e51",
      "f2df7de490bd448aa1506dad8bbc16cf",
      "f9d3683270ce4d668dc80f443ffee7cd",
      "96ef9cf589fc423199ef7cf898380d02",
      "796dc6688fa5487d8b9785ada6617341",
      "b5a5226d1cb74f8ea35a7f2ac04c4127",
      "9eba994e6e06482887db24ebafd8959e",
      "a2f46a7a2afd4dfb9a05a9e7e09ba579",
      "474615fb297e4e22b4b8721848d29795",
      "828d0dee70894548b9213513032813d9",
      "4ddf5c0fcd3941d69d8e07c476a75e88",
      "9d3bbb4a45c746919798c7838394f88c",
      "8f06b6aab0dc4215b0803ab97edb593a",
      "40b86ee4a7cd41d79c692fb9fbc72a37",
      "f95b63cb49854ee780b2c11978d10e14",
      "b5b8a943960243c99a8766790a05eb09",
      "6caaa442762f4a1aa415b620c346d5da",
      "91d1a69ed47940969f6a11dfd6e1d787",
      "a390e514d6e049a1812b6f82f97c65ef",
      "2cb075363acf4f9cbfab0e4b5b663560",
      "e0a13f5cda434c11a883c616553fe540",
      "dd9b51d059ce460ab1780fad46c734d2",
      "43a9f60388cb4779a184e6d81ab95329",
      "b9f33a6ea6184128bdda59e9e9300a9c",
      "1d6e8cbe303b4d8aa7bcb707f273b2c8",
      "50a47bdf7d6b4687a66fbbb2ba5619a8",
      "f5af128fbb2f4ad2907cb0589de14777",
      "449a80372ad24c16bf195f4a261c7bc3",
      "a29ecbb7a3a04adea1277e1b8f04dd3b",
      "d0f04a7838254f69a2b554edf0e678b6",
      "1ee604e9a3a24a96a3c766ddd556a31f",
      "8a176c5b2fa7400cbd698e89b15e3b7d",
      "91b8378cf8e346f29aff306a72119171",
      "c49cc93b495a4b61b116d4a43bfd1277",
      "619c0dac96864eedb53d42b8ba27e9d7",
      "0cf8d4dd8b7e4718bfe0b3d6445c5ac7",
      "53878390719f42ed8437f5a2a4ea2900",
      "f287fc2425a24e1a80ee2c464cd8d209",
      "8f51f8fbff49487ea04ced7a44097f5e"
     ]
    },
    "id": "n5vVzYUS69wb",
    "outputId": "4e99b85a-dff1-4f7e-e214-8ba8edc080c0"
   },
   "outputs": [],
   "source": [
    "def train_model(model_name, pairs_df, test_pairs_df, output_dir=\"model_outputs\", use_lora=True, epochs=10):\n",
    "    \"\"\"Train and evaluate a stance detection model.\"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Configuration\n",
    "    num_labels = 4\n",
    "    batch_size = 1  # Small batch size due to model size\n",
    "    grad_accum_steps = 16  # Effective batch size = batch_size * grad_accum_steps\n",
    "    learning_rate = 2e-5\n",
    "    max_length = 512\n",
    "    model_save_path = os.path.join(output_dir, f\"{model_name.split('/')[-1]}_stance_detector\")\n",
    "\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Training {model_name} model for stance detection\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Fix for the padding token issue\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"Set EOS token as padding token\")\n",
    "\n",
    "    # Split training data into train and validation\n",
    "    print(\"Preparing datasets...\")\n",
    "    train_df, val_df = train_test_split(\n",
    "        pairs_df,\n",
    "        test_size=0.1,\n",
    "        random_state=42,\n",
    "        stratify=pairs_df['stance']\n",
    "    )\n",
    "\n",
    "    print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_pairs_df)}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = StanceDataset(train_df, tokenizer, max_length)\n",
    "    val_dataset = StanceDataset(val_df, tokenizer, max_length)\n",
    "    test_dataset = StanceDataset(test_pairs_df, tokenizer, max_length)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"Initializing model...\")\n",
    "    if use_lora:\n",
    "        # Load base model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.bfloat16  # Use bfloat16 to save memory\n",
    "        )\n",
    "\n",
    "        # Set padding token id in the model config\n",
    "        if model.config.pad_token_id is None:\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        # Define LoRA configuration\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            inference_mode=False,\n",
    "            r=16,  # rank\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # Include more attention modules\n",
    "        )\n",
    "\n",
    "        # Create PEFT model\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.bfloat16  # Use bfloat16 to save memory\n",
    "        )\n",
    "\n",
    "        # Set padding token id in the model config\n",
    "        if model.config.pad_token_id is None:\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Move model to GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_loader) * epochs // grad_accum_steps\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=total_steps // 10,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_f1s = []\n",
    "    best_val_f1 = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\")):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / grad_accum_steps  # Normalize loss for gradient accumulation\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            train_loss += loss.item() * grad_accum_steps\n",
    "\n",
    "            # Update weights after accumulating gradients\n",
    "            if (step + 1) % grad_accum_steps == 0 or step == len(train_loader) - 1:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        valid_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                labels = batch.pop('labels')\n",
    "\n",
    "                outputs = model(**batch)\n",
    "                # Handle the case where loss is None\n",
    "                if outputs.loss is not None:\n",
    "                    loss = outputs.loss\n",
    "                    val_loss += loss.item()\n",
    "                    valid_batches += 1\n",
    "                else:\n",
    "                    # If loss is None, skip this batch for loss calculation\n",
    "                    pass\n",
    "\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Avoid division by zero if all batches had None loss\n",
    "        if valid_batches > 0:\n",
    "            val_loss /= valid_batches\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Calculate metrics\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        val_precision = precision_score(all_labels, all_preds, average='macro')\n",
    "        val_recall = recall_score(all_labels, all_preds, average='macro')\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Accuracy: {val_acc:.4f}\")\n",
    "        print(f\"  Val F1 (macro): {val_f1:.4f}\")\n",
    "        print(f\"  Val Precision: {val_precision:.4f}\")\n",
    "        print(f\"  Val Recall: {val_recall:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            # Save model\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            model_to_save.save_pretrained(model_save_path)\n",
    "            tokenizer.save_pretrained(model_save_path)\n",
    "            print(f\"  Model saved to {model_save_path}\")\n",
    "\n",
    "        # Early stopping check (optional)\n",
    "        if epoch > 2 and val_losses[-1] > val_losses[-2] and val_losses[-2] > val_losses[-3]:\n",
    "            print(\"Early stopping triggered - validation loss increasing for 3 consecutive epochs\")\n",
    "            break\n",
    "\n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(val_losses, 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_f1s, 'g-', label='Validation F1')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Validation F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{model_name.split(\"/\")[-1]}_training_curve.png'))\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    if use_lora:\n",
    "        # For LoRA we need to load the PEFT model\n",
    "        config = PeftConfig.from_pretrained(model_save_path)\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            config.base_model_name_or_path,\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        # Set padding token id in the model config\n",
    "        if base_model.config.pad_token_id is None:\n",
    "            base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        best_model = PeftModel.from_pretrained(base_model, model_save_path)\n",
    "    else:\n",
    "        best_model = AutoModelForSequenceClassification.from_pretrained(model_save_path)\n",
    "        # Set padding token id in the model config\n",
    "        if best_model.config.pad_token_id is None:\n",
    "            best_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    test_start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = batch.pop('labels')\n",
    "\n",
    "            outputs = best_model(**batch)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    test_time = time.time() - test_start_time\n",
    "\n",
    "    # Calculate metrics\n",
    "    label_names = ['Agree', 'Disagree', 'Discuss', 'Unrelated']\n",
    "    test_acc = accuracy_score(test_labels, test_preds)\n",
    "    test_f1 = f1_score(test_labels, test_preds, average='macro')\n",
    "    test_precision = precision_score(test_labels, test_preds, average='macro')\n",
    "    test_recall = recall_score(test_labels, test_preds, average='macro')\n",
    "    conf_matrix = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "    print(f\"\\nTest Results for {model_name}:\")\n",
    "    print(f\"  Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  F1 (macro): {test_f1:.4f}\")\n",
    "    print(f\"  Precision: {test_precision:.4f}\")\n",
    "    print(f\"  Recall: {test_recall:.4f}\")\n",
    "    print(f\"  Test time: {test_time:.2f} seconds ({test_time/len(test_loader):.2f} seconds per batch)\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix - {model_name.split(\"/\")[-1]}')\n",
    "    plt.savefig(os.path.join(output_dir, f'{model_name.split(\"/\")[-1]}_confusion_matrix.png'))\n",
    "    plt.show()\n",
    "\n",
    "    # Class-wise metrics\n",
    "    class_metrics = {}\n",
    "    print(\"\\nClass-wise metrics:\")\n",
    "    for i, label in enumerate(label_names):\n",
    "        class_precision = precision_score(\n",
    "            [1 if l == i else 0 for l in test_labels],\n",
    "            [1 if p == i else 0 for p in test_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "        class_recall = recall_score(\n",
    "            [1 if l == i else 0 for l in test_labels],\n",
    "            [1 if p == i else 0 for p in test_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "        class_f1 = f1_score(\n",
    "            [1 if l == i else 0 for l in test_labels],\n",
    "            [1 if p == i else 0 for p in test_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "\n",
    "        class_metrics[label] = {\n",
    "            'precision': class_precision,\n",
    "            'recall': class_recall,\n",
    "            'f1': class_f1\n",
    "        }\n",
    "\n",
    "        print(f\"  {label}:\")\n",
    "        print(f\"    Precision: {class_precision:.4f}\")\n",
    "        print(f\"    Recall: {class_recall:.4f}\")\n",
    "        print(f\"    F1: {class_f1:.4f}\")\n",
    "\n",
    "    # Save results to file\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': test_acc,\n",
    "        'f1_macro': test_f1,\n",
    "        'precision_macro': test_precision,\n",
    "        'recall_macro': test_recall,\n",
    "        'confusion_matrix': conf_matrix.tolist(),\n",
    "        'class_metrics': class_metrics,\n",
    "        'test_time': test_time\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(output_dir, f'{model_name.split(\"/\")[-1]}_results.json'), 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Main pipeline function\n",
    "def run_pipeline(pairs_df, test_pairs_df, run_tokenization=True, epochs=10):\n",
    "    \"\"\"Run the stance detection pipeline with Llama 3.1.\"\"\"\n",
    "    # Show dataset information\n",
    "    print(f\"Training set: {len(pairs_df)} claim-article pairs\")\n",
    "    print(\"Training stance distribution:\")\n",
    "    print(pairs_df['stance'].value_counts())\n",
    "    print(f\"\\nTest set: {len(test_pairs_df)} claim-article pairs\")\n",
    "    print(\"Test stance distribution:\")\n",
    "    print(test_pairs_df['stance'].value_counts())\n",
    "\n",
    "    # Analyze tokenization if requested\n",
    "    if run_tokenization:\n",
    "        # Basic tokenization analysis\n",
    "        print(\"\\n=== Running Basic Tokenization Analysis ===\")\n",
    "        token_df = analyze_tokenization(pairs_df, test_pairs_df)\n",
    "\n",
    "        # Enhanced tokenization fertility analysis\n",
    "        print(\"\\n=== Running Enhanced Tokenization Fertility Analysis ===\")\n",
    "        try:\n",
    "            from collections import Counter\n",
    "            fertility_df = analyze_tokenization_fertility(pairs_df, test_pairs_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in tokenization fertility analysis: {e}\")\n",
    "            print(\"Continuing with training...\")\n",
    "\n",
    "    # Train and evaluate Llama model\n",
    "    model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "    results = train_model(model_name, pairs_df, test_pairs_df, epochs=epochs)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the complete pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # This will run the entire pipeline\n",
    "    results = run_pipeline(pairs_df, test_pairs_df, run_tokenization=True, epochs=10)\n",
    "\n",
    "    # Print summary of results\n",
    "    print(\"\\n=== Final Results Summary ===\")\n",
    "    print(f\"Model: {results['model_name']}\")\n",
    "    print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"F1 Score (macro): {results['f1_macro']:.4f}\")\n",
    "    print(f\"Precision (macro): {results['precision_macro']:.4f}\")\n",
    "    print(f\"Recall (macro): {results['recall_macro']:.4f}\")\n",
    "\n",
    "    # Class-specific results\n",
    "    print(\"\\nClass-specific results:\")\n",
    "    for label, metrics in results['class_metrics'].items():\n",
    "        print(f\"  {label}: F1={metrics['f1']:.4f}, Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "lOHNQcQdcZLz"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1t0y_ps59rC6CSl6dXwoqP4R9dWdUrqHj",
     "timestamp": 1743438689595
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
