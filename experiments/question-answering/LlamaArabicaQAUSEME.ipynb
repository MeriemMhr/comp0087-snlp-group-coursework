{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9WCYLo914AM"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZZ5K8oNoH70h",
    "outputId": "7754b4f7-7db4-403c-b739-51f93dd03531"
   },
   "outputs": [],
   "source": [
    "!pip install peft --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CkRtn2YHQBS",
    "outputId": "0f316424-af96-476e-d90b-3f48925d75a1"
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers datasets pandas bitsandbytes accelerate evaluate scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5jZKAP40NYx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator\n",
    ")\n",
    "import torch\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CiN55Oma0Q-S",
    "outputId": "9401e604-6508-4381-f198-48794061750a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jegqt5Cy2dLx",
    "outputId": "b0455540-1768-48e3-d7d6-bcb5f7262cdf"
   },
   "outputs": [],
   "source": [
    "output_dir = \"/content/drive/MyDrive/qa_model_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "model_save_path = os.path.join(output_dir, f\"{model_name.split('/')[-1]}_qa_model\")\n",
    "\n",
    "print(f\"Model will be saved to: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "81653a4773424aaaa3f6adb1093a474d",
      "3024ed9bd2be4c31948e62156d7ab418",
      "8261f96be1544e0289bf9b682b8be018",
      "db1635b53a0340f4a991e38e4f5ee3ee",
      "ff3245babd984796a53642c6888fa45b",
      "9b075f28c57b409baface1ac12b2d116",
      "96daada7e2374a44afa3b7b2740fbe38",
      "55217f0ca3b14576ac5ad974e2537784",
      "5d1b5cc15ddc4ac599f3e9b3ebab7661",
      "9aebd42b368a4b51825ad77863d0bfcd",
      "96a86b8426b14c4f9edbc8d0631e5f0e",
      "d17ca286ab1049c19cd658d11a51a785",
      "7bc8068d53204dc28a54c49b13b2ca86",
      "47746542531b4fbdb90cd137984f9517",
      "770cfd77f8804203a8c5222991ff21ec",
      "8d84ede173324bf7916507fa541a8996",
      "d214182f3c4b476083a95f7b8ab870d6",
      "ccce59027f4743ff9f8f2b25da4ccbc6",
      "580de25b1ad440be86f244408c680885",
      "063dedc6adc34d2ca0e69fa82a3c8b12",
      "ac0c7aa87a774352883de8a18e1a100a",
      "dde8d44f94f743ba8c6ca17ba08205f0",
      "652f39eec7034035808e18d78a8c1d2e",
      "cc6177637fe14d3dbb3ddfc4d62b8bb4",
      "2bb16821aca2465cbc81d1b4c8ca1cba",
      "e918db9c71744ac4a80322a55a51b6fc",
      "bb0b8036c5ba40f4a7a1498cd5866bf6",
      "44fa9e05576241d5b005562372da3524",
      "c1ab67512eb44c4aa49d12cb1c9c3d6f",
      "014e8ea29b4d4671915a87c881646af5",
      "3f5520db93cc40728f05c84169d391ca",
      "7e18a71b64d04df8846d663a1b884edc",
      "908864c7058a4b9cac0f0e891c5ec18a",
      "ad2e4c70117f44d3afce2bfa1136add1",
      "824ae45d284446b0847624200dc9fdde",
      "ff31c980fdb0473f97b9f529d2454eeb",
      "325a035c0db847ecbecdb13d7c8d67ff",
      "246ca41f719a436e889de71ffbca21a3",
      "4bd4b2be8f8a4e9385841e8dc2af2a57",
      "7454b0659921428490b8a61bbf4a580a",
      "37909902ad8c4a4fa6916f051a02fb56",
      "ca7e7e204f0a4f82910707ba6e286de2",
      "c3fdc4cc561d459ea602dd5e54b8e1a6",
      "3a9556a6df3d46d5a89c09ec6b7600f4",
      "f03d73f0ea914119a611ec3217340b6b",
      "1a430fef3e774bae86f454533110afcb",
      "37bc99c2d14a4cc19174f68e2c31c073",
      "675085414b8449d0810859efc3dd65bb",
      "82b067ae49e24f7c9836d217716c587f",
      "d1a026ed7d5b481cab74555da4a22401",
      "d38c51c5bed6454c95b09b3f048a8c5e",
      "fdae3d10031c4e9b8bca609853ee4981",
      "69896f1a345e4e48827dacdb1c759e76",
      "04ad9f2a5c094f3b898df3730f4c2758",
      "2dd1d4a4d8344216b76937ab276715ea",
      "7c52cb0e9eaa4809888d310f6b08d364",
      "0aa084a01d14400db87bd47419492c44",
      "0a90df1461214f219698cf4a47e00b71",
      "569bf27e92844406abec29bdef69e54a",
      "6e021968984742a395459f36f8b8bd55",
      "a9976d13cdde4123a5e43679cde3d05b",
      "71e6e531b2684ed0a6fa9b14f4cfe011",
      "9cfa7e97ef8d45f98a444866988a4640",
      "9b690b7b5fdd488cb4a46e9d7edb3bbf",
      "3e4bf8575812428a95a0304eac6050c1",
      "f727e4effce34ad0add3389774103211",
      "44fa267608f344ed9daf638cb6d8492e",
      "b485d2432f5b47afb48b1c11e6a9b662",
      "6cec8245076045f182d874157ff46ca6",
      "9482a8ab8f754cfdb5c0e23fa88b4064",
      "3b6d01bfda6b4f9ca6313a66e6dc4c5d",
      "82e106bb57e741939c842122779dad75",
      "adbd050682ea4aeb80ceb0cbcb8d0851",
      "3911f867d48b4f038020128542bebe5e",
      "343f4640396e4f0e901658fe8f80b39b",
      "a54766be0f2a4cc59aa98a00bf71bf22",
      "7a156dbbbb6e43c79ba1786e2680c316"
     ]
    },
    "id": "d2vqSZtJHU2W",
    "outputId": "44859795-243b-48e9-827d-d44abfd06a14"
   },
   "outputs": [],
   "source": [
    "# Load the full dataset (train, validation, test splits)\n",
    "dataset = load_dataset(\"abdoelsayed/ArabicaQA\")\n",
    "print(\"ArabicaQA Loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOHNQcQdcZLz"
   },
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "voUAmWx2H1Km",
    "outputId": "ee586f70-0f5b-4d59-93ae-777f7bc8e798"
   },
   "outputs": [],
   "source": [
    "# General info about the dataset\n",
    "print(dataset)\n",
    "\n",
    "# Print a sample from the training set\n",
    "print(\"\\nTraining Sample:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BK546OjOH_v5",
    "outputId": "27b776aa-f57e-45e3-f244-b36c328f3f50"
   },
   "outputs": [],
   "source": [
    "# Show 3 random samples from the training set\n",
    "from random import sample\n",
    "\n",
    "for i in sample(range(len(dataset['train'])), 3):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(dataset['train'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eGpQaHT4vTl"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DKM1tp8h1OOq",
    "outputId": "a86dd8cf-e77e-433e-c718-940586e25db2"
   },
   "outputs": [],
   "source": [
    "# Convert data to DataFrames\n",
    "df_train = pd.DataFrame(dataset['train'])\n",
    "df_val = pd.DataFrame(dataset['validation'])\n",
    "df_test = pd.DataFrame(dataset['test'])\n",
    "\n",
    "print(\"Converted data to DataFrames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T4wL16tZOVph",
    "outputId": "098981bb-f152-48d2-cb60-35660c69a3f7"
   },
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yX9pVDCiOVph",
    "outputId": "c90530aa-8c2b-491e-9045-8d84c6fdf8a2"
   },
   "outputs": [],
   "source": [
    "# Flatten and expand dataframes\n",
    "def flatten_and_expand(df_expanded):\n",
    "    rows = []\n",
    "    for i, row in df_expanded.iterrows():\n",
    "        for para in row['paragraphs']:\n",
    "            context = para['context']\n",
    "            for qa in para['qas']:\n",
    "                question = qa['question']\n",
    "                q_id = qa['id']\n",
    "                is_impossible = qa.get('is_impossible', False)\n",
    "                answers = qa.get('answers', [])\n",
    "\n",
    "                for ans in answers:\n",
    "                    rows.append({\n",
    "                        'id': q_id,\n",
    "                        'question': question,\n",
    "                        'context': context,\n",
    "                        'answer': ans['text'],\n",
    "                        'answer_start': ans.get('answer_start', None),\n",
    "                        'is_impossible': is_impossible\n",
    "                    })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Expand the 'data' column into multiple columns for each dataframe\n",
    "df_train = pd.json_normalize(df_train['data'])\n",
    "df_val = pd.json_normalize(df_val['data'])\n",
    "df_test = pd.json_normalize(df_test['data'])\n",
    "\n",
    "# Process the data\n",
    "df_train = flatten_and_expand(df_train)\n",
    "df_val = flatten_and_expand(df_val)\n",
    "df_test = flatten_and_expand(df_test)\n",
    "\n",
    "print(\"Data loaded and flattened.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdtZzqOmOVph",
    "outputId": "a0cce456-2d91-4c03-c818-78d2210165b9"
   },
   "outputs": [],
   "source": [
    "df_train.info()\n",
    "display(df_train.head())\n",
    "display(df_train['question'].value_counts())\n",
    "display(df_train['answer'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6HN7BWuQWhSd",
    "outputId": "90a795ce-24d9-46ac-b7a3-a3803c7b1c4f"
   },
   "outputs": [],
   "source": [
    "# Filter out NA answers\n",
    "def count_na_answers(df):\n",
    "    na_answers = df['answer'].isin(['.', '. ', '', None]).sum()\n",
    "    return na_answers\n",
    "\n",
    "# Training data\n",
    "na_count_before_train = count_na_answers(df_train)\n",
    "df_train = df_train[~df_train['answer'].isin(['.', '. ', '', None])]\n",
    "na_count_after_train = count_na_answers(df_train)\n",
    "\n",
    "print(\"Training Data NA Answer Counts:\")\n",
    "print(f\"  NA answers before filtering: {na_count_before_train}\")\n",
    "print(f\"  NA answers after filtering: {na_count_after_train}\")\n",
    "\n",
    "# Validation data\n",
    "na_count_before_val = count_na_answers(df_val)\n",
    "df_val = df_val[~df_val['answer'].isin(['.', '. ', '', None])]\n",
    "na_count_after_val = count_na_answers(df_val)\n",
    "\n",
    "print(\"\\nValidation Data NA Answer Counts:\")\n",
    "print(f\"  NA answers before filtering: {na_count_before_val}\")\n",
    "print(f\"  NA answers after filtering: {na_count_after_val}\")\n",
    "\n",
    "# Test data\n",
    "na_count_before_test = count_na_answers(df_test)\n",
    "df_test = df_test[~df_test['answer'].isin(['.', '. ', '', None])]\n",
    "na_count_after_test = count_na_answers(df_test)\n",
    "\n",
    "print(\"\\nTest Data NA Answer Counts:\")\n",
    "print(f\"  NA answers before filtering: {na_count_before_test}\")\n",
    "print(f\"  NA answers after filtering: {na_count_after_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIocBQYqcg-S",
    "outputId": "8f8822e9-14eb-4a14-ced9-aa7d16d9e798"
   },
   "outputs": [],
   "source": [
    "# Checks for training data\n",
    "print(\"Training Data Checks:\")\n",
    "print(f\"  Duplicate IDs found: {df_train['id'].duplicated().sum()}\")\n",
    "print(f\"  Unique contexts: {df_train['context'].nunique()}\")\n",
    "print(\"  Null values per column:\")\n",
    "print(df_train.isnull().sum())\n",
    "\n",
    "# Checks for validation data\n",
    "print(\"\\nValidation Data Checks:\")\n",
    "print(f\"  Duplicate IDs found: {df_val['id'].duplicated().sum()}\")\n",
    "print(f\"  Unique contexts: {df_val['context'].nunique()}\")\n",
    "print(\"  Null values per column:\")\n",
    "print(df_val.isnull().sum())\n",
    "\n",
    "# Checks for test data\n",
    "print(\"\\nTest Data Checks:\")\n",
    "print(f\"  Duplicate IDs found: {df_test['id'].duplicated().sum()}\")\n",
    "print(f\"  Unique contexts: {df_test['context'].nunique()}\")\n",
    "print(\"  Null values per column:\")\n",
    "print(df_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ir69F2ZVPR7v",
    "outputId": "f9f649c4-0bb0-40f4-e452-5698584d6786"
   },
   "outputs": [],
   "source": [
    "# Find duplicate IDs in the test set\n",
    "duplicate_ids = df_test[df_test['id'].duplicated()]['id'].unique()\n",
    "\n",
    "# Display the full rows with duplicate IDs\n",
    "for id_ in duplicate_ids:\n",
    "    print(f\"--- ID: {id_} ---\")\n",
    "    print(df_test[df_test['id'] == id_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5ryd4gmPeS2",
    "outputId": "0e463525-ece4-4792-de7e-628baa766abf"
   },
   "outputs": [],
   "source": [
    "# Drop duplicate IDs\n",
    "duplicate_count_before_test = df_test['id'].duplicated().sum()\n",
    "\n",
    "# Find duplicate IDs\n",
    "duplicate_ids = df_test[df_test['id'].duplicated()]['id'].unique()\n",
    "\n",
    "# Drop duplicate rows based on 'id', keeping the first occurrence\n",
    "df_test = df_test.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "# Re-check for duplicate IDs\n",
    "duplicate_count_after_test = df_test['id'].duplicated().sum()\n",
    "\n",
    "print(\"Test Data Duplicate ID Counts:\")\n",
    "print(f\"  Duplicate IDs found before dropping: {duplicate_count_before_test}\")\n",
    "print(f\"  Duplicate IDs found after dropping: {duplicate_count_after_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tw63Knt8pqMg",
    "outputId": "09690e05-c08e-4ddf-b349-99e9905d74a6"
   },
   "outputs": [],
   "source": [
    "missing_answer_count = 0\n",
    "total_samples = len(df_train)\n",
    "\n",
    "for idx, row in df_train.iterrows():\n",
    "    context = row[\"context\"]\n",
    "    answer = row[\"answer\"]\n",
    "    if answer not in context:\n",
    "        print(f\"Sample {idx}: Answer not found in context. Answer: {answer}\")\n",
    "        missing_answer_count += 1\n",
    "\n",
    "print(f\"Missing answer count: {missing_answer_count} out of {total_samples} samples\")\n",
    "print(f\"Percentage of missing answers: {100 * missing_answer_count / total_samples:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zpLbGXAUddFq",
    "outputId": "bd55ecb0-5f2c-46cc-fb0f-2201f436fa4c"
   },
   "outputs": [],
   "source": [
    "# Calculate length features for training data\n",
    "df_train['context_len'] = df_train['context'].apply(len)\n",
    "df_train['question_len'] = df_train['question'].apply(len)\n",
    "df_train['answer_len'] = df_train['answer'].apply(len)\n",
    "\n",
    "print(\"Training Data Length Features:\")\n",
    "print(df_train[['context_len', 'question_len', 'answer_len']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zvNxAevxdi4D",
    "outputId": "e063dc1e-5acb-42f0-c6ae-db5550ca7456"
   },
   "outputs": [],
   "source": [
    "# Calculate 'is_impossible' value counts for all 3 dataframes\n",
    "def check_impossible_values(df, df_name):\n",
    "    value_counts = df['is_impossible'].value_counts(normalize=True)\n",
    "    print(f\"{df_name} Data 'is_impossible' Value Counts (Normalized):\")\n",
    "    print(value_counts)\n",
    "\n",
    "    if len(value_counts) == 1 and value_counts.index[0] == False and value_counts[False] == 1.0:\n",
    "        print(f\"No impossible questions in {df_name} DataFrame.\")\n",
    "\n",
    "# Training data\n",
    "check_impossible_values(df_train, \"Training\")\n",
    "\n",
    "# Validation data\n",
    "print() #add an extra line for readability\n",
    "check_impossible_values(df_val, \"Validation\")\n",
    "\n",
    "# Test data\n",
    "print() #add an extra line for readability\n",
    "check_impossible_values(df_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38CpuLqQyYsZ",
    "outputId": "a4f5a483-c5f4-441c-f8a2-74d144c2807f"
   },
   "outputs": [],
   "source": [
    "# Display random examples from test set\n",
    "num_examples = 4\n",
    "top_answers = df_test['answer'].value_counts().head(num_examples).index.tolist()\n",
    "\n",
    "print(\"--- Random Examples from Test Data ---\\n\")\n",
    "for ans in top_answers:\n",
    "    subset = df_test[df_test['answer'] == ans]\n",
    "\n",
    "    if len(subset) > 0:\n",
    "        random_index = random.randint(0, len(subset) - 1)\n",
    "        example = subset.iloc[random_index]\n",
    "\n",
    "        print(f\"Question: {example['question']}\")\n",
    "        print(f\"Context (first 150 chars): {example['context'][:150]}...\")\n",
    "        print(f\"Answer: {example['answer']}\")\n",
    "        print(f\"Is Impossible: {example['is_impossible']}\")\n",
    "        print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S06Ouqsueuub",
    "outputId": "325c69e6-7d93-4619-9004-7a5167dd3473"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_dataset(df, dataset_name=\"Dataset\"):\n",
    "    print(f\"---- Analyzing {dataset_name} ----\")\n",
    "    # Print the first 5 samples to inspect the raw data\n",
    "    print(\"First 5 samples:\")\n",
    "    print(df.head(), \"\\n\")\n",
    "\n",
    "    # Print DataFrame information to see data types and non-null counts\n",
    "    print(\"DataFrame Info:\")\n",
    "    print(df.info(), \"\\n\")\n",
    "\n",
    "    # Check if required columns are present\n",
    "    required_columns = ['question', 'context', 'answer']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Error: Column '{col}' not found in {dataset_name}.\")\n",
    "            return\n",
    "\n",
    "    # Check if each answer appears somewhere in its corresponding context\n",
    "    def answer_in_context(row):\n",
    "        # Compare after stripping whitespace (you may also add lower() if needed)\n",
    "        return row['answer'].strip() in row['context'].strip()\n",
    "\n",
    "    df['answer_in_context'] = df.apply(answer_in_context, axis=1)\n",
    "    missing_answers = df[~df['answer_in_context']]\n",
    "    print(f\"Total samples in {dataset_name}: {len(df)}\")\n",
    "    print(f\"Samples where the answer is NOT found in the context: {len(missing_answers)}\\n\")\n",
    "\n",
    "    # Print a few examples where the answer does not appear\n",
    "    if len(missing_answers) > 0:\n",
    "        print(\"Examples where the answer is missing from the context:\")\n",
    "        print(missing_answers[['question', 'context', 'answer']].head(), \"\\n\")\n",
    "\n",
    "    # Compute word count statistics\n",
    "    df['question_length'] = df['question'].apply(lambda x: len(x.split()))\n",
    "    df['context_length'] = df['context'].apply(lambda x: len(x.split()))\n",
    "    df['answer_length'] = df['answer'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    print(\"Average question length (words):\", df['question_length'].mean())\n",
    "    print(\"Average context length (words):\", df['context_length'].mean())\n",
    "    print(\"Average answer length (words):\", df['answer_length'].mean(), \"\\n\")\n",
    "\n",
    "    # Plot histograms for word count distributions\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(df['question_length'], bins=20, color='blue', alpha=0.7)\n",
    "    plt.title(\"Question Length Distribution (words)\")\n",
    "    plt.xlabel(\"Words\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(df['context_length'], bins=20, color='green', alpha=0.7)\n",
    "    plt.title(\"Context Length Distribution (words)\")\n",
    "    plt.xlabel(\"Words\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.hist(df['answer_length'], bins=20, color='red', alpha=0.7)\n",
    "    plt.title(\"Answer Length Distribution (words)\")\n",
    "    plt.xlabel(\"Words\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze each dataset:\n",
    "analyze_dataset(df_train, \"Training Data\")\n",
    "analyze_dataset(df_val, \"Validation Data\")\n",
    "analyze_dataset(df_test, \"Test Data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmxtGM8L_9t0"
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FlJdzrTxZzwt",
    "outputId": "e4416c92-c96d-46ae-a907-d4f715614375"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login\n",
    "\n",
    "\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fXmaQUtRN1qh",
    "outputId": "d655f365-f088-4481-8a2b-054eeb0ef443"
   },
   "outputs": [],
   "source": [
    "# Match dataset size with Arastance for more accurate representation\n",
    "print(\"Dataset sizes before limiting:\")\n",
    "print(\"Train set:\", len(df_train))\n",
    "print(\"Validation set:\", len(df_val))\n",
    "print(\"Test set:\", len(df_test))\n",
    "\n",
    "df_train = df_train.sample(n=2848, random_state=42)  # Limit train set\n",
    "df_val = df_val.sample(n=569, random_state=42)    # Limit validation set\n",
    "df_test = df_test.sample(n=646, random_state=42)   # Limit test set\n",
    "\n",
    "print(\"\\nDataset sizes after limiting:\")\n",
    "print(f\"Train: {len(df_train)}\")\n",
    "print(f\"Validation: {len(df_val)}\")\n",
    "print(f\"Test: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0b4a148cf6d44f07bc153e2f1f2de2ce",
      "dceb84dcae454685bafffa14fe6c457a",
      "360064a240414e938861694d4da7777f",
      "11f72e8c62c343ddafb841c38704936a",
      "2787a3df0c764c44902be39946a2159e",
      "3696d4b05a0f4d6f8167ae4385b88ee5",
      "90d37d2d7b7f4e47a97a03caa213c920",
      "a53c9d7e7f7042bbb29d3e5314770b23",
      "6ca732d4df7c401fa217df7f891b7976",
      "403a9e8915244e979d8b0da99d8492db",
      "f4b22a6656b345abacfc97b8f11ab3fd",
      "34a90bdcb9aa41edbfefa3b2f381fd3d",
      "094f64e0cf2b4672b5928809e724801f",
      "e8834ad12c6f4f338a37e47887236f01",
      "fc650ce747ea4161a8de6294141d4050",
      "c88fa45be58940cb83ba3975961afcc1",
      "2e775e8436004cb99467f411d610c1d2",
      "5bc6574edfd04f0da39b4f105a67a692",
      "63c46693f9a2452b962655c5ba99752e",
      "7962ebac8de04406a818d725c11f92a6",
      "350db804089f4380842c3f95edb7b728",
      "17ba2671f2ef4a84bca26566798f5e06"
     ]
    },
    "id": "aarmlwHouK9v",
    "outputId": "c4602f65-f15b-4e71-d9b9-c4149db331ba"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import html\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# Import PEFT classes for LoRA support\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig\n",
    "\n",
    "# -------------------------------\n",
    "# QADataset and qa_collate_fn definitions\n",
    "# -------------------------------\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Expects dataframe columns: 'question', 'context', 'answer', and optionally 'answer_start'.\n",
    "        \"\"\"\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        question = html.unescape(row['question'])\n",
    "        context = html.unescape(row['context'])\n",
    "        answer_text = html.unescape(row['answer'])\n",
    "        answer_start = row['answer_start'] if 'answer_start' in row and not pd.isna(row['answer_start']) else context.find(answer_text)\n",
    "        if answer_start == -1:\n",
    "            answer_start = 0\n",
    "            answer_end = 0\n",
    "        else:\n",
    "            answer_end = answer_start + len(answer_text)\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            question,\n",
    "            context,\n",
    "            truncation=\"only_second\",\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        offset_mapping = encoding.pop(\"offset_mapping\")\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        offset_mapping = offset_mapping.squeeze(0)\n",
    "\n",
    "        token_type_ids = encoding.get(\"token_type_ids\", None)\n",
    "        start_pos, end_pos = None, None\n",
    "        for idx_tok, (start_char, end_char) in enumerate(offset_mapping):\n",
    "            start_char = int(start_char)\n",
    "            end_char = int(end_char)\n",
    "            if token_type_ids is None or token_type_ids[idx_tok] == 1:\n",
    "                if start_pos is None and start_char <= answer_start < end_char:\n",
    "                    start_pos = idx_tok\n",
    "                if start_pos is not None and end_char >= answer_end:\n",
    "                    end_pos = idx_tok\n",
    "                    break\n",
    "        if start_pos is None or end_pos is None:\n",
    "            start_pos, end_pos = -100, -100\n",
    "\n",
    "        encoding[\"start_positions\"] = torch.tensor(start_pos, dtype=torch.long)\n",
    "        encoding[\"end_positions\"] = torch.tensor(end_pos, dtype=torch.long)\n",
    "        encoding[\"context\"] = context\n",
    "        encoding[\"question\"] = question\n",
    "        encoding[\"answer_text\"] = answer_text\n",
    "        encoding[\"offset_mapping\"] = offset_mapping.tolist()\n",
    "        return encoding\n",
    "\n",
    "def qa_collate_fn(batch):\n",
    "    collated = {}\n",
    "    for key in ['input_ids', 'attention_mask', 'start_positions', 'end_positions', 'token_type_ids']:\n",
    "        if key in batch[0]:\n",
    "            collated[key] = torch.stack([sample[key] for sample in batch])\n",
    "    collated['context'] = [sample['context'] for sample in batch]\n",
    "    collated['question'] = [sample['question'] for sample in batch]\n",
    "    collated['answer_text'] = [sample['answer_text'] for sample in batch]\n",
    "    collated['offset_mapping'] = [sample['offset_mapping'] for sample in batch]\n",
    "    return collated\n",
    "\n",
    "# -------------------------------\n",
    "# QA Tokenization Analysis on the Full Dataset (Train+Val+Test)\n",
    "# -------------------------------\n",
    "def analyze_tokenization_qa_full(df_train, df_val, df_test):\n",
    "    \"\"\"\n",
    "    Analyze tokenization for QA on the entire dataset (train, validation, test).\n",
    "    Computes metrics and produces visualization plots:\n",
    "      - Scatter plot: Token count vs. Text Length\n",
    "      - Histogram: Distribution of Token Fertility\n",
    "      - Bar plot: Token Length Distribution\n",
    "      - Histogram: Compression Ratio Distribution (derived from token fertility)\n",
    "    \"\"\"\n",
    "    print(\"Loading tokenizer for QA tokenization analysis on the full dataset...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\", use_fast=True)\n",
    "    combined_df = pd.concat([df_train, df_val, df_test])\n",
    "    print(f\"Analyzing tokenization on {len(combined_df)} samples...\")\n",
    "\n",
    "    results = []\n",
    "    total_chars = 0\n",
    "    total_tokens = 0\n",
    "    token_lengths = []\n",
    "    tokens_per_char_values = []\n",
    "    oov_count = 0\n",
    "    unique_tokens = set()\n",
    "    vocabulary_size = len(tokenizer.get_vocab())\n",
    "    highly_fragmented_words = []\n",
    "    words_analyzed = 0\n",
    "\n",
    "    for _, row in combined_df.iterrows():\n",
    "        question = html.unescape(row['question'])\n",
    "        # Use a truncated version of the context for speed (adjust as needed)\n",
    "        context = html.unescape(row['context'][:500])\n",
    "        q_chars = len(question)\n",
    "        q_tokens = tokenizer.encode(question, add_special_tokens=False)\n",
    "        q_tokens_texts = tokenizer.convert_ids_to_tokens(q_tokens)\n",
    "\n",
    "        # Token fertility: tokens per character in question\n",
    "        q_fertility = len(q_tokens) / q_chars if q_chars > 0 else 0\n",
    "        tokens_per_char_values.append(q_fertility)\n",
    "\n",
    "        for token in q_tokens_texts:\n",
    "            token_lengths.append(len(token))\n",
    "            unique_tokens.add(token)\n",
    "\n",
    "        total_chars += q_chars\n",
    "        total_tokens += len(q_tokens)\n",
    "\n",
    "        # OOV estimation: count words that get split into 3 or more tokens\n",
    "        for word in question.split():\n",
    "            if len(word) >= 3:\n",
    "                words_analyzed += 1\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                if len(word_tokens) >= 3:\n",
    "                    highly_fragmented_words.append(word)\n",
    "                    oov_count += 1\n",
    "\n",
    "        results.append({\n",
    "            'text_length': q_chars + len(context),\n",
    "            'llama_tokens': len(q_tokens) + len(tokenizer.encode(context, add_special_tokens=False)),\n",
    "            'fertility': q_fertility\n",
    "        })\n",
    "\n",
    "    token_df = pd.DataFrame(results)\n",
    "    avg_token_fertility = np.mean(tokens_per_char_values)\n",
    "    token_length_distribution = Counter(token_lengths)\n",
    "    avg_token_length = np.mean(token_lengths)\n",
    "    median_token_length = np.median(token_lengths)\n",
    "    compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0\n",
    "    vocabulary_coverage = len(unique_tokens) / vocabulary_size\n",
    "    oov_rate = oov_count / words_analyzed if words_analyzed > 0 else 0\n",
    "\n",
    "    print(\"\\n===== Tokenization Analysis Results =====\")\n",
    "    print(f\"1. Token Fertility (tokens/char): {avg_token_fertility:.4f}\")\n",
    "    print(f\"2. Token Length: Mean = {avg_token_length:.2f}, Median = {median_token_length:.2f}\")\n",
    "    print(f\"3. Compression Ratio (chars/token): {compression_ratio:.4f}\")\n",
    "    print(f\"4. Vocabulary: Used {len(unique_tokens)} of {vocabulary_size} tokens ({vocabulary_coverage:.2%})\")\n",
    "    print(f\"5. OOV Rate: {oov_rate:.4f} ({oov_count}/{words_analyzed} words)\")\n",
    "\n",
    "    if highly_fragmented_words:\n",
    "        print(\"\\nExample highly fragmented words (potential OOVs):\")\n",
    "        sample_oov = random.sample(highly_fragmented_words, min(10, len(highly_fragmented_words)))\n",
    "        for word in sample_oov:\n",
    "            tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "            token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
    "            print(f\"  '{word}' → {len(tokens)} tokens: {token_texts}\")\n",
    "\n",
    "    # Visualize tokenization with 4 subplots.\n",
    "    plt.figure(figsize=(16, 12))\n",
    "\n",
    "    # Plot 1: Scatter plot of Token Count vs Text Length\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(token_df['text_length'], token_df['llama_tokens'])\n",
    "    plt.xlabel('Text Length (characters)')\n",
    "    plt.ylabel('Llama Token Count')\n",
    "    plt.title('Token Count vs Text Length')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Histogram of Token Fertility\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.hist(tokens_per_char_values, bins=20)\n",
    "    plt.xlabel('Token Fertility (tokens/char)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Token Fertility')\n",
    "\n",
    "    # Plot 3: Bar plot of Token Length Distribution\n",
    "    plt.subplot(2, 2, 3)\n",
    "    lengths = list(token_length_distribution.keys())\n",
    "    frequencies = list(token_length_distribution.values())\n",
    "    plt.bar(lengths, frequencies)\n",
    "    plt.xlabel('Token Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Token Length Distribution')\n",
    "\n",
    "    # Plot 4: Histogram of Compression Ratio Distribution (chars/token)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    compression_values = [1/f if f > 0 else 0 for f in tokens_per_char_values]\n",
    "    plt.hist(compression_values, bins=20)\n",
    "    plt.xlabel('Compression Ratio (chars/token)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Compression Ratio Distribution')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tokenization_analysis.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the computed metrics to a JSON file.\n",
    "    tokenization_metrics = {\n",
    "        'token_fertility': avg_token_fertility,\n",
    "        'token_length_mean': avg_token_length,\n",
    "        'token_length_median': median_token_length,\n",
    "        'compression_ratio': compression_ratio,\n",
    "        'vocabulary_size': vocabulary_size,\n",
    "        'vocabulary_coverage': vocabulary_coverage,\n",
    "        'oov_rate': oov_rate,\n",
    "        'token_length_distribution': dict(token_length_distribution)\n",
    "    }\n",
    "    with open('qa_tokenization_metrics_full.json', 'w') as f:\n",
    "        json.dump(tokenization_metrics, f, indent=2)\n",
    "\n",
    "    return token_df\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluate QA Model Function\n",
    "# -------------------------------\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
    "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
    "\n",
    "def compute_exact(a_pred, a_gold):\n",
    "    return int(normalize_answer(a_pred) == normalize_answer(a_gold))\n",
    "\n",
    "def compute_f1(a_pred, a_gold):\n",
    "    pred_tokens = normalize_answer(a_pred).split()\n",
    "    gold_tokens = normalize_answer(a_gold).split()\n",
    "    common = Counter(pred_tokens) & Counter(gold_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "        return int(pred_tokens == gold_tokens)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gold_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def evaluate_qa_model(model, data_loader, tokenizer, device, verbose=True):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    all_em = []\n",
    "    all_f1 = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch_tensors = {k: v.to(device) for k, v in batch.items() if k in ['input_ids', 'attention_mask', 'start_positions', 'end_positions', 'token_type_ids']}\n",
    "            outputs = model(**batch_tensors)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            start_logits = outputs.start_logits\n",
    "            end_logits = outputs.end_logits\n",
    "            batch_size = start_logits.size(0)\n",
    "            for i in range(batch_size):\n",
    "                input_ids = batch['input_ids'][i]\n",
    "                offsets = batch['offset_mapping'][i]\n",
    "                start_idx = torch.argmax(start_logits[i]).item()\n",
    "                end_idx = torch.argmax(end_logits[i]).item()\n",
    "                if end_idx < start_idx:\n",
    "                    pred_answer = \"\"\n",
    "                else:\n",
    "                    context = batch['context'][i]\n",
    "                    char_start = offsets[start_idx][0]\n",
    "                    char_end = offsets[end_idx][1]\n",
    "                    pred_answer = context[char_start:char_end]\n",
    "                true_answer = batch['answer_text'][i]\n",
    "                all_em.append(compute_exact(pred_answer, true_answer))\n",
    "                all_f1.append(compute_f1(pred_answer, true_answer))\n",
    "\n",
    "    #avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    avg_em = np.mean(all_em) if all_em else 0\n",
    "    avg_f1 = np.mean(all_f1) if all_f1 else 0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"  Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  EM: {avg_em:.4f}\")\n",
    "        print(f\"  F1: {avg_f1:.4f}\")\n",
    "    return {'loss': avg_loss, 'exact_match': avg_em, 'f1': avg_f1}\n",
    "\n",
    "# -------------------------------\n",
    "# Training Function for QA with LoRA Integration\n",
    "# -------------------------------\n",
    "def train_model_qa(model_name, df_train, df_val, df_test, output_dir=output_dir, use_lora=False, epochs=10):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    batch_size = 4\n",
    "    grad_accum_steps = 8\n",
    "    learning_rate = 2e-5\n",
    "    max_length = 512\n",
    "    model_save_path = os.path.join(output_dir, f\"{model_name.split('/')[-1]}_qa_model\")\n",
    "\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Training {model_name} model for Question Answering\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"Set EOS token as padding token\")\n",
    "\n",
    "    print(\"Preparing QA datasets...\")\n",
    "    train_dataset = QADataset(df_train, tokenizer, max_length)\n",
    "    val_dataset = QADataset(df_val, tokenizer, max_length)\n",
    "    test_dataset = QADataset(df_test, tokenizer, max_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=qa_collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=qa_collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=qa_collate_fn)\n",
    "\n",
    "    # === Sanity Check 1: Count Fallback Occurrences in QA Dataset ===\n",
    "    fallback_count = 0\n",
    "    total_samples = len(train_dataset)\n",
    "    fallback_samples = []  # Initialize the list to store fallback samples\n",
    "\n",
    "    for i in range(total_samples):\n",
    "        sample = train_dataset[i]\n",
    "        # Check if both start_positions and end_positions are set to 0 (fallback case)\n",
    "        if sample[\"start_positions\"].item() == 0 and sample[\"end_positions\"].item() == 0:\n",
    "            fallback_count += 1\n",
    "            fallback_samples.append((df_train.iloc[i]['context'], df_train.iloc[i]['answer']))\n",
    "\n",
    "\n",
    "    #for idx, (context, answer) in enumerate(fallback_samples):\n",
    "        #print(f\"\\nFallback Sample {idx+1}:\")\n",
    "        #print(\"Context snippet:\", context[:200])\n",
    "        #print(\"Answer:\", answer)\n",
    "\n",
    "    print(f\"Fallback occurrences: {fallback_count} out of {total_samples} samples\")\n",
    "    print(f\"Percentage of fallbacks: {100 * fallback_count / total_samples:.2f}%\")\n",
    "\n",
    "    print(\"Initializing QA model...\")\n",
    "    if use_lora:\n",
    "        base_model = AutoModelForQuestionAnswering.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "        if base_model.config.pad_token_id is None:\n",
    "            base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        peft_config = LoraConfig(\n",
    "            task_type='QUESTION_ANS',\n",
    "            inference_mode=False,\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "        )\n",
    "        model = get_peft_model(base_model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "    else:\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "        if model.config.pad_token_id is None:\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_loader) * epochs // grad_accum_steps\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=total_steps // 10, num_training_steps=total_steps)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_f1 = 0\n",
    "    val_em_list = []\n",
    "    val_f1_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch_tensors = {k: v.to(device) for k, v in batch.items()\n",
    "                             if k in ['input_ids', 'attention_mask', 'start_positions', 'end_positions', 'token_type_ids']}\n",
    "            outputs = model(**batch_tensors)\n",
    "            loss = outputs.loss / grad_accum_steps\n",
    "            loss.backward()\n",
    "            epoch_train_loss += loss.item() * grad_accum_steps\n",
    "            if (step + 1) % grad_accum_steps == 0 or (step + 1) == len(train_loader):\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch_tensors = {k: v.to(device) for k, v in batch.items()\n",
    "                                 if k in ['input_ids', 'attention_mask', 'start_positions', 'end_positions', 'token_type_ids']}\n",
    "                outputs = model(**batch_tensors)\n",
    "                if outputs.loss is not None:\n",
    "                    epoch_val_loss += outputs.loss.item()\n",
    "        avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        eval_results = evaluate_qa_model(model, val_loader, tokenizer, device, verbose=False)\n",
    "        print(f\"  Val EM: {eval_results['exact_match']:.4f}\")\n",
    "        print(f\"  Val F1: {eval_results['f1']:.4f}\")\n",
    "\n",
    "        # Record evaluation metrics for plotting\n",
    "        val_em_list.append(eval_results['exact_match'])\n",
    "        val_f1_list.append(eval_results['f1'])\n",
    "\n",
    "        # Save model based on best validation F1 (instead of best EM)\n",
    "        if eval_results['f1'] > best_val_f1:\n",
    "            best_val_f1 = eval_results['f1']\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            model_to_save.save_pretrained(model_save_path)\n",
    "            tokenizer.save_pretrained(model_save_path)\n",
    "            print(f\"  Model saved to {model_save_path} with F1: {best_val_f1:.4f}\")\n",
    "\n",
    "    # Plotting evaluation metrics after training\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, 'b-', label='Train Loss')\n",
    "    plt.plot(val_losses, 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot evaluation metrics (EM and F1)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_em_list, 'g-', label='Validation EM')\n",
    "    plt.plot(val_f1_list, 'm-', label='Validation F1')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Validation Evaluation Metrics')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{model_name.split(\"/\")[-1]}_evaluation_metrics.png'))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    if use_lora:\n",
    "        # Load LoRA adapter configuration from the local path\n",
    "        config = PeftConfig.from_pretrained(model_save_path, local_files_only=True)\n",
    "        base_model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "            config.base_model_name_or_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        if base_model.config.pad_token_id is None:\n",
    "            base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        best_model = PeftModel.from_pretrained(base_model, model_save_path, local_files_only=True)\n",
    "    else:\n",
    "        best_model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "            model_save_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        if best_model.config.pad_token_id is None:\n",
    "            best_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "    test_results = evaluate_qa_model(best_model, test_loader, tokenizer, device, verbose=True)\n",
    "    with open(os.path.join(output_dir, f'{model_name.split(\"/\")[-1]}_test_results.json'), 'w') as f:\n",
    "        json.dump(test_results, f, indent=2)\n",
    "    return test_results\n",
    "\n",
    "# -------------------------------\n",
    "# Pipeline Function for QA with LoRA Option\n",
    "# -------------------------------\n",
    "def run_pipeline_qa(df_train, df_val, df_test, run_tokenization=True, epochs=10, use_lora=False):\n",
    "    print(f\"Train set: {len(df_train)} samples\")\n",
    "    print(f\"Validation set: {len(df_val)} samples\")\n",
    "    print(f\"Test set: {len(df_test)} samples\")\n",
    "    print(\"\\nSample answer distribution (first 5 samples):\")\n",
    "    print(df_train[['question', 'answer']].head())\n",
    "    if run_tokenization:\n",
    "        print(\"\\n=== Running Tokenization Analysis for QA ===\")\n",
    "        analyze_tokenization_qa_full(df_train, df_val, df_test)\n",
    "    model_name = \"meta-llama/Llama-3.1-8B\"  # Change as required.\n",
    "    results = train_model_qa(model_name, df_train, df_val, df_test, epochs=epochs, use_lora=use_lora)\n",
    "    return results\n",
    "\n",
    "# -------------------------------\n",
    "# Main\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming df_train, df_val, df_test are loaded with your arabicaQA data.\n",
    "    results = run_pipeline_qa(df_train, df_val, df_test, run_tokenization=True, epochs=10, use_lora=True)\n",
    "    print(\"\\n=== Final QA Test Results Summary ===\")\n",
    "    print(f\"Loss: {results['loss']:.4f}\")\n",
    "    print(f\"Exact Match (EM): {results['exact_match']:.4f}\")\n",
    "    print(f\"F1 Score: {results['f1']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "j9WCYLo914AM",
    "lOHNQcQdcZLz",
    "_eGpQaHT4vTl"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
