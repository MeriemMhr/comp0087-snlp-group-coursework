{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_01ScrXYsbmF"
   },
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KNgzxkwccDr6",
    "outputId": "26fd114a-10ef-40b2-a44d-74a393b24827"
   },
   "outputs": [],
   "source": [
    "!pip install datasets pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IStduJO_hB3c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import html\n",
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qn1C6ZdqATlp",
    "outputId": "dd0b30a0-d187-4772-a5c3-eee86eb8cf9b"
   },
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "print(\"Loading dataset from Hugging Face...\")\n",
    "dataset = load_dataset(\"okite97/news-data\")\n",
    "\n",
    "# Convert to pandas DataFrames\n",
    "df_train = pd.DataFrame(dataset['train'])\n",
    "df_test = pd.DataFrame(dataset['test'])\n",
    "\n",
    "print(f\"Training set: {len(df_train)} samples\")\n",
    "print(f\"Test set: {len(df_test)} samples\")\n",
    "df_test = pd.DataFrame(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2oLJrwpPA2wE",
    "outputId": "09ad11d3-d44c-4663-af31-2f1cfa4d7328"
   },
   "outputs": [],
   "source": [
    "# Display sample of the data\n",
    "print(\"\\nFirst 5 rows of training data:\")\n",
    "print(df_train.head())\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nCategory distribution in training set:\")\n",
    "print(df_train['Category'].value_counts())\n",
    "\n",
    "print(\"\\nCategory distribution in test set:\")\n",
    "print(df_test['Category'].value_counts())\n",
    "\n",
    "# Display examples from each category\n",
    "print(\"\\nExamples from each category:\")\n",
    "categories = df_train['Category'].unique()\n",
    "for category in categories:\n",
    "    if category in df_train['Category'].values:\n",
    "        example = df_train[df_train['Category'] == category].iloc[0]\n",
    "        print(f\"\\n--- {category} Example ---\")\n",
    "        print(f\"Title: {example['Title']}\")\n",
    "        print(f\"Excerpt: {example['Excerpt'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nZd85AKXA3L9",
    "outputId": "b69cad8f-6daa-4e02-e338-bad4d4502022"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade huggingface_hub\n",
    "!pip install transformers==4.49.0\n",
    "!pip install accelerate\n",
    "!pip install peft==0.5.0\n",
    "!pip install datasets\n",
    "!pip install bitsandbytes==0.38.2\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i_u3Ec2tBC4n",
    "outputId": "1787fb84-331f-4739-98e9-3893105f9560"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login\n",
    "\n",
    "# 3. Import libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, roc_curve, auc\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import html\n",
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    print(\"CUDA is available! Using GPU.\")\n",
    "else:\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGUIX6spABRC"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create news dataset class\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Map category labels to integers\n",
    "        categories = sorted(dataframe['Category'].unique())\n",
    "        self.label_map = {category: i for i, category in enumerate(categories)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        title = str(row['Title'])\n",
    "        excerpt = str(row['Excerpt'])\n",
    "\n",
    "        # Combine title and excerpt\n",
    "        text = f\"Title: {title} Excerpt: {excerpt}\"\n",
    "\n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Remove batch dimension added by tokenizer\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "\n",
    "        # Add label\n",
    "        encoding['labels'] = torch.tensor(self.label_map[row['Category']])\n",
    "\n",
    "        return encoding\n",
    "\n",
    "\n",
    "# Tokenization analysis function for the new dataset\n",
    "def analyze_tokenization(train_df, test_df, model_name=\"google/gemma-7b\"):\n",
    "    \"\"\"\n",
    "    Analyze tokenization for Gemma with specific metrics:\n",
    "    - Token Count (Token Fertility)\n",
    "    - Token Length Distribution\n",
    "    - Compression Ratio\n",
    "    - Vocabulary Size\n",
    "    - OOV Rate\n",
    "    \"\"\"\n",
    "    print(\"Loading tokenizer for tokenization analysis...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "    # Use a subsample for analysis to speed up execution\n",
    "    combined_df = pd.concat([train_df, test_df])\n",
    "    sample_size = min(1000, len(combined_df))\n",
    "    sample_df = combined_df.sample(sample_size, random_state=42)\n",
    "\n",
    "    print(f\"Analyzing tokenization on {sample_size} samples...\")\n",
    "\n",
    "    # Initialize metrics\n",
    "    results = []\n",
    "    total_chars = 0\n",
    "    total_tokens = 0\n",
    "    token_lengths = []\n",
    "    tokens_per_char_values = []\n",
    "    oov_count = 0\n",
    "    unique_tokens = set()\n",
    "    vocabulary_size = len(tokenizer.get_vocab())\n",
    "    highly_fragmented_words = []\n",
    "    words_analyzed = 0\n",
    "\n",
    "    # Process each row\n",
    "    for _, row in tqdm(sample_df.iterrows(), desc=\"Analyzing tokenization\"):\n",
    "        # Clean text\n",
    "        title = str(row['Title'])\n",
    "        excerpt = str(row['Excerpt'])[:500]  # Limit excerpt for speed\n",
    "\n",
    "        # Analyze title tokenization\n",
    "        title_chars = len(title)\n",
    "        title_tokens = tokenizer.encode(title, add_special_tokens=False)\n",
    "        title_tokens_texts = tokenizer.convert_ids_to_tokens(title_tokens)\n",
    "\n",
    "        # Token fertility (tokens per character)\n",
    "        title_fertility = len(title_tokens) / title_chars if title_chars > 0 else 0\n",
    "        tokens_per_char_values.append(title_fertility)\n",
    "\n",
    "        # Track token lengths for distribution\n",
    "        for token in title_tokens_texts:\n",
    "            token_lengths.append(len(token))\n",
    "            unique_tokens.add(token)\n",
    "\n",
    "        # Update counts\n",
    "        total_chars += title_chars\n",
    "        total_tokens += len(title_tokens)\n",
    "\n",
    "        # Check word-level fragmentation (OOV estimation)\n",
    "        title_words = title.split()\n",
    "        for word in title_words:\n",
    "            if len(word) >= 3:  # Only check non-trivial words\n",
    "                words_analyzed += 1\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                if len(word_tokens) >= 3:  # If a word is broken into 3+ tokens\n",
    "                    highly_fragmented_words.append(word)\n",
    "                    oov_count += 1\n",
    "\n",
    "        # Do the same analysis for excerpt\n",
    "        excerpt_chars = len(excerpt)\n",
    "        excerpt_tokens = tokenizer.encode(excerpt, add_special_tokens=False)\n",
    "        excerpt_tokens_texts = tokenizer.convert_ids_to_tokens(excerpt_tokens)\n",
    "\n",
    "        excerpt_fertility = len(excerpt_tokens) / excerpt_chars if excerpt_chars > 0 else 0\n",
    "        tokens_per_char_values.append(excerpt_fertility)\n",
    "\n",
    "        for token in excerpt_tokens_texts:\n",
    "            token_lengths.append(len(token))\n",
    "            unique_tokens.add(token)\n",
    "\n",
    "        total_chars += excerpt_chars\n",
    "        total_tokens += len(excerpt_tokens)\n",
    "\n",
    "        excerpt_words = excerpt.split()\n",
    "        for word in excerpt_words:\n",
    "            if len(word) >= 3:\n",
    "                words_analyzed += 1\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                if len(word_tokens) >= 3:\n",
    "                    highly_fragmented_words.append(word)\n",
    "                    oov_count += 1\n",
    "\n",
    "        # Store results for this sample\n",
    "        results.append({\n",
    "            'text_length': len(title) + len(excerpt),\n",
    "            'gemma_tokens': len(title_tokens) + len(excerpt_tokens),\n",
    "            'fertility': (len(title_tokens) + len(excerpt_tokens)) / (len(title) + len(excerpt)) if (len(title) + len(excerpt)) > 0 else 0,\n",
    "            'category': row['Category']\n",
    "        })\n",
    "\n",
    "    token_df = pd.DataFrame(results)\n",
    "\n",
    "    # Calculate final metrics\n",
    "    avg_token_fertility = np.mean(tokens_per_char_values)\n",
    "    avg_token_length = np.mean(token_lengths)\n",
    "    median_token_length = np.median(token_lengths)\n",
    "    compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0\n",
    "    vocabulary_coverage = len(unique_tokens) / vocabulary_size\n",
    "    oov_rate = oov_count / words_analyzed if words_analyzed > 0 else 0\n",
    "\n",
    "    # Display statistics\n",
    "    print(\"\\n===== Tokenization Analysis Results =====\")\n",
    "    print(f\"1. Token Fertility (tokens/char): {avg_token_fertility:.4f}\")\n",
    "    print(f\"2. Token Length: Mean={avg_token_length:.2f}, Median={median_token_length:.2f}\")\n",
    "    print(f\"3. Compression Ratio (chars/token): {compression_ratio:.4f}\")\n",
    "    print(f\"4. Vocabulary: Used {len(unique_tokens)} of {vocabulary_size} tokens ({vocabulary_coverage:.2%})\")\n",
    "    print(f\"5. OOV Rate: {oov_rate:.4f} ({oov_count}/{words_analyzed} words)\")\n",
    "\n",
    "    # Print example highly fragmented words\n",
    "    if highly_fragmented_words:\n",
    "        print(\"\\nExample highly fragmented words (potential OOVs):\")\n",
    "        sample_oov = random.sample(highly_fragmented_words, min(10, len(highly_fragmented_words)))\n",
    "        for word in sample_oov:\n",
    "            tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "            token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
    "            print(f\"  '{word}' → {len(tokens)} tokens: {token_texts}\")\n",
    "\n",
    "    # Visualize tokenization\n",
    "    plt.figure(figsize=(16, 12))\n",
    "\n",
    "    # Plot 1: Token Count vs Text Length\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(token_df['text_length'], token_df['gemma_tokens'], alpha=0.7)\n",
    "    plt.xlabel('Text Length (characters)')\n",
    "    plt.ylabel('Gemma Token Count')\n",
    "    plt.title('Tokenization Analysis: Gemma')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Token Fertility by Category\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.boxplot(x='category', y='fertility', data=token_df)\n",
    "    plt.title('Token Fertility by Category')\n",
    "    plt.ylabel('Tokens per Character')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Plot 3: Token Length Distribution\n",
    "    plt.subplot(2, 2, 3)\n",
    "    token_length_distribution = Counter(token_lengths)\n",
    "    filtered_dist = {k: v for k, v in token_length_distribution.items()\n",
    "                     if v > max(token_length_distribution.values()) * 0.01}\n",
    "    plt.bar(filtered_dist.keys(), filtered_dist.values())\n",
    "    plt.xlabel('Token Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Token Length Distribution (Filtered)')\n",
    "\n",
    "    # Plot 4: Compression Ratio Distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    compression_values = [1/f if f > 0 else 0 for f in token_df['fertility']]\n",
    "    plt.hist(compression_values, bins=20)\n",
    "    plt.xlabel('Compression Ratio (chars/token)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Compression Ratio Distribution')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tokenization_analysis.png')\n",
    "    plt.show()\n",
    "\n",
    "    return token_df\n",
    "\n",
    "\n",
    "# Comprehensive model evaluation function\n",
    "def evaluate_model_comprehensive(model, test_loader, device, label_names):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with comprehensive metrics:\n",
    "    - AUC, ROC curves\n",
    "    - Micro F1, Macro F1\n",
    "    - Precision, Recall (micro and macro)\n",
    "    - Accuracy\n",
    "    - Loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Use CrossEntropyLoss for loss calculation\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    test_start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating model\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = batch.pop('labels')\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Get predictions and probabilities\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().float().numpy())\n",
    "\n",
    "    test_time = time.time() - test_start_time\n",
    "\n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "\n",
    "    # Convert predictions and labels to numpy arrays\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.vstack(all_probs)\n",
    "\n",
    "    # 1. Accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    # 2-3. Precision (Micro and Macro)\n",
    "    precision_micro = precision_score(all_labels, all_preds, average='micro')\n",
    "    precision_macro = precision_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    # 4-5. Recall (Micro and Macro)\n",
    "    recall_micro = recall_score(all_labels, all_preds, average='micro')\n",
    "    recall_macro = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    # 6-7. F1 Score (Micro and Macro)\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # 8. AUC-ROC (One-vs-Rest for multiclass)\n",
    "    try:\n",
    "        # Convert labels to one-hot encoding for ROC AUC calculation\n",
    "        num_classes = len(np.unique(all_labels))\n",
    "        labels_one_hot = np.eye(num_classes)[all_labels]\n",
    "\n",
    "        # Calculate ROC AUC\n",
    "        roc_auc = roc_auc_score(labels_one_hot, all_probs, multi_class='ovr')\n",
    "\n",
    "        # Calculate ROC curves for plotting\n",
    "        fpr = {}\n",
    "        tpr = {}\n",
    "        roc_auc_per_class = {}\n",
    "\n",
    "        for i in range(num_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(labels_one_hot[:, i], all_probs[:, i])\n",
    "            roc_auc_per_class[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        # Plot ROC curves\n",
    "        plt.figure(figsize=(10, 8))\n",
    "\n",
    "        for i in range(num_classes):\n",
    "            plt.plot(\n",
    "                fpr[i],\n",
    "                tpr[i],\n",
    "                lw=2,\n",
    "                label=f'{label_names[i]} (AUC = {roc_auc_per_class[i]:.2f})'\n",
    "            )\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curves (One-vs-Rest, Overall AUC = {roc_auc:.2f})')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig('roc_curves.png')\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not calculate ROC AUC due to {str(e)}\")\n",
    "        roc_auc = None\n",
    "        roc_auc_per_class = None\n",
    "\n",
    "    # Class-wise metrics\n",
    "    class_metrics = {}\n",
    "    for i, label in enumerate(label_names):\n",
    "        class_precision = precision_score(\n",
    "            [1 if l == i else 0 for l in all_labels],\n",
    "            [1 if p == i else 0 for p in all_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "        class_recall = recall_score(\n",
    "            [1 if l == i else 0 for l in all_labels],\n",
    "            [1 if p == i else 0 for p in all_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "        class_f1 = f1_score(\n",
    "            [1 if l == i else 0 for l in all_labels],\n",
    "            [1 if p == i else 0 for p in all_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "\n",
    "        class_metrics[label] = {\n",
    "            'precision': class_precision,\n",
    "            'recall': class_recall,\n",
    "            'f1': class_f1\n",
    "        }\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nComprehensive Model Evaluation Results:\")\n",
    "    print(f\"  Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: Micro={precision_micro:.4f}, Macro={precision_macro:.4f}\")\n",
    "    print(f\"  Recall: Micro={recall_micro:.4f}, Macro={recall_macro:.4f}\")\n",
    "    print(f\"  F1 Score: Micro={f1_micro:.4f}, Macro={f1_macro:.4f}\")\n",
    "    if roc_auc is not None:\n",
    "        print(f\"  ROC AUC (OVR): {roc_auc:.4f}\")\n",
    "    print(f\"  Evaluation time: {test_time:.2f} seconds\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_micro': precision_micro,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_micro': recall_micro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'roc_auc': roc_auc,\n",
    "        'roc_auc_per_class': {label_names[i]: auc_val for i, auc_val in roc_auc_per_class.items()} if roc_auc_per_class else None,\n",
    "        'confusion_matrix': conf_matrix.tolist(),\n",
    "        'class_metrics': class_metrics,\n",
    "        'test_time': test_time\n",
    "    }\n",
    "\n",
    "    return results, all_preds, all_labels\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_model(model_name, train_df, test_df, output_dir=\"model_outputs\", use_lora=True, epochs=10):\n",
    "    \"\"\"Train and evaluate a news classification model.\"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get unique categories\n",
    "    categories = sorted(train_df['Category'].unique())\n",
    "    num_labels = len(categories)\n",
    "\n",
    "    # Configuration\n",
    "    batch_size = 1  # Small batch size due to model size\n",
    "    grad_accum_steps = 16  # Effective batch size = batch_size * grad_accum_steps\n",
    "    learning_rate = 2e-5\n",
    "    max_length = 512\n",
    "    model_save_path = os.path.join(output_dir, f\"{model_name.split('/')[-1]}_news_classifier\")\n",
    "\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Training {model_name} model for news classification\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Fix for the padding token issue\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"Set EOS token as padding token\")\n",
    "\n",
    "    # Split training data into train and validation\n",
    "    print(\"Preparing datasets...\")\n",
    "    train_df_split, val_df = train_test_split(\n",
    "        train_df,\n",
    "        test_size=0.1,\n",
    "        random_state=42,\n",
    "        stratify=train_df['Category']\n",
    "    )\n",
    "\n",
    "    print(f\"Train size: {len(train_df_split)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = NewsDataset(train_df_split, tokenizer, max_length)\n",
    "    val_dataset = NewsDataset(val_df, tokenizer, max_length)\n",
    "    test_dataset = NewsDataset(test_df, tokenizer, max_length)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"Initializing model...\")\n",
    "    if use_lora:\n",
    "        # Load base model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.bfloat16  # Use bfloat16 to save memory\n",
    "        )\n",
    "\n",
    "        # Set padding token id in the model config\n",
    "        if model.config.pad_token_id is None:\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        # Define LoRA configuration\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            inference_mode=False,\n",
    "            r=16,  # rank\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # Include more attention modules\n",
    "        )\n",
    "\n",
    "        # Create PEFT model\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.bfloat16  # Use bfloat16 to save memory\n",
    "        )\n",
    "\n",
    "        # Set padding token id in the model config\n",
    "        if model.config.pad_token_id is None:\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Move model to GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_loader) * epochs // grad_accum_steps\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=total_steps // 10,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_f1s = []\n",
    "    best_val_f1 = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\")):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / grad_accum_steps  # Normalize loss for gradient accumulation\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            train_loss += loss.item() * grad_accum_steps\n",
    "\n",
    "            # Update weights after accumulating gradients\n",
    "            if (step + 1) % grad_accum_steps == 0 or step == len(train_loader) - 1:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        valid_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                labels = batch.pop('labels')\n",
    "\n",
    "                outputs = model(**batch)\n",
    "                # Handle the case where loss is None\n",
    "                if outputs.loss is not None:\n",
    "                    loss = outputs.loss\n",
    "                    val_loss += loss.item()\n",
    "                    valid_batches += 1\n",
    "                else:\n",
    "                    # If loss is None, skip this batch for loss calculation\n",
    "                    pass\n",
    "\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Avoid division by zero if all batches had None loss\n",
    "        if valid_batches > 0:\n",
    "            val_loss /= valid_batches\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Calculate metrics\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        val_precision = precision_score(all_labels, all_preds, average='macro')\n",
    "        val_recall = recall_score(all_labels, all_preds, average='macro')\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Accuracy: {val_acc:.4f}\")\n",
    "        print(f\"  Val F1 (macro): {val_f1:.4f}\")\n",
    "        print(f\"  Val Precision: {val_precision:.4f}\")\n",
    "        print(f\"  Val Recall: {val_recall:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            # Save model\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            model_to_save.save_pretrained(model_save_path)\n",
    "            tokenizer.save_pretrained(model_save_path)\n",
    "            print(f\"  Model saved to {model_save_path}\")\n",
    "\n",
    "        # Early stopping check (optional)\n",
    "        if epoch > 2 and val_losses[-1] > val_losses[-2] and val_losses[-2] > val_losses[-3]:\n",
    "            print(\"Early stopping triggered - validation loss increasing for 3 consecutive epochs\")\n",
    "            break\n",
    "\n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(val_losses, 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_f1s, 'g-', label='Validation F1')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Validation F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{model_name.split(\"/\")[-1]}_training_curve.png'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    if use_lora:\n",
    "        # For LoRA we need to load the PEFT model\n",
    "        config = PeftConfig.from_pretrained(model_save_path)\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            config.base_model_name_or_path,\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        # Set padding token id in the model config\n",
    "        if base_model.config.pad_token_id is None:\n",
    "            base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        best_model = PeftModel.from_pretrained(base_model, model_save_path)\n",
    "    else:\n",
    "        best_model = AutoModelForSequenceClassification.from_pretrained(model_save_path)\n",
    "        # Set padding token id in the model config\n",
    "        if best_model.config.pad_token_id is None:\n",
    "            best_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "\n",
    "    # Evaluate with comprehensive metrics\n",
    "    eval_results, test_preds, test_labels = evaluate_model_comprehensive(\n",
    "        best_model,\n",
    "        test_loader,\n",
    "        device,\n",
    "        categories\n",
    "    )\n",
    "\n",
    "    # Save results to file\n",
    "    with open(os.path.join(output_dir, f'{model_name.split(\"/\")[-1]}_results.json'), 'w') as f:\n",
    "        json.dump(eval_results, f, indent=2)\n",
    "\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnEYATJ3ikXI"
   },
   "outputs": [],
   "source": [
    "# Main pipeline function\n",
    "def run_pipeline(train_df, test_df, run_tokenization=True, epochs=10):\n",
    "    \"\"\"Run the news classification pipeline with Gemma.\"\"\"\n",
    "    # Show dataset information\n",
    "    print(f\"Training set: {len(train_df)} samples\")\n",
    "    print(\"Training category distribution:\")\n",
    "    print(train_df['Category'].value_counts())\n",
    "    print(f\"\\nTest set: {len(test_df)} samples\")\n",
    "    print(\"Test category distribution:\")\n",
    "    print(test_df['Category'].value_counts())\n",
    "\n",
    "    # Analyze tokenization if requested\n",
    "    if run_tokenization:\n",
    "        print(\"\\n=== Running Tokenization Analysis ===\")\n",
    "        token_df = analyze_tokenization(train_df, test_df)\n",
    "\n",
    "    # Train and evaluate Gemma model\n",
    "    model_name = \"google/gemma-7b\"\n",
    "    results = train_model(model_name, train_df, test_df, epochs=epochs)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "059a36a733bf4d07b4b512fe10212f35",
      "db119a5f410c4a96b95a8a5b4c7ca7c7",
      "76d1f5b3e4fb42e4b6e782667655015d",
      "5343dc08770b49ceb55121464205b49f",
      "b875aad76bc6499888567dfa265eef37",
      "f69aa84457564b8586dc24796309d073",
      "16aa85615db849e0a63787e6dbad17c0",
      "08d5ef32b1e4459dac71551c0923f16f",
      "1ee798f7beb84b219f75c56c5cdd8d9e",
      "4ec982bab6c44f4dbb670227ed4887bc",
      "476d7360cdbf49448c11c83a07ef9db7",
      "445806ffd46941668dfddc6209cb1d0a",
      "0781daa361b64fbebc3ad31cd878a7fb",
      "3b936ed3cafb41ccb75e154ed54331e0",
      "79010a64dbc640ee9bbd11948bb9db0b",
      "e61c423500c34789bf475ed7e0737a2d",
      "1c6c27e297014a7087e3ae356d02daa1",
      "67b66e4ee0644fc58df5d31a2583d8d4",
      "92ead1fdf24149eda2f6f9a816b90ba3",
      "f7bc1f6e7ba44bd8964aadabc09d82db",
      "84a894fb1f7247d9bebcc5f66a579472",
      "54695d0545a74b76aed558d82c43eba2",
      "41e2b859315647eb840e683dd2fcdde5",
      "168cf125024a418f89fb5a4d3d77c85e",
      "5da455db469a401e85f5d5f823b298b5",
      "ac0aef3a40474c0a9f2745b8020a4028",
      "f9a69d9d5cc6429a8cca2360f662b4c0",
      "ca29e8561faf432eb645c3897beccaf1",
      "bc2a5b2034714cb58b3cadf95e4415c3",
      "7943c593810d408eb9e75eca1c929f0e",
      "b6656021686d43a9a3a1e913c16d6b60",
      "d1ba3a3da41546bdbe63571eb2a4af98",
      "cfa89cb6e98147d4949e18d936e5d01c",
      "ad5803615b844595996db63736ceedd9",
      "9644bb90361943d3810436c67c863656",
      "59865d120aaf46b5b1e3bd1d6804ff53",
      "c1bd1ae2c08642febddd8553e81dcf66",
      "a66c519f131449fab4df72a27d2a279a",
      "7026fe40584e4ef7961fe6100b5de5f3",
      "3a2734081b1e4414a31f4344d77a2c63",
      "936d9401801c4a5abe01e2695e33cc6f",
      "049e220a33c843028f97ed287e455dfa",
      "f770e15a6ac84aafa327aa28590b8979",
      "ef1e3bfa87f9478eaccf76dcc5a2b047",
      "defbeb6de29943909385f8fd3efc5ab7",
      "95ddc204687e4637a48e25f99359be1c",
      "c4665e01efd44bd9a936dad04e412c5d",
      "bfd9efbeb3e4479db057a1c2417e9b55",
      "a8bdf1c0564b4530ab92b893ee2cf81c",
      "22dc9d82784448adb794e06d9aa9c2e1",
      "3270b3a70ef448dda4daa2202099d740",
      "8601ef052dd04562bfb591f40460f7fd",
      "e4fcaaf8f0804842a169f0fda3a4c38b",
      "153db7e1a2394755984d04db3c4670af",
      "003a35dacd6a4c6a9299f4aab0015b5c",
      "bdf755f4f658485daefb49498db54eb3",
      "389d3ddea18444ffa72ce1c21dbecfc9",
      "bbb613ab010c44329b19496fa5de2ba0",
      "77b5dfa2a9a84977858409ebc19f74cc",
      "fa1b27cd33d54f31899a3fe58b080032",
      "237edd62d8f6417fb7ce788b2c0d3e33",
      "8b4f3e58fe3d4d4d98247742713859c1",
      "61daac66cc584105907b5047793954ce",
      "e278ba8cb39b4a71a9a2c09ef583134c",
      "2c60f1aabe284684a83f4e0c3b0626b1",
      "d4c648e606c3422ab4a6a130ceb76ba7",
      "9ca612193f7844e098326ece29da22a9",
      "a36bef2a67694117a2b7de8041309111",
      "1dcc6f2a204649dcb530e66c60a02fe3",
      "c31d1e836e754b6aa382d39edc111186",
      "c3176b4155ba4b02907d6dc81a2db048",
      "1e7a3518878a4f3f89e881e96c0bf690",
      "66f292048a1f4257a7ca4438ea53a05d",
      "af4b9a1149e849fdab1ab55a2585c69e",
      "9c1ad68f50d24c128616942845387332",
      "2f4128ba5d294badbc04a57e5d060f95",
      "93706f1c446d4138aa564a4d7a799c4f",
      "7c51a7f8f7784caf8458d40727be5c1a",
      "b7de25f52b6546679c562cc7ccd01eef",
      "b7624356e27540fa8cb71741c0d28340",
      "0a7aab96f6f54b5cb3b0c87413075f29",
      "8d98c830dcc243ed9e8950120ecfb827",
      "6b31468edd814f30a4ae81ef402c0f7e",
      "7c6a8a5c6bc24c9b96f847c289e0715f",
      "66e634adccca44359d34fe3773fa727c",
      "fe63bbde4b1944ed9a11170ca0b45838",
      "92794342aad74e77a063446bc371f664",
      "d7967d9baf0547fda831a7598bffa2c3",
      "a8df37ce88f843c9a4f33a490d7a5836",
      "e5ea6c4e2b6643d29d07dda90cd2794e",
      "d782d597a3044b5584d845d6be79e973",
      "05482cc099b145e8804f9c66f0c8df26",
      "a2bc9708c686418da6d8f9540de13762",
      "95a040dd17b64b89a7530c93442997d5",
      "b6f9a3ba41134e27a96339f1c81d5c6f",
      "a7ba7f042e374ae995d8ccc87f1397b1",
      "26beb6c7e95e464db38a832cfe4f5fb3",
      "d8a4638ffb1640a98f09d01d267fad7f",
      "2146376571794b679a69d315e5a0e290",
      "f3e3ce9122ea40c9a3935d5f7c794942",
      "481e7e893258448f85f5a58c037d4b3f",
      "6964a8b6fb4e45748647a4f887b153e4",
      "f0ce1f6cda5e41c48885dd6dd8009964",
      "67044632c88e4d92bbd8307ac8d68933",
      "1d8d1ea4bfae426fabb53b726d7977da",
      "f288fed973e54b2785de5aa00ecc7af7",
      "25437b320fe04b60a383d15e39b6979c",
      "05e8e10adfd542e7926712ab79390085",
      "995e795bf7e0438cb0bd4ea0f06b0d61",
      "3270aad4f21545188d9556df983e298a",
      "c2a2897b51e049d8ab29878d21a1f26b",
      "1d1c64645fa641e79904f855bff6f20b",
      "b95445c6d1ff4ada929f3e50d1c2c346",
      "cf9f6ff1dff44cf39478f25496ebb7ad",
      "d4408c3066e1413a973e4819118c2b05",
      "9b07203198a345b795ac456270818825",
      "630807a255bf438aaa512afbde7bf976",
      "1ce7d891b4044f7daad6b610add1e7be",
      "fd84e257e8894c7ebb9dee8e7e245c63",
      "20057ce1e51b44f3a238e1efdb5bb83e",
      "f1525f5b8a5e4b0ab642501ca978a2a4",
      "999e75a4706f4a108486cf682f882eea",
      "004624cf334b4e3b9e08728555d34ced",
      "e207fd4c1ad34543b4da7af920a1879c",
      "cea663a88bf04d12ac956ab89e55e7eb",
      "b9c66e4e5baa4757b327118e66813613",
      "4447825a91fc4af3a0fd2e282ac0d4ec",
      "c7e33fdff47f43ca967065422b677d68",
      "c1fbb89bc53847eca4ad3bddcd38bee1",
      "d0a3e46c6f3648018341027b34789e11",
      "93cc9390caad40a5ba0115d2d7af9a3b",
      "6be0f489790c4225ac53f2b45742605b",
      "ffce57226b2b4b968d8b089c79354f75",
      "9c149cdf80d547879a6c41a662ecb63a",
      "4b8e08fe6d8d4f0b90ce5ff3155e44f4",
      "fa92c296c595474aaf72e9bc8a617946",
      "bf4dfc8913a0478c9744c064439abe1d",
      "c01746d2e0a1412ca6a5db1037a636d2",
      "84b1e24c48e946a1ae25d0715fee1aa2",
      "dd5f34ebce804664822074206b367a83",
      "741ede93f6984f629f4323bea942590b",
      "401b3062f9bd4898b7332e8637d18c18",
      "c63e1e903cad4507b0acb7d3a1ea11a4"
     ]
    },
    "id": "aMogJwbyi13S",
    "outputId": "afc721fe-fa67-4390-980e-0e3173d8a86e"
   },
   "outputs": [],
   "source": [
    "# Run the complete pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the dataset from Hugging Face\n",
    "    print(\"Loading dataset from Hugging Face...\")\n",
    "    dataset = load_dataset(\"okite97/news-data\")\n",
    "\n",
    "    # Convert to pandas DataFrames\n",
    "    df_train = pd.DataFrame(dataset['train'])\n",
    "    df_test = pd.DataFrame(dataset['test'])\n",
    "\n",
    "    # This will run the entire pipeline\n",
    "    results = run_pipeline(df_train, df_test, run_tokenization=True, epochs=10)\n",
    "\n",
    "    # Print summary of results\n",
    "    print(\"\\n=== Final Results Summary ===\")\n",
    "    print(f\"Model: google/gemma-7b\")\n",
    "    print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"F1 Score (macro): {results['f1_macro']:.4f}\")\n",
    "    print(f\"F1 Score (micro): {results['f1_micro']:.4f}\")\n",
    "    print(f\"Precision (macro): {results['precision_macro']:.4f}\")\n",
    "    print(f\"Recall (macro): {results['recall_macro']:.4f}\")\n",
    "    if results['roc_auc'] is not None:\n",
    "        print(f\"ROC AUC: {results['roc_auc']:.4f}\")\n",
    "\n",
    "    print(\"\\nCategory-specific results:\")\n",
    "    for label, metrics in results['class_metrics'].items():\n",
    "        print(f\"  {label}: F1={metrics['f1']:.4f}, Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
