{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17163,
     "status": "ok",
     "timestamp": 1743443794683,
     "user": {
      "displayName": "Sadaf Mekan",
      "userId": "08346187652630117368"
     },
     "user_tz": -60
    },
    "id": "Qn1C6ZdqATlp",
    "outputId": "bab725a4-2db4-4a21-c155-4150797533d4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "data_path = \"/content/drive/MyDrive/AraStance\"\n",
    "train_file = os.path.join(data_path, \"train.jsonl\")\n",
    "test_file = os.path.join(data_path, \"test.jsonl\")\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "df_train = load_jsonl(train_file)\n",
    "df_test = load_jsonl(test_file)\n",
    "\n",
    "df_train = pd.DataFrame(df_train)\n",
    "df_test = pd.DataFrame(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 84,
     "status": "ok",
     "timestamp": 1743443837925,
     "user": {
      "displayName": "Sadaf Mekan",
      "userId": "08346187652630117368"
     },
     "user_tz": -60
    },
    "id": "2oLJrwpPA2wE",
    "outputId": "b6d2c5bd-b84c-4ad1-ac30-bdee77b9cf58"
   },
   "outputs": [],
   "source": [
    "import html\n",
    "import random\n",
    "\n",
    "pairs = []\n",
    "for i, row in df_train.iterrows():\n",
    "    claim = row['claim']\n",
    "    for j, (article, stance) in enumerate(zip(row['article'], row['stance'])):\n",
    "        article_title = row['article_title'][j] if isinstance(row['article_title'], list) else row['article_title']\n",
    "        pairs.append({\n",
    "            'claim': claim,\n",
    "            'article': article,\n",
    "            'article_title': article_title,\n",
    "            'stance': stance\n",
    "        })\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs)\n",
    "\n",
    "# Add these lines to see the output\n",
    "print(f\"Total claim-article pairs: {len(pairs_df)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(pairs_df.head())\n",
    "\n",
    "# Count the stance labels\n",
    "print(\"\\nStance distribution:\")\n",
    "print(pairs_df['stance'].value_counts())\n",
    "\n",
    "# Show one example of each stance\n",
    "print(\"\\nExamples of each stance:\")\n",
    "for stance in [\"Agree\", \"Disagree\", \"Discuss\", \"Unrelated\"]:\n",
    "    if stance in pairs_df['stance'].values:\n",
    "        example = pairs_df[pairs_df['stance'] == stance].iloc[0]\n",
    "        print(f\"\\n--- {stance} Example ---\")\n",
    "        print(f\"Claim: {example['claim'][:100]}...\")\n",
    "        print(f\"Article Title: {example['article_title']}\")\n",
    "\n",
    "# Explode the test dataset into claim-article pairs\n",
    "test_pairs = []\n",
    "for i, row in df_test.iterrows():\n",
    "    claim = row['claim']\n",
    "    for j, (article, stance) in enumerate(zip(row['article'], row['stance'])):\n",
    "        article_title = row['article_title'][j] if isinstance(row['article_title'], list) else row['article_title']\n",
    "        test_pairs.append({\n",
    "            'claim': claim,\n",
    "            'article': article,\n",
    "            'article_title': article_title,\n",
    "            'stance': stance\n",
    "        })\n",
    "\n",
    "test_pairs_df = pd.DataFrame(test_pairs)\n",
    "print(f\"Total claim-article pairs in test set: {len(test_pairs_df)}\")\n",
    "print(\"\\nStance distribution in test set:\")\n",
    "print(test_pairs_df['stance'].value_counts())\n",
    "\n",
    "# Clean HTML entities from a sample\n",
    "for stance in [\"Agree\", \"Disagree\", \"Discuss\", \"Unrelated\"]:\n",
    "    stance_examples = pairs_df[pairs_df['stance'] == stance]\n",
    "\n",
    "    if len(stance_examples) > 0:\n",
    "        random_index = random.randint(0, len(stance_examples) - 1)\n",
    "        example = stance_examples.iloc[random_index]\n",
    "\n",
    "        # Clean HTML entities\n",
    "        cleaned_title = html.unescape(example['article_title'])\n",
    "        cleaned_article = html.unescape(example['article'][:150])\n",
    "\n",
    "        print(f\"\\n--- Random {stance} Example (Cleaned) ---\")\n",
    "        print(f\"Claim: {example['claim']}\")\n",
    "        print(f\"Article Title: {cleaned_title}\")\n",
    "        print(f\"Article Excerpt: {cleaned_article}...\")\n",
    "        print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98517,
     "status": "ok",
     "timestamp": 1743443940981,
     "user": {
      "displayName": "Sadaf Mekan",
      "userId": "08346187652630117368"
     },
     "user_tz": -60
    },
    "id": "nZd85AKXA3L9",
    "outputId": "e6bc3bf3-8ae1-47b2-b3e5-c77623b8f0a9"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade huggingface_hub\n",
    "!pip install transformers==4.49.0\n",
    "!pip install accelerate\n",
    "!pip install peft==0.5.0\n",
    "!pip install datasets\n",
    "!pip install bitsandbytes==0.38.2\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6563,
     "status": "ok",
     "timestamp": 1743444016886,
     "user": {
      "displayName": "Sadaf Mekan",
      "userId": "08346187652630117368"
     },
     "user_tz": -60
    },
    "id": "i_u3Ec2tBC4n",
    "outputId": "f25bb64d-6adf-4a4f-c8df-ce0dc44c43a1"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login\n",
    "\n",
    "# 3. Import libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import html\n",
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    print(\"CUDA is available! Using GPU.\")\n",
    "else:\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfpEKRGEBEl7"
   },
   "outputs": [],
   "source": [
    "class StanceDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Map stance labels to integers\n",
    "        self.label_map = {'Agree': 0, 'Disagree': 1, 'Discuss': 2, 'Unrelated': 3}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        claim = html.unescape(row['claim'])\n",
    "        article = html.unescape(row['article'])\n",
    "\n",
    "        # Truncate article if needed to fit within max_length\n",
    "        if len(article) > 5000:  # Arbitrary limit to avoid very long sequences\n",
    "            article = article[:5000]\n",
    "\n",
    "        # Combine claim and article\n",
    "        text = f\"Claim: {claim} Article: {article}\"\n",
    "\n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Remove batch dimension added by tokenizer\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "\n",
    "        # Add label\n",
    "        encoding['labels'] = torch.tensor(self.label_map[row['stance']])\n",
    "\n",
    "        return encoding\n",
    "\n",
    "# Basic Tokenization Analysis Function\n",
    "def analyze_tokenization(pairs_df, test_pairs_df, sample_size=100):\n",
    "    \"\"\"Analyze tokenization differences for Gemma model.\"\"\"\n",
    "    print(\"Loading tokenizer for tokenization analysis...\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\", use_fast=True)\n",
    "\n",
    "    # Sample data for analysis\n",
    "    combined_df = pd.concat([pairs_df, test_pairs_df])\n",
    "    sample_df = combined_df.sample(sample_size, random_state=42)\n",
    "\n",
    "    print(f\"Analyzing tokenization on {sample_size} samples...\")\n",
    "\n",
    "    # Analyze tokenization\n",
    "    results = []\n",
    "    for _, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "        # Clean HTML entities\n",
    "        claim = html.unescape(row['claim'])\n",
    "        article = html.unescape(row['article'][:500])  # Truncate article for speed\n",
    "\n",
    "        # Prepare text (combine claim and article for stance detection)\n",
    "        text = f\"Claim: {claim} Article: {article}\"\n",
    "\n",
    "        # Tokenize with Gemma\n",
    "        gemma_tokens = tokenizer.encode(text)\n",
    "\n",
    "        results.append({\n",
    "            'text_length': len(text),\n",
    "            'gemma_tokens': len(gemma_tokens),\n",
    "            'stance': row['stance']\n",
    "        })\n",
    "\n",
    "    token_df = pd.DataFrame(results)\n",
    "\n",
    "    # Visualize tokenization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(token_df['text_length'], token_df['gemma_tokens'], alpha=0.7)\n",
    "    plt.xlabel('Text Length (characters)')\n",
    "    plt.ylabel('Gemma Token Count')\n",
    "    plt.title('Tokenization Analysis: Gemma')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('tokenization_analysis.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Token statistics\n",
    "    print(\"\\nAverage tokens per example:\")\n",
    "    print(f\"Gemma: {token_df['gemma_tokens'].mean():.1f} tokens\")\n",
    "\n",
    "    # Tokens by stance\n",
    "    print(\"\\nTokens by stance category:\")\n",
    "    for stance in token_df['stance'].unique():\n",
    "        subset = token_df[token_df['stance'] == stance]\n",
    "        print(f\"\\n{stance}:\")\n",
    "        print(f\"  Gemma 3.1: {subset['gemma_tokens'].mean():.1f} tokens\")\n",
    "\n",
    "    return token_df\n",
    "\n",
    "# Enhanced Tokenization Analysis with Fertility Metrics\n",
    "def analyze_tokenization_fertility(pairs_df, test_pairs_df, sample_size=100):\n",
    "    \"\"\"Analyze tokenization fertility and OOV rates for Arabic text with Gemma.\"\"\"\n",
    "    print(\"Loading tokenizer for detailed tokenization analysis...\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\", use_fast=True)\n",
    "\n",
    "    # Sample data for analysis\n",
    "    combined_df = pd.concat([pairs_df, test_pairs_df])\n",
    "    sample_df = combined_df.sample(sample_size, random_state=42)\n",
    "\n",
    "    # Initialize metrics\n",
    "    results = []\n",
    "\n",
    "    for _, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Analyzing token fertility\"):\n",
    "        # Clean text\n",
    "        claim = html.unescape(row['claim'])\n",
    "        article = html.unescape(row['article'][:500])  # Truncate for speed\n",
    "\n",
    "        # Tokenize\n",
    "        claim_tokens = tokenizer.encode(claim, add_special_tokens=False)\n",
    "        article_tokens = tokenizer.encode(article, add_special_tokens=False)\n",
    "\n",
    "        # Calculate fertility (tokens per character)\n",
    "        claim_fertility = len(claim_tokens) / len(claim) if len(claim) > 0 else 0\n",
    "        article_fertility = len(article_tokens) / len(article) if len(article) > 0 else 0\n",
    "\n",
    "        # Calculate fragmentation (tokens per word) - rough estimate for Arabic\n",
    "        claim_words = len(claim.split())\n",
    "        article_words = len(article.split())\n",
    "        claim_fragmentation = len(claim_tokens) / claim_words if claim_words > 0 else 0\n",
    "        article_fragmentation = len(article_tokens) / article_words if article_words > 0 else 0\n",
    "\n",
    "        # Estimate OOV by checking for multi-token words\n",
    "        # In Arabic, words broken into many tokens often indicate OOV issues\n",
    "        claim_words_list = claim.split()\n",
    "        article_words_list = article.split()\n",
    "\n",
    "        # Track highly fragmented words (potential OOVs)\n",
    "        highly_fragmented_words = []\n",
    "        for word in claim_words_list + article_words_list:\n",
    "            if len(word) >= 3:  # Only check non-trivial words\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                if len(word_tokens) >= 3:  # If a word is broken into 3+ tokens\n",
    "                    highly_fragmented_words.append(word)\n",
    "\n",
    "        results.append({\n",
    "            'claim_tokens': len(claim_tokens),\n",
    "            'article_tokens': len(article_tokens),\n",
    "            'claim_fertility': claim_fertility,\n",
    "            'article_fertility': article_fertility,\n",
    "            'claim_fragmentation': claim_fragmentation,\n",
    "            'article_fragmentation': article_fragmentation,\n",
    "            'highly_fragmented_words': len(highly_fragmented_words),\n",
    "            'highly_fragmented_word_examples': highly_fragmented_words[:5] if highly_fragmented_words else [],\n",
    "            'stance': row['stance']\n",
    "        })\n",
    "\n",
    "    token_df = pd.DataFrame(results)\n",
    "\n",
    "    # Display statistics\n",
    "    print(\"\\n=== Tokenization Fertility Analysis ===\")\n",
    "    print(f\"Average claim fertility: {token_df['claim_fertility'].mean():.4f} tokens/char\")\n",
    "    print(f\"Average article fertility: {token_df['article_fertility'].mean():.4f} tokens/char\")\n",
    "    print(f\"Average claim fragmentation: {token_df['claim_fragmentation'].mean():.2f} tokens/word\")\n",
    "    print(f\"Average article fragmentation: {token_df['article_fragmentation'].mean():.2f} tokens/word\")\n",
    "    print(f\"Average highly fragmented words: {token_df['highly_fragmented_words'].mean():.2f} per sample\")\n",
    "\n",
    "    # Print some example highly fragmented words (potential OOVs)\n",
    "    if len([w for row in token_df['highly_fragmented_word_examples'] for w in row]) > 0:\n",
    "        print(\"\\nExample highly fragmented words (potential OOVs):\")\n",
    "        all_examples = [w for row in token_df['highly_fragmented_word_examples'] for w in row]\n",
    "        for word in random.sample(all_examples, min(10, len(all_examples))):\n",
    "            tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "            print(f\"  '{word}' → {len(tokens)} tokens\")\n",
    "\n",
    "    # Visualize fertility metrics\n",
    "    plt.figure(figsize=(16, 12))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.boxplot(x='stance', y='claim_fertility', data=token_df)\n",
    "    plt.title('Claim Fertility by Stance')\n",
    "    plt.ylabel('Tokens per Character')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.boxplot(x='stance', y='article_fertility', data=token_df)\n",
    "    plt.title('Article Fertility by Stance')\n",
    "    plt.ylabel('Tokens per Character')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.boxplot(x='stance', y='claim_fragmentation', data=token_df)\n",
    "    plt.title('Claim Fragmentation by Stance')\n",
    "    plt.ylabel('Tokens per Word')\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.boxplot(x='stance', y='highly_fragmented_words', data=token_df)\n",
    "    plt.title('Potential OOV Words by Stance')\n",
    "    plt.ylabel('Count of Highly Fragmented Words')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tokenization_fertility_analysis.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Analyze token distribution\n",
    "    claim_token_counts = Counter([t for row in sample_df['claim'] for t in tokenizer.encode(row, add_special_tokens=False)])\n",
    "    article_token_counts = Counter([t for row in sample_df['article'].apply(lambda x: x[:500]) for t in tokenizer.encode(row, add_special_tokens=False)])\n",
    "\n",
    "    print(\"\\nToken frequency analysis:\")\n",
    "    print(f\"Unique tokens in claims: {len(claim_token_counts)}\")\n",
    "    print(f\"Unique tokens in articles: {len(article_token_counts)}\")\n",
    "    print(f\"Token overlap: {len(set(claim_token_counts.keys()) & set(article_token_counts.keys()))}\")\n",
    "\n",
    "    # Save results to file\n",
    "    with open('tokenization_analysis_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            'avg_claim_fertility': token_df['claim_fertility'].mean(),\n",
    "            'avg_article_fertility': token_df['article_fertility'].mean(),\n",
    "            'avg_claim_fragmentation': token_df['claim_fragmentation'].mean(),\n",
    "            'avg_article_fragmentation': token_df['article_fragmentation'].mean(),\n",
    "            'avg_oov_count': token_df['highly_fragmented_words'].mean(),\n",
    "            'unique_claim_tokens': len(claim_token_counts),\n",
    "            'unique_article_tokens': len(article_token_counts),\n",
    "            'token_overlap': len(set(claim_token_counts.keys()) & set(article_token_counts.keys()))\n",
    "        }, f, indent=2)\n",
    "\n",
    "    return token_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e565868d98a448c4845befa2dcec7589",
      "19568b9c6bfc4cd38ae492a49a72fe16",
      "43a3c79527b845f0bb4d45f19f10c475",
      "2d29ab41c9614efe969fd32b9a99ba4c",
      "cbe187a107e44572a220ee88efffeba6",
      "c8edb837e8e94f20b945810692856dfb",
      "e05c2aa902f34636a149de7c64a64923",
      "5f9d292a83794d0faa602ddb6174f342",
      "22c91ec675124f9eafda835eb842aa6d",
      "8fccd862f22447dda01587c539126892",
      "77713a62f27749e59db7ccffa2b49af3",
      "415f11d987dc462dbf7da25cba231d90",
      "ade291d17e9c4ee59f09fb0d607769d3",
      "e6fa9890248b47b393651a2d4130f206",
      "b5e1ff147986423eb55154d12ab44c30",
      "f5b76385c65e4c21af53fc01283de013",
      "9e765ea1dcf84a9788c18ae3a9694fcf",
      "5667a1ef8eac4e3f8c08320f683c58f8",
      "5780540afddd491e8303b72f1788f92d",
      "bd5ed0f95a59464ea9cc09152a01a5ac",
      "b736376da52e4d3486676342f2bf6baa",
      "7bc4d627658f446c8c079723d6277b4d",
      "34c18fa2b43c48b19e65b434e8621345",
      "9dba70261e3c451a85b7e71cc696f1bd",
      "26c3a64f76b843cf8ed0d79f71fe4ff6",
      "c6c2c2cf12de4fb7ae2fd97795a8a2e6",
      "c437c369db2a44b5a32bde97f65ace76",
      "9e14bb9c9636480d95bac5fed1670653",
      "478b42fd6f8848b087715ac78b989e69",
      "45314629f0df4415899c2ac726de40a5",
      "b8d3689c3f8549aaac61882f373b4637",
      "4d26fccbf1834943a48465414967635e",
      "a9ca18c0b4eb4360b730f0d13b6bb45c",
      "e182984b37dc4b468315ebe7ccfdad5e",
      "52eaf072c6cc4810bb1f52217942b588",
      "a21e4816fa2043b3b93be85d87bd18bf",
      "93789971602b420b93523faceb6bdfe8",
      "7276ab8e5639450886742084589d5608",
      "7af6941bb008483391534a770e751839",
      "4d948b5a010043a29d60a749399942b7",
      "7a702641d2154d1f810be7313e429184",
      "1cca06c459654c2791a3e7973d2872dd",
      "dabdf56e620b494fa5476f0d71017ba0",
      "13cd1f75693949cbbc95f1947b68fdb3",
      "0a3fc5ff8c5b4dcc9cc015c41994e17d",
      "e2d56e9916f8406b854ae27ff94b35f4",
      "115eed44ddbe48b89d8f2cb26a0551d9",
      "610bc21743c341e9bcb8eb59315fd6ff",
      "46ac90508823435d9f494a60e6ac5a38",
      "bc7238d193ed4937ab8c44cc3ec2b965",
      "7b78b2faf80744aab4d73bc9a303673a",
      "ef0c1775283847d088dd16ce8b2d77cf",
      "403d5b6d01ee4390a48f80afc54467b1",
      "da1777a488904be9b6561f36785647e8",
      "f9a20cbceef1494a82577f95f65640a7",
      "9332535d59b84dfcbc3b34528a6c60d0",
      "bdaed3a4810945969f1d2be34c543672",
      "08f4f8dfc56044fcbd5bef005a4f7dcf",
      "289d3ff1697843ad9ddd47a41d0b2dd3",
      "7887f7fdc9a240b8ad8b655d1e29d2f4",
      "7bc1f3973e2245dd8da1b1fb084df51a",
      "201685cfbe92468384931787aac067ba",
      "ad1cb13521714243b23c2525772a3793",
      "3a600a68cb134a4aa5472cf0d41f7840",
      "0d120ee145484894a14461f7eee9b3da",
      "81a1a325ff544cb5b2304cafd724f2f6",
      "7d2ca6a11a084e788cad563ab14f7c27",
      "7b2f816915d34178a26718c9cdba61aa",
      "2228c3e760334caabe650221c6851200",
      "e5646d93d7234c5eb26a91a15916afad",
      "239bd54e7d4c4748a031235c8ff38957",
      "30ac098bb63940d99b107619053d0d4f",
      "7a58ab00aecd4e68b7cd6beabfe4c2f3",
      "a7cab606124940ca9ffcb9ccb8fb1184",
      "e204dfd2475e4f64a41340a87e029372",
      "caeb6b918b9a46cd937c624e3905c32f",
      "d4c2b51f82224bafa6302a0e02c42f79",
      "fff84beef7044a14a5b5bb0296903f20",
      "0a892992fe6a4f108d0685b5725cad57",
      "36bdb38b9e9d419b976f9dad8e691ef6",
      "e118272d4f7144a79858ebac99223ad7",
      "7e4036dac63847cba328b64e48a4431d",
      "fc4e38fce9924d0f88e96f80c824c43a",
      "fd50bfca58a443c99f5bdd8e74c5283a",
      "aa01ed45e9dd461b80e5ac9e0469ff0e",
      "bad38765cf364aa581dc89f1a66bce14",
      "9f411019c23b46baaae2124e202fa329",
      "48f728210510461794a25beb2817a3ea",
      "ccb232415310443bb15b69cf6728404c",
      "0ed981fc098a4ffb8e84c927a51b987c",
      "8366795283fd4cc3928004ff1b1d0e2d",
      "357a364077694c079ec1cd06d9b865d2",
      "944dd168d1934fef8e10a543e0212d3a",
      "6d36d47f6b9f4da4a2060bd37fca2724",
      "f5499f78439b4f689771b4972d675e78",
      "e2fa6101659f4018a13330f78b72fb96",
      "73bc823605fc41599dd569e1aee796ed",
      "fe280eba699e43f0bdc4fe3c75ecda80",
      "52a1a178f18b43b8b4821946b8a2020a",
      "2d48c483bcdf40a9bedc0bed2c87ccb9",
      "07637a5a512e43caa50e76b719b36b07",
      "da78058ce35c4a22ba376b0c1693ce9c",
      "9f0497fafa8c47108be3b2e1339a5926",
      "833d4832dc424904b51b250210e2e0ed",
      "efad16b0c7864d7f83651809a401af6b",
      "cec88dfc70c24e7296fec1127df943d9",
      "4637bd28a1d24666a13785ad525e838a",
      "a0a0ab561c4a46139e63ad497b21fa30",
      "7dd10359f2df4876ab3187690186ac24",
      "93da55ddbd0d4353abc0873b532dca65",
      "f62786ce843348e3ae1b3fd423d5c0c2",
      "e1a95cb6ec1f40e4aa119ca59aba43cf",
      "a5ced8b976824eb4800d7271ac46daf4",
      "37611f371ddf44e3a84dc32c87b6fc65",
      "750318eb4dfd4e5fbac72a30294e1d00",
      "01b09a98ee4449f283ebedf1bd0d8dda",
      "dffe0b16548a41b29a36941eb1b5c1ea",
      "149f0a7d0042478e9b90fdd28b7eedd2",
      "fbead779f83c402184e315210ec181db",
      "91b993f285394cc6a5dad3f94a21474e",
      "243ecce4fc8042e793d9311220001329",
      "fc9bfc72cc174a82b315faf0e7daad9e",
      "33a66ced3c1c4da1a8c0be2afc852699",
      "1bc58ecd03e940bcab9aef64f22ad1b5",
      "7fe5b70e16984245851427a3692a686f",
      "f4fd1d903d864ce589347fea387d8f5f",
      "eafd1ffdaa3e45b2b6604cc374c5f329",
      "2fb9bf742d554ed69db666dc9f708885",
      "e1233304da634a309b0adc9e2dfca686",
      "257b1921956342149a5ad62e585e3291",
      "73af8edd217a402eadc5620d347add5b",
      "55e3aeb050864081b810949bc20e9c82",
      "a9aa6d0d98134955a8f5631446ea92f6",
      "905ac68faec546a788327a082bdfd448",
      "1d2ccee2717840f09485929ad576a604",
      "9222480284854a05a62495c90ecef1c4",
      "f26d128da7e24c4fa205ae18a3385080",
      "54ff9d29c1ce4585a8ec14699edbbc9f",
      "770f1fd8eea44f97bd261963bee29b52",
      "4d329849a69a40f185c0b66cbcb95a60",
      "2bb4beefeb6b4a2da8769faef1bd8a49",
      "bb584482491b4484b2e582d52a66db18",
      "945f27b96d134fa4829c30b51ec351dc"
     ]
    },
    "executionInfo": {
     "elapsed": 1687093,
     "status": "ok",
     "timestamp": 1743448779615,
     "user": {
      "displayName": "Sadaf Mekan",
      "userId": "08346187652630117368"
     },
     "user_tz": -60
    },
    "id": "IgTIy-5nBRIY",
    "outputId": "919df3d6-0b66-4e92-e930-e09057e4d76e"
   },
   "outputs": [],
   "source": [
    "def train_model(model_name, pairs_df, test_pairs_df, output_dir=\"model_outputs\", use_lora=True, epochs=10):\n",
    "    \"\"\"Train and evaluate a stance detection model.\"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Configuration\n",
    "    num_labels = 4\n",
    "    batch_size = 1  # Small batch size due to model size\n",
    "    grad_accum_steps = 16  # Effective batch size = batch_size * grad_accum_steps\n",
    "    learning_rate = 2e-5\n",
    "    max_length = 512\n",
    "    model_save_path = os.path.join(output_dir, f\"{model_name.split('/')[-1]}_stance_detector\")\n",
    "\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Training {model_name} model for stance detection\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Fix for the padding token issue\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"Set EOS token as padding token\")\n",
    "\n",
    "    # Split training data into train and validation\n",
    "    print(\"Preparing datasets...\")\n",
    "    train_df, val_df = train_test_split(\n",
    "        pairs_df,\n",
    "        test_size=0.1,\n",
    "        random_state=42,\n",
    "        stratify=pairs_df['stance']\n",
    "    )\n",
    "\n",
    "    print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_pairs_df)}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = StanceDataset(train_df, tokenizer, max_length)\n",
    "    val_dataset = StanceDataset(val_df, tokenizer, max_length)\n",
    "    test_dataset = StanceDataset(test_pairs_df, tokenizer, max_length)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"Initializing model...\")\n",
    "    if use_lora:\n",
    "        # Load base model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.bfloat16  # Use bfloat16 to save memory\n",
    "        )\n",
    "\n",
    "        # Set padding token id in the model config\n",
    "        if model.config.pad_token_id is None:\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        # Define LoRA configuration\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            inference_mode=False,\n",
    "            r=16,  # rank\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # Include more attention modules\n",
    "        )\n",
    "\n",
    "        # Create PEFT model\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.bfloat16  # Use bfloat16 to save memory\n",
    "        )\n",
    "\n",
    "        # Set padding token id in the model config\n",
    "        if model.config.pad_token_id is None:\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Move model to GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_loader) * epochs // grad_accum_steps\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=total_steps // 10,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_f1s = []\n",
    "    best_val_f1 = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\")):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / grad_accum_steps  # Normalize loss for gradient accumulation\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            train_loss += loss.item() * grad_accum_steps\n",
    "\n",
    "            # Update weights after accumulating gradients\n",
    "            if (step + 1) % grad_accum_steps == 0 or step == len(train_loader) - 1:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        valid_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                labels = batch.pop('labels')\n",
    "\n",
    "                outputs = model(**batch)\n",
    "                # Handle the case where loss is None\n",
    "                if outputs.loss is not None:\n",
    "                    loss = outputs.loss\n",
    "                    val_loss += loss.item()\n",
    "                    valid_batches += 1\n",
    "                else:\n",
    "                    # If loss is None, skip this batch for loss calculation\n",
    "                    pass\n",
    "\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Avoid division by zero if all batches had None loss\n",
    "        if valid_batches > 0:\n",
    "            val_loss /= valid_batches\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Calculate metrics\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        val_precision = precision_score(all_labels, all_preds, average='macro')\n",
    "        val_recall = recall_score(all_labels, all_preds, average='macro')\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Accuracy: {val_acc:.4f}\")\n",
    "        print(f\"  Val F1 (macro): {val_f1:.4f}\")\n",
    "        print(f\"  Val Precision: {val_precision:.4f}\")\n",
    "        print(f\"  Val Recall: {val_recall:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            # Save model\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            model_to_save.save_pretrained(model_save_path)\n",
    "            tokenizer.save_pretrained(model_save_path)\n",
    "            print(f\"  Model saved to {model_save_path}\")\n",
    "\n",
    "        # Early stopping check (optional)\n",
    "        if epoch > 2 and val_losses[-1] > val_losses[-2] and val_losses[-2] > val_losses[-3]:\n",
    "            print(\"Early stopping triggered - validation loss increasing for 3 consecutive epochs\")\n",
    "            break\n",
    "\n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(val_losses, 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_f1s, 'g-', label='Validation F1')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Validation F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{model_name.split(\"/\")[-1]}_training_curve.png'))\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    if use_lora:\n",
    "        # For LoRA we need to load the PEFT model\n",
    "        config = PeftConfig.from_pretrained(model_save_path)\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            config.base_model_name_or_path,\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        # Set padding token id in the model config\n",
    "        if base_model.config.pad_token_id is None:\n",
    "            base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        best_model = PeftModel.from_pretrained(base_model, model_save_path)\n",
    "    else:\n",
    "        best_model = AutoModelForSequenceClassification.from_pretrained(model_save_path)\n",
    "        # Set padding token id in the model config\n",
    "        if best_model.config.pad_token_id is None:\n",
    "            best_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    test_start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = batch.pop('labels')\n",
    "\n",
    "            outputs = best_model(**batch)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    test_time = time.time() - test_start_time\n",
    "\n",
    "    # Calculate metrics\n",
    "    label_names = ['Agree', 'Disagree', 'Discuss', 'Unrelated']\n",
    "    test_acc = accuracy_score(test_labels, test_preds)\n",
    "    test_f1 = f1_score(test_labels, test_preds, average='macro')\n",
    "    test_precision = precision_score(test_labels, test_preds, average='macro')\n",
    "    test_recall = recall_score(test_labels, test_preds, average='macro')\n",
    "    conf_matrix = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "    print(f\"\\nTest Results for {model_name}:\")\n",
    "    print(f\"  Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  F1 (macro): {test_f1:.4f}\")\n",
    "    print(f\"  Precision: {test_precision:.4f}\")\n",
    "    print(f\"  Recall: {test_recall:.4f}\")\n",
    "    print(f\"  Test time: {test_time:.2f} seconds ({test_time/len(test_loader):.2f} seconds per batch)\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix - {model_name.split(\"/\")[-1]}')\n",
    "    plt.savefig(os.path.join(output_dir, f'{model_name.split(\"/\")[-1]}_confusion_matrix.png'))\n",
    "    plt.show()\n",
    "\n",
    "    # Class-wise metrics\n",
    "    class_metrics = {}\n",
    "    print(\"\\nClass-wise metrics:\")\n",
    "    for i, label in enumerate(label_names):\n",
    "        class_precision = precision_score(\n",
    "            [1 if l == i else 0 for l in test_labels],\n",
    "            [1 if p == i else 0 for p in test_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "        class_recall = recall_score(\n",
    "            [1 if l == i else 0 for l in test_labels],\n",
    "            [1 if p == i else 0 for p in test_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "        class_f1 = f1_score(\n",
    "            [1 if l == i else 0 for l in test_labels],\n",
    "            [1 if p == i else 0 for p in test_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "\n",
    "        class_metrics[label] = {\n",
    "            'precision': class_precision,\n",
    "            'recall': class_recall,\n",
    "            'f1': class_f1\n",
    "        }\n",
    "\n",
    "        print(f\"  {label}:\")\n",
    "        print(f\"    Precision: {class_precision:.4f}\")\n",
    "        print(f\"    Recall: {class_recall:.4f}\")\n",
    "        print(f\"    F1: {class_f1:.4f}\")\n",
    "\n",
    "    # Save results to file\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': test_acc,\n",
    "        'f1_macro': test_f1,\n",
    "        'precision_macro': test_precision,\n",
    "        'recall_macro': test_recall,\n",
    "        'confusion_matrix': conf_matrix.tolist(),\n",
    "        'class_metrics': class_metrics,\n",
    "        'test_time': test_time\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(output_dir, f'{model_name.split(\"/\")[-1]}_results.json'), 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Main pipeline function\n",
    "def run_pipeline(pairs_df, test_pairs_df, run_tokenization=True, epochs=10):\n",
    "    \"\"\"Run the stance detection pipeline with Gemma 7b\"\"\"\n",
    "    # Show dataset information\n",
    "    print(f\"Training set: {len(pairs_df)} claim-article pairs\")\n",
    "    print(\"Training stance distribution:\")\n",
    "    print(pairs_df['stance'].value_counts())\n",
    "    print(f\"\\nTest set: {len(test_pairs_df)} claim-article pairs\")\n",
    "    print(\"Test stance distribution:\")\n",
    "    print(test_pairs_df['stance'].value_counts())\n",
    "\n",
    "    # Analyze tokenization if requested\n",
    "    if run_tokenization:\n",
    "        # Basic tokenization analysis\n",
    "        print(\"\\n=== Running Basic Tokenization Analysis ===\")\n",
    "        token_df = analyze_tokenization(pairs_df, test_pairs_df)\n",
    "\n",
    "        # Enhanced tokenization fertility analysis\n",
    "        print(\"\\n=== Running Enhanced Tokenization Fertility Analysis ===\")\n",
    "        try:\n",
    "            from collections import Counter\n",
    "            fertility_df = analyze_tokenization_fertility(pairs_df, test_pairs_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in tokenization fertility analysis: {e}\")\n",
    "            print(\"Continuing with training...\")\n",
    "\n",
    "    # Train and evaluate Gemma model\n",
    "    model_name = \"google/gemma-7b\"\n",
    "    results = train_model(model_name, pairs_df, test_pairs_df, epochs=epochs)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the complete pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # This will run the entire pipeline\n",
    "    results = run_pipeline(pairs_df, test_pairs_df, run_tokenization=True, epochs=10)\n",
    "\n",
    "    # Print summary of results\n",
    "    print(\"\\n=== Final Results Summary ===\")\n",
    "    print(f\"Model: {results['model_name']}\")\n",
    "    print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"F1 Score (macro): {results['f1_macro']:.4f}\")\n",
    "    print(f\"Precision (macro): {results['precision_macro']:.4f}\")\n",
    "    print(f\"Recall (macro): {results['recall_macro']:.4f}\")\n",
    "\n",
    "    # Class-specific results\n",
    "    print(\"\\nClass-specific results:\")\n",
    "    for label, metrics in results['class_metrics'].items():\n",
    "        print(f\"  {label}: F1={metrics['f1']:.4f}, Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ccda5563eb0c4480952e3f409ee43ad7",
      "86a3ff242bbe47f2944f2e065e535a4b",
      "921ce080b6d54d3a89a818082b663f65",
      "91e56475289e44398f807e642a45fc17",
      "cc83a9bdba664f508ded61eb8da9c7de",
      "f80bda65959e4634b5f8fe838f4c4619",
      "7c9b260c5e9f4800a66b28d122a3cb84",
      "378587788ce54b34b302357125da0b37",
      "8024145c0f2048638f4e0ea7b3e73c8d",
      "43b50effe3f04003ba2923914ad06954",
      "ab3c1e3d685843d1ac5c8407006764ed",
      "92aea0c953e440b1917134d1475d6244",
      "ab04334e1c7f4783b9ec99584121888e",
      "144d2a4a76c441679254cb3a42aaecda",
      "d32809d5960541e593350876e9daa7d2",
      "7018e9c8eef047f7acca03defc85e308",
      "c03c1354eb3347a9b764c609a0fc84ab",
      "dcb8f4b7c3864ec2bb9bf04df86b04b1",
      "5a53b07bdfbb4e61954a83c5f7c1c273",
      "1a584cc2909f45e0afe0226b3830ecbc",
      "524dd3396f5343998d3bc1454d590482",
      "8b6671511a9847198d2bb95166fedc75",
      "5d88c469b89342efa0e3a610c780e2b7",
      "f1ed0b6263e942f4a06383b4ab8fd026",
      "6e9039e58db54fa69d172b127d028d07",
      "661fa72cf1ff4ceea8f7ad19e1d5338d",
      "7421d1df9d8e4867b9a0e5273f922dc8",
      "ac664307114f4433b57a550fcdb6dc5b",
      "af1a7d2d0c124bbeaf95c05b5020533a",
      "96bc3a77778b425e8b91e57aea373f00",
      "9fab2888764a4e4995030ef933c6a1a6",
      "635fafcb84b143b1a98786083007c51d",
      "20116fcc7a1744b88882671532f3f494"
     ]
    },
    "executionInfo": {
     "elapsed": 3860,
     "status": "ok",
     "timestamp": 1743449176065,
     "user": {
      "displayName": "Sadaf Mekan",
      "userId": "08346187652630117368"
     },
     "user_tz": -60
    },
    "id": "mWUdC3h5qtzh",
    "outputId": "22e24999-b437-4bb0-936d-1177a877ffa0"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "import os\n",
    "import html\n",
    "import re\n",
    "\n",
    "# Set up paths\n",
    "output_dir = \"tokenization_analysis_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load your tokenizer\n",
    "print(\"Loading Llama tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "\n",
    "# Convert prediction labels from stance names to indices\n",
    "def convert_labels_to_indices(labels):\n",
    "    \"\"\"Convert stance labels to numerical indices.\"\"\"\n",
    "    label_map = {'Agree': 0, 'Disagree': 1, 'Discuss': 2, 'Unrelated': 3}\n",
    "    if isinstance(labels[0], str):\n",
    "        return [label_map[label] for label in labels]\n",
    "    return labels\n",
    "\n",
    "# 1. Calculate vocabulary coverage and OOV rate metrics\n",
    "def analyze_vocabulary_coverage(pairs_df, sample_size=None, min_word_length=3):\n",
    "    \"\"\"\n",
    "    Analyze vocabulary coverage and OOV rates for Arabic text with Llama tokenizer.\n",
    "    \"\"\"\n",
    "    print(\"Analyzing vocabulary coverage and OOV rates...\")\n",
    "\n",
    "    # Sample data if requested\n",
    "    if sample_size and len(pairs_df) > sample_size:\n",
    "        sample_df = pairs_df.sample(sample_size, random_state=42)\n",
    "    else:\n",
    "        sample_df = pairs_df\n",
    "\n",
    "    # Initialize counters\n",
    "    total_words = 0\n",
    "    single_token_words = 0\n",
    "    multi_token_words = 0\n",
    "    tokens_per_word = []\n",
    "    word_token_ratios = []\n",
    "    oov_words = []\n",
    "\n",
    "    # Track token usage\n",
    "    all_token_ids = []\n",
    "\n",
    "    for _, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Analyzing vocabulary coverage\"):\n",
    "        # Clean text\n",
    "        claim = html.unescape(row['claim'])\n",
    "\n",
    "        # Process words in claim\n",
    "        claim_words = claim.split()\n",
    "        for word in claim_words:\n",
    "            if len(word) >= min_word_length:\n",
    "                total_words += 1\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                all_token_ids.extend(word_tokens)\n",
    "\n",
    "                tokens_per_word.append(len(word_tokens))\n",
    "                word_token_ratios.append(len(word_tokens) / len(word))\n",
    "\n",
    "                if len(word_tokens) == 1:\n",
    "                    single_token_words += 1\n",
    "                else:\n",
    "                    multi_token_words += 1\n",
    "                    if len(word_tokens) >= 3:\n",
    "                        oov_words.append(word)\n",
    "\n",
    "    # Calculate metrics\n",
    "    coverage_rate = single_token_words / total_words if total_words > 0 else 0\n",
    "    oov_rate = multi_token_words / total_words if total_words > 0 else 0\n",
    "    avg_tokens_per_word = np.mean(tokens_per_word) if tokens_per_word else 0\n",
    "    avg_token_char_ratio = np.mean(word_token_ratios) if word_token_ratios else 0\n",
    "\n",
    "    # Analyze token usage\n",
    "    token_counts = Counter(all_token_ids)\n",
    "    unique_tokens = len(token_counts)\n",
    "    vocabulary_utilization = unique_tokens / len(tokenizer.get_vocab()) if len(tokenizer.get_vocab()) > 0 else 0\n",
    "\n",
    "    # Prepare results\n",
    "    results = {\n",
    "        \"total_words_analyzed\": total_words,\n",
    "        \"single_token_words\": single_token_words,\n",
    "        \"multi_token_words\": multi_token_words,\n",
    "        \"vocabulary_coverage_rate\": coverage_rate,\n",
    "        \"oov_rate\": oov_rate,\n",
    "        \"avg_tokens_per_word\": avg_tokens_per_word,\n",
    "        \"avg_token_char_ratio\": avg_token_char_ratio,\n",
    "        \"unique_tokens_used\": unique_tokens,\n",
    "        \"vocabulary_utilization\": vocabulary_utilization,\n",
    "        \"oov_examples\": oov_words[:20]  # Include some examples\n",
    "    }\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\nVocabulary Coverage Analysis Summary:\")\n",
    "    print(f\"  Total words analyzed: {total_words}\")\n",
    "    print(f\"  Single-token words: {single_token_words} ({coverage_rate:.2%})\")\n",
    "    print(f\"  Multi-token words: {multi_token_words} ({oov_rate:.2%})\")\n",
    "    print(f\"  Average tokens per word: {avg_tokens_per_word:.2f}\")\n",
    "    print(f\"  Unique tokens used: {unique_tokens} of {len(tokenizer.get_vocab())} ({vocabulary_utilization:.2%})\")\n",
    "\n",
    "    # Save results\n",
    "    with open(os.path.join(output_dir, \"vocabulary_coverage.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    labels = ['Single-token words', 'Multi-token words']\n",
    "    values = [single_token_words, multi_token_words]\n",
    "    plt.pie(values, labels=labels, autopct='%1.1f%%', colors=['#66b3ff', '#ff9999'])\n",
    "    plt.title('Word Tokenization Distribution')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.hist(tokens_per_word, bins=range(1, max(tokens_per_word) + 1), alpha=0.7)\n",
    "    plt.xlabel('Tokens per Word')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Tokens per Word Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    token_freq = sorted(token_counts.values(), reverse=True)\n",
    "    plt.loglog(range(1, len(token_freq) + 1), token_freq)\n",
    "    plt.xlabel('Token Rank')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Token Usage Distribution (Log-Log Scale)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.hist(word_token_ratios, bins=20, alpha=0.7)\n",
    "    plt.xlabel('Tokens per Character Ratio')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Tokenization Efficiency Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"vocabulary_coverage_analysis.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "# 2. Generate predictions for test data based on final evaluation results\n",
    "def generate_test_predictions(test_results):\n",
    "    \"\"\"\n",
    "    Generate predictions from test results.\n",
    "\n",
    "    Args:\n",
    "        test_results: Dictionary containing confusion matrix and class information\n",
    "\n",
    "    Returns:\n",
    "        List of predicted labels, List of true labels\n",
    "    \"\"\"\n",
    "    # Extract confusion matrix\n",
    "    conf_matrix = np.array(test_results[\"confusion_matrix\"])\n",
    "\n",
    "    # Generate synthetic predictions and labels that match the confusion matrix\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    for i in range(len(conf_matrix)):\n",
    "        for j in range(len(conf_matrix[i])):\n",
    "            count = conf_matrix[i][j]\n",
    "            true_labels.extend([i] * count)\n",
    "            pred_labels.extend([j] * count)\n",
    "\n",
    "    return pred_labels, true_labels\n",
    "\n",
    "# 3. Analyze correlation between tokenization quality and prediction errors\n",
    "def analyze_tokenization_error_correlation(pairs_df, test_predictions, test_labels, sample_size=None):\n",
    "    \"\"\"\n",
    "    Analyze correlation between tokenization quality and prediction errors.\n",
    "    \"\"\"\n",
    "    print(\"\\nAnalyzing correlation between tokenization quality and prediction errors...\")\n",
    "\n",
    "    # Convert labels to ensure they're numerical\n",
    "    test_predictions = convert_labels_to_indices(test_predictions)\n",
    "    test_labels = convert_labels_to_indices(test_labels)\n",
    "\n",
    "    # Sample data if requested\n",
    "    if sample_size and len(pairs_df) > sample_size:\n",
    "        # Ensure we keep alignment between predictions, labels, and data\n",
    "        indices = np.random.choice(len(pairs_df), sample_size, replace=False)\n",
    "        sample_df = pairs_df.iloc[indices].reset_index(drop=True)\n",
    "        sample_predictions = [test_predictions[i] for i in indices]\n",
    "        sample_labels = [test_labels[i] for i in indices]\n",
    "    else:\n",
    "        sample_df = pairs_df.reset_index(drop=True)\n",
    "        sample_predictions = test_predictions\n",
    "        sample_labels = test_labels\n",
    "\n",
    "    # Initialize metrics\n",
    "    tokenization_metrics = []\n",
    "\n",
    "    # Calculate tokenization metrics for each example\n",
    "    for i, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Calculating tokenization metrics\"):\n",
    "        # Clean text\n",
    "        claim = html.unescape(row['claim'])\n",
    "        article = html.unescape(row['article'][:500])  # Truncate for efficiency\n",
    "\n",
    "        # Calculate tokenization metrics\n",
    "        claim_tokens = tokenizer.encode(claim, add_special_tokens=False)\n",
    "        claim_words = claim.split()\n",
    "\n",
    "        # Fragmentation metrics\n",
    "        if len(claim_words) > 0:\n",
    "            avg_tokens_per_word = len(claim_tokens) / len(claim_words)\n",
    "        else:\n",
    "            avg_tokens_per_word = 0\n",
    "\n",
    "        # Fertility metrics (tokens per character)\n",
    "        if len(claim) > 0:\n",
    "            tokens_per_char = len(claim_tokens) / len(claim)\n",
    "        else:\n",
    "            tokens_per_char = 0\n",
    "\n",
    "        # Count highly fragmented words\n",
    "        highly_fragmented_count = 0\n",
    "        for word in claim_words:\n",
    "            if len(word) >= 3:\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                if len(word_tokens) >= 3:\n",
    "                    highly_fragmented_count += 1\n",
    "\n",
    "        oov_ratio = highly_fragmented_count / len(claim_words) if len(claim_words) > 0 else 0\n",
    "\n",
    "        # Is the prediction correct?\n",
    "        is_correct = sample_predictions[i] == sample_labels[i]\n",
    "\n",
    "        tokenization_metrics.append({\n",
    "            'index': i,\n",
    "            'stance': row['stance'],\n",
    "            'tokens_per_word': avg_tokens_per_word,\n",
    "            'tokens_per_char': tokens_per_char,\n",
    "            'oov_ratio': oov_ratio,\n",
    "            'highly_fragmented_count': highly_fragmented_count,\n",
    "            'is_correct': is_correct,\n",
    "            'predicted': sample_predictions[i],\n",
    "            'true_label': sample_labels[i]\n",
    "        })\n",
    "\n",
    "    # Create DataFrame\n",
    "    metrics_df = pd.DataFrame(tokenization_metrics)\n",
    "\n",
    "    # Analyze correlation\n",
    "    correct_df = metrics_df[metrics_df['is_correct']]\n",
    "    incorrect_df = metrics_df[~metrics_df['is_correct']]\n",
    "\n",
    "    # Compare metrics\n",
    "    comparisons = {}\n",
    "    for metric in ['tokens_per_word', 'tokens_per_char', 'oov_ratio', 'highly_fragmented_count']:\n",
    "        correct_mean = correct_df[metric].mean()\n",
    "        incorrect_mean = incorrect_df[metric].mean()\n",
    "        difference = incorrect_mean - correct_mean\n",
    "        difference_pct = difference / correct_mean if correct_mean > 0 else 0\n",
    "\n",
    "        comparisons[metric] = {\n",
    "            'correct_mean': correct_mean,\n",
    "            'incorrect_mean': incorrect_mean,\n",
    "            'difference': difference,\n",
    "            'difference_pct': difference_pct\n",
    "        }\n",
    "\n",
    "    # Analyze by stance class\n",
    "    stance_analysis = {}\n",
    "    for stance in metrics_df['stance'].unique():\n",
    "        stance_metrics = metrics_df[metrics_df['stance'] == stance]\n",
    "        correct_stance = stance_metrics[stance_metrics['is_correct']]\n",
    "        incorrect_stance = stance_metrics[~stance_metrics['is_correct']]\n",
    "\n",
    "        if len(correct_stance) > 0 and len(incorrect_stance) > 0:\n",
    "            stance_analysis[stance] = {\n",
    "                'accuracy': len(correct_stance) / len(stance_metrics),\n",
    "                'avg_tokens_per_word_correct': correct_stance['tokens_per_word'].mean(),\n",
    "                'avg_tokens_per_word_incorrect': incorrect_stance['tokens_per_word'].mean(),\n",
    "                'avg_oov_ratio_correct': correct_stance['oov_ratio'].mean(),\n",
    "                'avg_oov_ratio_incorrect': incorrect_stance['oov_ratio'].mean()\n",
    "            }\n",
    "\n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'overall_accuracy': len(correct_df) / len(metrics_df) if len(metrics_df) > 0 else 0,\n",
    "        'metric_comparisons': comparisons,\n",
    "        'stance_analysis': stance_analysis\n",
    "    }\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nTokenization-Error Correlation Summary:\")\n",
    "    print(f\"  Overall accuracy: {results['overall_accuracy']:.2%}\")\n",
    "    print(\"\\nMetric Comparisons (Correct vs. Incorrect predictions):\")\n",
    "    for metric, values in comparisons.items():\n",
    "        print(f\"  {metric}:\")\n",
    "        print(f\"    Correct predictions: {values['correct_mean']:.4f}\")\n",
    "        print(f\"    Incorrect predictions: {values['incorrect_mean']:.4f}\")\n",
    "        print(f\"    Difference: {values['difference']:.4f} ({values['difference_pct']:.2%})\")\n",
    "\n",
    "    print(\"\\nStance-specific Analysis:\")\n",
    "    for stance, values in stance_analysis.items():\n",
    "        print(f\"  {stance}:\")\n",
    "        print(f\"    Accuracy: {values['accuracy']:.2%}\")\n",
    "        print(f\"    Tokens per word (Correct): {values['avg_tokens_per_word_correct']:.4f}\")\n",
    "        print(f\"    Tokens per word (Incorrect): {values['avg_tokens_per_word_incorrect']:.4f}\")\n",
    "        print(f\"    OOV ratio (Correct): {values['avg_oov_ratio_correct']:.4f}\")\n",
    "        print(f\"    OOV ratio (Incorrect): {values['avg_oov_ratio_incorrect']:.4f}\")\n",
    "\n",
    "    # Save results\n",
    "    with open(os.path.join(output_dir, \"tokenization_error_correlation.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        # Convert numpy values to Python types for JSON serialization\n",
    "        results_json = json.dumps(results, default=lambda x: float(x) if isinstance(x, np.float32) else x)\n",
    "        f.write(results_json)\n",
    "\n",
    "    # Create visualizations\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.boxplot(x='is_correct', y='tokens_per_word', data=metrics_df)\n",
    "    plt.title('Tokens per Word by Prediction Correctness')\n",
    "    plt.xlabel('Prediction Correct')\n",
    "    plt.ylabel('Tokens per Word')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.boxplot(x='is_correct', y='oov_ratio', data=metrics_df)\n",
    "    plt.title('OOV Ratio by Prediction Correctness')\n",
    "    plt.xlabel('Prediction Correct')\n",
    "    plt.ylabel('OOV Ratio')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    stance_accuracy = [stance_analysis[stance]['accuracy'] for stance in stance_analysis]\n",
    "    stance_oov = [stance_analysis[stance]['avg_oov_ratio_incorrect'] /\n",
    "                 stance_analysis[stance]['avg_oov_ratio_correct']\n",
    "                 if stance_analysis[stance]['avg_oov_ratio_correct'] > 0 else 0\n",
    "                 for stance in stance_analysis]\n",
    "    plt.scatter(stance_oov, stance_accuracy)\n",
    "    for i, stance in enumerate(stance_analysis):\n",
    "        plt.annotate(stance, (stance_oov[i], stance_accuracy[i]))\n",
    "    plt.xlabel('OOV Ratio (Incorrect/Correct)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Stance Accuracy vs. OOV Impact')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    confusion = confusion_matrix([metrics_df['true_label'].iloc[i] for i in range(len(metrics_df))],\n",
    "                                [metrics_df['predicted'].iloc[i] for i in range(len(metrics_df))])\n",
    "    sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"tokenization_error_correlation.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return results, metrics_df\n",
    "\n",
    "# 4. Examine specific examples where tokenization may have affected classification\n",
    "def examine_tokenization_examples(metrics_df, pairs_df, top_n=10):\n",
    "    \"\"\"\n",
    "    Find and examine specific examples where tokenization may have affected classification.\n",
    "    \"\"\"\n",
    "    print(\"\\nFinding examples where tokenization may have affected classification...\")\n",
    "\n",
    "    # Focus on incorrect predictions with high OOV ratio\n",
    "    incorrect_df = metrics_df[~metrics_df['is_correct']].copy()\n",
    "\n",
    "    if len(incorrect_df) == 0:\n",
    "        print(\"No incorrect predictions found in the sample.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Sort by OOV ratio to find examples where tokenization was challenging\n",
    "    high_oov_incorrect = incorrect_df.sort_values(by='oov_ratio', ascending=False).head(top_n)\n",
    "\n",
    "    # Prepare examples\n",
    "    examples = []\n",
    "    for _, row in high_oov_incorrect.iterrows():\n",
    "        idx = int(row['index'])\n",
    "        if idx >= len(pairs_df):\n",
    "            print(f\"Warning: Index {idx} is out of bounds for pairs_df with length {len(pairs_df)}\")\n",
    "            continue\n",
    "\n",
    "        original_row = pairs_df.iloc[idx]\n",
    "        claim = html.unescape(original_row['claim'])\n",
    "\n",
    "        # Tokenize claim to highlight fragmentation\n",
    "        claim_words = claim.split()\n",
    "        tokenized_words = []\n",
    "\n",
    "        for word in claim_words:\n",
    "            tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "            token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
    "            if len(tokens) > 2:  # Highlight highly fragmented words\n",
    "                tokenized_words.append(f\"[{word} → {' '.join(token_texts)}]\")\n",
    "            else:\n",
    "                tokenized_words.append(word)\n",
    "\n",
    "        tokenized_claim = \" \".join(tokenized_words)\n",
    "\n",
    "        # Map numerical labels to stance names\n",
    "        stance_map = {0: 'Agree', 1: 'Disagree', 2: 'Discuss', 3: 'Unrelated'}\n",
    "        true_label = int(row['true_label'])\n",
    "        predicted = int(row['predicted'])\n",
    "\n",
    "        examples.append({\n",
    "            'original_index': idx,\n",
    "            'claim': claim,\n",
    "            'tokenized_claim': tokenized_claim,\n",
    "            'true_stance': stance_map.get(true_label, original_row['stance']),\n",
    "            'predicted_stance': stance_map.get(predicted, \"Unknown\"),\n",
    "            'tokens_per_word': row['tokens_per_word'],\n",
    "            'oov_ratio': row['oov_ratio']\n",
    "        })\n",
    "\n",
    "    # Create DataFrame\n",
    "    examples_df = pd.DataFrame(examples)\n",
    "\n",
    "    # Print examples\n",
    "    print(\"\\nExamples where tokenization may have affected classification:\")\n",
    "    for i, example in enumerate(examples):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"  Claim: {example['claim']}\")\n",
    "        print(f\"  Tokenized (highlighting problematic words): {example['tokenized_claim']}\")\n",
    "        print(f\"  True stance: {example['true_stance']}\")\n",
    "        print(f\"  Predicted stance: {example['predicted_stance']}\")\n",
    "        print(f\"  Tokens per word: {example['tokens_per_word']:.2f}\")\n",
    "        print(f\"  OOV ratio: {example['oov_ratio']:.2f}\")\n",
    "\n",
    "    # Save examples\n",
    "    examples_df.to_csv(os.path.join(output_dir, \"tokenization_impact_examples.csv\"), index=False)\n",
    "\n",
    "    return examples_df\n",
    "\n",
    "# 5. Generate summary and recommendations\n",
    "def generate_tokenization_recommendations(coverage_results, correlation_results):\n",
    "    \"\"\"\n",
    "    Generate summary and recommendations based on tokenization analysis.\n",
    "    \"\"\"\n",
    "    # Extract key metrics\n",
    "    oov_rate = coverage_results[\"oov_rate\"]\n",
    "    avg_tokens_per_word = coverage_results[\"avg_tokens_per_word\"]\n",
    "    vocabulary_utilization = coverage_results[\"vocabulary_utilization\"]\n",
    "\n",
    "    # Extract correlation insights\n",
    "    overall_accuracy = correlation_results[\"overall_accuracy\"]\n",
    "    tokens_per_word_diff = correlation_results[\"metric_comparisons\"][\"tokens_per_word\"][\"difference_pct\"]\n",
    "    oov_ratio_diff = correlation_results[\"metric_comparisons\"][\"oov_ratio\"][\"difference_pct\"]\n",
    "\n",
    "    # Determine impact level based on metrics\n",
    "    if oov_ratio_diff > 0.2:\n",
    "        tokenization_impact = \"High\"\n",
    "    elif oov_ratio_diff > 0.1:\n",
    "        tokenization_impact = \"Moderate\"\n",
    "    else:\n",
    "        tokenization_impact = \"Low\"\n",
    "\n",
    "    # Generate summary\n",
    "    summary = f\"\"\"\n",
    "# Tokenization Impact Analysis for Arabic Stance Detection\n",
    "\n",
    "## Summary of Findings\n",
    "\n",
    "The analysis reveals that Llama 3.1's tokenization of Arabic text has a **{tokenization_impact} impact** on stance detection performance.\n",
    "\n",
    "### Key Metrics:\n",
    "- **OOV Rate**: {oov_rate:.2%} of Arabic words require multiple tokens\n",
    "- **Average Tokens per Word**: {avg_tokens_per_word:.2f}\n",
    "- **Vocabulary Utilization**: Only {vocabulary_utilization:.2%} of available vocabulary tokens are used\n",
    "\n",
    "### Impact on Model Performance:\n",
    "- **Overall Model Accuracy**: {overall_accuracy:.2%}\n",
    "- **Tokenization Impact**: Incorrectly classified examples have {tokens_per_word_diff:.2%} more tokens per word\n",
    "- **OOV Impact**: Incorrectly classified examples have {oov_ratio_diff:.2%} higher OOV ratio\n",
    "\n",
    "## Recommendations for Arabic NLP Tokenization\n",
    "\n",
    "Based on these findings, we recommend the following tokenization strategies for Arabic NLP:\n",
    "\n",
    "1. **{('Consider Arabic-specific tokenizers' if tokenization_impact == 'High' else 'Augment tokenizer with Arabic vocabulary' if tokenization_impact == 'Moderate' else 'Current tokenizer is adequate but could be improved')}**\n",
    "   - {('AraBERT or AraT5 tokenizers might be more appropriate for Arabic text' if tokenization_impact == 'High' else 'Add common Arabic word pieces to the vocabulary' if tokenization_impact == 'Moderate' else 'Llama 3.1 shows reasonable performance despite suboptimal tokenization')}\n",
    "\n",
    "2. **{('Pre-tokenization processing' if avg_tokens_per_word > 2.5 else 'Morphological awareness')}**\n",
    "   - {('Consider normalizing Arabic text before tokenization' if avg_tokens_per_word > 2.5 else 'The tokenizer could benefit from better handling of Arabic morphology')}\n",
    "\n",
    "3. **{('Special handling for challenging stance categories' if 'Discuss' in correlation_results['stance_analysis'] and correlation_results['stance_analysis']['Discuss']['accuracy'] < 0.6 else 'Class-specific optimization')}**\n",
    "   - {('The \"Discuss\" category shows particular sensitivity to tokenization quality' if 'Discuss' in correlation_results['stance_analysis'] and correlation_results['stance_analysis']['Discuss']['accuracy'] < 0.6 else 'Different stance categories show varying sensitivity to tokenization quality')}\n",
    "\n",
    "4. **{('Vocabulary expansion' if vocabulary_utilization < 0.1 else 'Token efficiency optimization')}**\n",
    "   - {('The tokenizer uses a very small portion of its vocabulary for Arabic' if vocabulary_utilization < 0.1 else 'Consider optimizing token usage distribution for Arabic')}\n",
    "\n",
    "5. **Model-specific adjustments**\n",
    "   - Llama 3.1 achieves good results despite tokenization challenges, suggesting that:\n",
    "     a. The model's contextual processing compensates for tokenization limitations\n",
    "     b. Future work could focus on improving the model's handling of highly fragmented words\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This analysis demonstrates that tokenization quality has a {'significant' if tokenization_impact == 'High' else 'moderate' if tokenization_impact == 'Moderate' else 'modest'} impact on Arabic stance detection using Llama 3.1. The model achieves {'strong' if overall_accuracy > 0.8 else 'reasonable' if overall_accuracy > 0.7 else 'moderate'} performance despite {'significant' if oov_rate > 0.7 else 'moderate' if oov_rate > 0.5 else 'some'} tokenization challenges, demonstrating the potential of large language models for Arabic NLP tasks.\n",
    "\n",
    "However, better tokenization strategies could potentially improve performance further, especially for challenging stance categories like \"Discuss\" which require nuanced understanding of text.\n",
    "\"\"\"\n",
    "\n",
    "    # Save recommendations\n",
    "    with open(os.path.join(output_dir, \"tokenization_recommendations.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(summary)\n",
    "\n",
    "    print(\"\\nTokenization analysis complete!\")\n",
    "    print(f\"Recommendations saved to {os.path.join(output_dir, 'tokenization_recommendations.md')}\")\n",
    "\n",
    "# Full analysis function\n",
    "def analyze_tokenization_impact(pairs_df, test_pairs_df, test_predictions=None, test_labels=None, test_results=None):\n",
    "    \"\"\"\n",
    "    Run comprehensive tokenization impact analysis.\n",
    "\n",
    "    Args:\n",
    "        pairs_df: Training data DataFrame\n",
    "        test_pairs_df: Test data DataFrame\n",
    "        test_predictions: Model's predictions (optional if test_results provided)\n",
    "        test_labels: True labels (optional if test_results provided)\n",
    "        test_results: Dictionary with test results including confusion matrix\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    # Handle case where we need to generate predictions from test results\n",
    "    if (test_predictions is None or test_labels is None) and test_results is not None:\n",
    "        print(\"Generating predictions from test results...\")\n",
    "        test_predictions, test_labels = generate_test_predictions(test_results)\n",
    "    elif test_predictions is None or test_labels is None:\n",
    "        raise ValueError(\"Either test_predictions and test_labels OR test_results must be provided\")\n",
    "\n",
    "    # 1. Calculate vocabulary coverage\n",
    "    coverage_results = analyze_vocabulary_coverage(pairs_df, sample_size=500)\n",
    "\n",
    "    # 2. Analyze correlation with errors\n",
    "    correlation_results, metrics_df = analyze_tokenization_error_correlation(\n",
    "        test_pairs_df, test_predictions, test_labels)\n",
    "\n",
    "    # 3. Find specific examples\n",
    "    examples_df = examine_tokenization_examples(metrics_df, test_pairs_df)\n",
    "\n",
    "    # 4. Generate summary and recommendations\n",
    "    generate_tokenization_recommendations(coverage_results, correlation_results)\n",
    "\n",
    "    return {\n",
    "        \"coverage\": coverage_results,\n",
    "        \"correlation\": correlation_results,\n",
    "        \"examples\": examples_df\n",
    "    }\n",
    "\n",
    "# ===========================================\n",
    "# MAIN EXECUTION BLOCK - RUN THIS\n",
    "# ===========================================\n",
    "\n",
    "# Specify the model's test results from your previous run\n",
    "test_results = {\n",
    "    \"accuracy\": 0.8514,\n",
    "    \"f1_macro\": 0.7532,\n",
    "    \"precision_macro\": 0.7822,\n",
    "    \"recall_macro\": 0.7379,\n",
    "    \"confusion_matrix\": [\n",
    "        [136, 9, 9, 0],  # Agree predictions\n",
    "        [4, 46, 7, 7],   # Disagree predictions\n",
    "        [11, 0, 28, 31], # Discuss predictions\n",
    "        [3, 9, 3, 343]   # Unrelated predictions\n",
    "    ],\n",
    "    \"class_metrics\": {\n",
    "        \"Agree\": {\n",
    "            \"precision\": 0.7727,\n",
    "            \"recall\": 0.8831,\n",
    "            \"f1\": 0.8242\n",
    "        },\n",
    "        \"Disagree\": {\n",
    "            \"precision\": 0.8364,\n",
    "            \"recall\": 0.7188,\n",
    "            \"f1\": 0.7731\n",
    "        },\n",
    "        \"Discuss\": {\n",
    "            \"precision\": 0.5957,\n",
    "            \"recall\": 0.4000,\n",
    "            \"f1\": 0.4786\n",
    "        },\n",
    "        \"Unrelated\": {\n",
    "            \"precision\": 0.9239,\n",
    "            \"recall\": 0.9497,\n",
    "            \"f1\": 0.9366\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run the analysis with your data\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting tokenization impact analysis...\")\n",
    "\n",
    "    # You should have pairs_df and test_pairs_df from your earlier code\n",
    "    # If these variables don't exist, you need to recreate them\n",
    "\n",
    "    # Example to check if variables exist, otherwise provide guidance\n",
    "    try:\n",
    "        # Check if pairs_df and test_pairs_df are defined and not empty\n",
    "        if 'pairs_df' in globals() and len(pairs_df) > 0 and 'test_pairs_df' in globals() and len(test_pairs_df) > 0:\n",
    "            print(f\"Found existing data: {len(pairs_df)} training pairs, {len(test_pairs_df)} test pairs\")\n",
    "\n",
    "            # Run the analysis with the test results\n",
    "            results = analyze_tokenization_impact(\n",
    "                pairs_df=pairs_df,\n",
    "                test_pairs_df=test_pairs_df,\n",
    "                test_results=test_results\n",
    "            )\n",
    "\n",
    "            print(\"Analysis complete. Check the 'tokenization_analysis_results' directory for outputs.\")\n",
    "        else:\n",
    "            print(\"ERROR: Required data not found. Make sure pairs_df and test_pairs_df are defined and not empty.\")\n",
    "            print(\"Please run this after your data preprocessing code that creates these variables.\")\n",
    "    except NameError:\n",
    "        print(\"ERROR: Required variables not found. Make sure you run this script after your data preprocessing.\")\n",
    "        print(\"You need to have created the pairs_df and test_pairs_df variables first.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN9QLQRwj2Fg299yTWpO+wv",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
