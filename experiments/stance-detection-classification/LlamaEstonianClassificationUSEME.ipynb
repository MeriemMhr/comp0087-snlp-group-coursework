{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAJxratByskA"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NXnzsHt0ywZ-",
    "outputId": "e3284f20-a21f-427c-9485-fbadd5e9e42a"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade huggingface_hub\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "!pip install peft --upgrade\n",
    "!pip install triton\n",
    "!pip install bitsandbytes\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install tqdm\n",
    "!pip install tiktoken\n",
    "\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/wheels/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmc_74Z9yuWq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, roc_curve, auc\n",
    ")\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    GemmaForSequenceClassification,\n",
    "    LlamaForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorWithPadding,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "import html\n",
    "import random\n",
    "import time\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9WCYLo914AM"
   },
   "source": [
    "### Preprocessing / EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CiN55Oma0Q-S",
    "outputId": "5b486fc2-fcf4-4787-9ad4-5e6dbaa53dc2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3ahqPze0azt"
   },
   "outputs": [],
   "source": [
    "# File path\n",
    "data_path = \"/content/drive/MyDrive/SNLP Group Project/Datasets/EstonianStanceDetection/notebooks/data.csv\"\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mXPWqVHx0kHV",
    "outputId": "eee57064-a0e4-4bee-b18e-9eff628fee86"
   },
   "outputs": [],
   "source": [
    "# Visualise dataset\n",
    "print(\"Dataset Columns:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "glSsQjW2yoYr",
    "outputId": "9dff5d93-423a-4b0e-c783-225755105051"
   },
   "outputs": [],
   "source": [
    "# Train/test split (80/20) while keeping stance distribution balanced\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['stance'])\n",
    "\n",
    "print(f\"Training data shape: {df_train.shape}\")\n",
    "print(f\"Test data shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VBf_ZUQ61kJx",
    "outputId": "40ca2308-5d43-43af-f0f8-05ad78a2e552"
   },
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "ka7kIBRl2H4S",
    "outputId": "814d0e44-b2b5-483f-d006-9f7582da45ad"
   },
   "outputs": [],
   "source": [
    "df_train['stance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bb9t-lBV54Tm",
    "outputId": "935faced-a40e-4125-c116-19ed64af720f"
   },
   "outputs": [],
   "source": [
    "# Print five sample rows for all columns\n",
    "print(\"Five sample rows of the entire DataFrame:\")\n",
    "print(df_train.sample(5))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-_PjZDWh6On2",
    "outputId": "e38c4bff-fcc3-4c1b-f321-d3431a489540"
   },
   "outputs": [],
   "source": [
    "# Count total NaNs in each DataFrame before filling\n",
    "missing_train = df_train.isna().sum().sum()\n",
    "missing_test = df_test.isna().sum().sum()\n",
    "\n",
    "# Fill all NaN values with \"Unknown\"\n",
    "df_train.fillna(\"Unknown\", inplace=True)\n",
    "df_test.fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# Print how many NaNs were replaced and the new shape of each DataFrame\n",
    "print(f\"Replaced {missing_train} NaN values in df_train with 'Unknown'. | Shape: {df_train.shape}\")\n",
    "print(f\"Replaced {missing_test} NaN values in df_test with 'Unknown' | Shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "DKM1tp8h1OOq",
    "outputId": "aba42217-30a6-4974-e5b8-ad509628fc1c"
   },
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(df_train)\n",
    "df_test = pd.DataFrame(df_test)\n",
    "\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmxtGM8L_9t0"
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5pJzM0AB4KB",
    "outputId": "efff1cd5-a2c3-4aef-d415-b8b31ab76659"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login\n",
    "\n",
    "\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "16d4be5b3e3c4d0e8028e4e55e61e99c",
      "bba3cd081ccb40e4a84cc65a51df3cce",
      "c22b420347d149e1ac7406aaaa735ca4",
      "bf2275fbc57641c79fe4fdf79f45e01a",
      "2800f683ab764501b7021bbf9150cb62",
      "a1671767cf1f4e1aaf3906a9a39db8d9",
      "a387024de621495d8a9117f413b1b38d",
      "be1bcbd88c1d48d39e70f77acc5452c2",
      "11c315a1ad594ba2b79e104d1c921164",
      "ad9f5fc006dc40f68069cb1df9193b4f",
      "7d1c300d1caf49af8e8928a2171b1367",
      "3b4ad3923fbe497ba033ac319ccc49d0",
      "8fc90020f7654a21ac8fbd85a6336cf9",
      "c60080e79c2a4e35a34c3e1b460026d7",
      "c84382a63d3a43cdbc11b3db239b3aa7",
      "9319787b9c2d4e66b6f2da8684c87f74",
      "ce6d7c4242d74a5798bef228f3f035a5",
      "aa181498a4244d0c90d2786f9752caf5",
      "aee11f3ceaf5487aa38b7353b2e021a6",
      "54384800074547ff9a7128c1ea045141",
      "4af4b66d2d734e5caab6268a14e8625c",
      "69ffc4123c6f43578bfb543850f20d1b"
     ]
    },
    "id": "Q5csRzriZq6p",
    "outputId": "735a1299-3049-424b-ad4f-4b6afe9763db"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import html\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, roc_curve, auc\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "\n",
    "# for using LoRA modifications\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    PeftModel,\n",
    "    PeftConfig\n",
    ")\n",
    "\n",
    "############################################\n",
    "# 1. Custom Dataset for Estonian Stance Data\n",
    "############################################\n",
    "class StanceDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        # Map stance labels to integers.\n",
    "        # Adjust the mapping if your Estonian dataset uses different labels.\n",
    "        self.label_map = {'neutral': 0, 'against': 1, 'supportive': 2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        # For the Estonian dataset, we use the \"sentence\" field.\n",
    "        text = html.unescape(row['sentence'])\n",
    "\n",
    "        # Truncate if the text is very long (adjust as needed)\n",
    "        if len(text) > 5000:\n",
    "            text = text[:5000]\n",
    "\n",
    "        # Tokenize the sentence\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Remove batch dimension added by the tokenizer\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "\n",
    "        # Add label (using \"stance\" from the dataset)\n",
    "        encoding['labels'] = torch.tensor(self.label_map[row['stance']])\n",
    "        return encoding\n",
    "\n",
    "\n",
    "############################################\n",
    "# 2. Enhanced Tokenization Analysis Functions\n",
    "############################################\n",
    "def analyze_tokenization(df_train, df_test, sample_size=100):\n",
    "    \"\"\"\n",
    "    Analyze tokenization for Llama with 5 specific metrics on the Estonian dataset:\n",
    "    - Token Count (Token Fertility)\n",
    "    - Token Length Distribution\n",
    "    - Compression Ratio\n",
    "    - Vocabulary Size\n",
    "    - Out-of-Vocabulary (OOV) Rate\n",
    "    \"\"\"\n",
    "    print(\"Loading tokenizer for tokenization analysis...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\", use_fast=True)\n",
    "\n",
    "    # Sample data for analysis from both training and test sets\n",
    "    combined_df = pd.concat([df_train, df_test])\n",
    "    sample_df = combined_df.sample(sample_size, random_state=42)\n",
    "\n",
    "    print(f\"Analyzing tokenization on {sample_size} samples...\")\n",
    "\n",
    "    results = []\n",
    "    total_chars = 0\n",
    "    total_tokens = 0\n",
    "    token_lengths = []\n",
    "    tokens_per_char_values = []\n",
    "    oov_count = 0\n",
    "    unique_tokens = set()\n",
    "    vocabulary_size = len(tokenizer.get_vocab())\n",
    "    highly_fragmented_words = []\n",
    "    words_analyzed = 0\n",
    "\n",
    "    for _, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Tokenization Analysis\"):\n",
    "        text = html.unescape(row['sentence'])\n",
    "        if len(text) > 5000:\n",
    "            text = text[:5000]\n",
    "\n",
    "        text_chars = len(text)\n",
    "        text_tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        text_tokens_texts = tokenizer.convert_ids_to_tokens(text_tokens)\n",
    "\n",
    "        # Token Fertility: tokens per character\n",
    "        fertility = len(text_tokens) / text_chars if text_chars > 0 else 0\n",
    "        tokens_per_char_values.append(fertility)\n",
    "\n",
    "        # Token length distribution and vocabulary\n",
    "        for token in text_tokens_texts:\n",
    "            token_lengths.append(len(token))\n",
    "            unique_tokens.add(token)\n",
    "\n",
    "        total_chars += text_chars\n",
    "        total_tokens += len(text_tokens)\n",
    "\n",
    "        # Calculate OOV estimation (words split into 3+ tokens)\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            if len(word) >= 3:  # Only check non-trivial words\n",
    "                words_analyzed += 1\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                if len(word_tokens) >= 3:  # Consider this a fragmented (potential OOV) word\n",
    "                    highly_fragmented_words.append(word)\n",
    "                    oov_count += 1\n",
    "\n",
    "        # Store metrics for this sample\n",
    "        results.append({\n",
    "            'text_length': text_chars,\n",
    "            'llama_tokens': len(text_tokens),\n",
    "            'fertility': fertility,\n",
    "            'stance': row['stance']\n",
    "        })\n",
    "\n",
    "    token_df = pd.DataFrame(results)\n",
    "\n",
    "    # Calculate metrics\n",
    "    avg_token_fertility = np.mean(tokens_per_char_values)\n",
    "    token_length_distribution = Counter(token_lengths)\n",
    "    avg_token_length = np.mean(token_lengths)\n",
    "    median_token_length = np.median(token_lengths)\n",
    "    compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0\n",
    "    vocabulary_coverage = len(unique_tokens) / vocabulary_size\n",
    "    oov_rate = oov_count / words_analyzed if words_analyzed > 0 else 0\n",
    "\n",
    "    # Display statistics\n",
    "    print(\"\\n===== Tokenization Analysis Results =====\")\n",
    "    print(f\"1. Token Fertility (tokens/char): {avg_token_fertility:.4f}\")\n",
    "    print(f\"2. Token Length Distribution: Mean={avg_token_length:.2f}, Median={median_token_length:.2f}\")\n",
    "    print(f\"3. Compression Ratio (chars/token): {compression_ratio:.4f}\")\n",
    "    print(f\"4. Vocabulary: Used {len(unique_tokens)} of {vocabulary_size} tokens ({vocabulary_coverage:.2%})\")\n",
    "    print(f\"5. OOV Rate: {oov_rate:.4f} ({oov_count}/{words_analyzed} words)\")\n",
    "\n",
    "    # Print some example highly fragmented words (potential OOVs)\n",
    "    if highly_fragmented_words:\n",
    "        print(\"\\nExample highly fragmented words (potential OOVs):\")\n",
    "        sample_oov = random.sample(highly_fragmented_words, min(10, len(highly_fragmented_words)))\n",
    "        for word in sample_oov:\n",
    "            tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "            token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
    "            print(f\"  '{word}' → {len(tokens)} tokens: {token_texts}\")\n",
    "\n",
    "    # Visualizations\n",
    "    plt.figure(figsize=(16, 12))\n",
    "\n",
    "    # Plot 1: Token Count vs Text Length\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(token_df['text_length'], token_df['llama_tokens'], alpha=0.7)\n",
    "    plt.xlabel('Text Length (characters)')\n",
    "    plt.ylabel('Llama 3.1 Token Count')\n",
    "    plt.title('Tokenization Analysis: Llama 3.1')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Token Fertility by Stance\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.boxplot(x='stance', y='fertility', data=token_df)\n",
    "    plt.title('Token Fertility by Stance')\n",
    "    plt.ylabel('Tokens per Character')\n",
    "\n",
    "    # Plot 3: Token Length Distribution\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.bar(list(token_length_distribution.keys()), list(token_length_distribution.values()))\n",
    "    plt.xlabel('Token Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Token Length Distribution')\n",
    "\n",
    "    # Plot 4: Compression Ratio Distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    compression_values = [1/f if f > 0 else 0 for f in token_df['fertility']]\n",
    "    plt.hist(compression_values, bins=20)\n",
    "    plt.xlabel('Compression Ratio (chars/token)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Compression Ratio Distribution')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tokenization_analysis.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Save results to file\n",
    "    tokenization_metrics = {\n",
    "        'token_fertility': avg_token_fertility,\n",
    "        'token_length_mean': avg_token_length,\n",
    "        'token_length_median': median_token_length,\n",
    "        'compression_ratio': compression_ratio,\n",
    "        'vocabulary_size': vocabulary_size,\n",
    "        'vocabulary_coverage': vocabulary_coverage,\n",
    "        'oov_rate': oov_rate,\n",
    "        'token_length_distribution': {str(k): v for k, v in token_length_distribution.items()}\n",
    "    }\n",
    "\n",
    "    with open('tokenization_metrics.json', 'w') as f:\n",
    "        json.dump(tokenization_metrics, f, indent=2)\n",
    "\n",
    "    return token_df\n",
    "\n",
    "\n",
    "def analyze_tokenization_fertility(df_train, df_test, sample_size=100):\n",
    "    \"\"\"\n",
    "    Analyze tokenization with five metrics for Estonian text using Llama:\n",
    "    - Token Count (Token Fertility)\n",
    "    - Token Length Distribution\n",
    "    - Compression Ratio\n",
    "    - Vocabulary Size\n",
    "    - Out-of-Vocabulary (OOV) Rate\n",
    "    \"\"\"\n",
    "    print(\"Loading tokenizer for detailed tokenization analysis...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\", use_fast=True)\n",
    "\n",
    "    combined_df = pd.concat([df_train, df_test])\n",
    "    sample_df = combined_df.sample(sample_size, random_state=42)\n",
    "\n",
    "    results = []\n",
    "    vocabulary_size = len(tokenizer.get_vocab())\n",
    "    unique_tokens_used = set()\n",
    "    token_lengths = []\n",
    "    total_chars = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for _, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Analyzing token metrics\"):\n",
    "        text = html.unescape(row['sentence'])\n",
    "        if len(text) > 5000:\n",
    "            text = text[:500]\n",
    "        text_tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        token_texts = tokenizer.convert_ids_to_tokens(text_tokens)\n",
    "        unique_tokens_used.update(text_tokens)\n",
    "\n",
    "        for token in token_texts:\n",
    "            token_lengths.append(len(token))\n",
    "\n",
    "        total_chars += len(text)\n",
    "        total_tokens += len(text_tokens)\n",
    "        fertility = len(text_tokens) / len(text) if len(text) > 0 else 0\n",
    "\n",
    "        words = text.split()\n",
    "        fragmentation = len(text_tokens) / len(words) if words else 0\n",
    "        highly_fragmented_words = []\n",
    "        for word in words:\n",
    "            if len(word) >= 3:\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                if len(word_tokens) >= 3:\n",
    "                    highly_fragmented_words.append(word)\n",
    "\n",
    "        results.append({\n",
    "            'text_tokens': len(text_tokens),\n",
    "            'fertility': fertility,\n",
    "            'fragmentation': fragmentation,\n",
    "            'highly_fragmented_words': len(highly_fragmented_words),\n",
    "            'highly_fragmented_word_examples': highly_fragmented_words[:5] if highly_fragmented_words else [],\n",
    "            'stance': row['stance']\n",
    "        })\n",
    "\n",
    "    token_df = pd.DataFrame(results)\n",
    "    avg_fertility = token_df['fertility'].mean()\n",
    "    token_length_dist = Counter(token_lengths)\n",
    "    avg_token_length = np.mean(token_lengths)\n",
    "    compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0\n",
    "    vocabulary_coverage = len(unique_tokens_used) / vocabulary_size\n",
    "    avg_oov_count = token_df['highly_fragmented_words'].mean()\n",
    "    total_words = sum(len(html.unescape(row['sentence']).split()) for _, row in sample_df.iterrows())\n",
    "    total_highly_fragmented = token_df['highly_fragmented_words'].sum()\n",
    "    oov_rate = total_highly_fragmented / total_words if total_words > 0 else 0\n",
    "\n",
    "    print(\"\\n=== Tokenization Metrics Analysis ===\")\n",
    "    print(f\"1. Token Fertility: Overall = {avg_fertility:.4f} tokens/char\")\n",
    "    print(f\"\\n2. Token Length Distribution: Mean = {avg_token_length:.2f} chars, Median = {np.median(token_lengths):.2f} chars\")\n",
    "    print(f\"\\n3. Compression Ratio: {compression_ratio:.4f} chars/token\")\n",
    "    print(f\"\\n4. Vocabulary: Used {len(unique_tokens_used)} of {vocabulary_size} tokens ({vocabulary_coverage:.2%} coverage)\")\n",
    "    print(f\"\\n5. OOV Rate: {oov_rate:.2%} ({total_highly_fragmented}/{total_words} words)\")\n",
    "\n",
    "    if token_df['highly_fragmented_word_examples'].apply(len).sum() > 0:\n",
    "        print(\"\\nExample highly fragmented words (potential OOVs):\")\n",
    "        all_examples = [w for sublist in token_df['highly_fragmented_word_examples'] for w in sublist]\n",
    "        for word in random.sample(all_examples, min(10, len(all_examples))):\n",
    "            tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "            token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
    "            print(f\"  '{word}' → {len(tokens)} tokens: {token_texts}\")\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.boxplot(x='stance', y='fertility', data=token_df)\n",
    "    plt.title('Token Fertility by Stance')\n",
    "    plt.ylabel('Tokens per Character')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.boxplot(x='stance', y='highly_fragmented_words', data=token_df)\n",
    "    plt.title('OOV Words by Stance')\n",
    "    plt.ylabel('Count of Highly Fragmented Words')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.bar(list(token_length_dist.keys()), list(token_length_dist.values()))\n",
    "    plt.xlabel('Token Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Token Length Distribution')\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.hist([1/f if f > 0 else 0 for f in token_df['fertility']], bins=20)\n",
    "    plt.xlabel('Compression Ratio (chars/token)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Compression Ratio Distribution')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tokenization_metrics_analysis.png')\n",
    "    plt.show()\n",
    "\n",
    "    metrics_results = {\n",
    "        'token_fertility': avg_fertility,\n",
    "        'token_length_distribution': {\n",
    "            'mean': avg_token_length,\n",
    "            'median': float(np.median(token_lengths)),\n",
    "            'distribution': {str(k): v for k, v in token_length_dist.items()}\n",
    "        },\n",
    "        'compression_ratio': compression_ratio,\n",
    "        'vocabulary_size': {\n",
    "            'total': vocabulary_size,\n",
    "            'used': len(unique_tokens_used),\n",
    "            'coverage': vocabulary_coverage\n",
    "        },\n",
    "        'oov_rate': {\n",
    "            'avg_highly_fragmented_per_sample': avg_oov_count,\n",
    "            'overall_rate': oov_rate\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open('tokenization_metrics_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics_results, f, indent=2)\n",
    "\n",
    "    return token_df\n",
    "\n",
    "\n",
    "############################################\n",
    "# 3. Comprehensive Model Evaluation Function\n",
    "############################################\n",
    "def evaluate_model_comprehensive(best_model, test_loader, device, label_names=['Agree', 'Disagree', 'Discuss', 'Unrelated']):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with comprehensive metrics:\n",
    "    - AUC, ROC curves\n",
    "    - Micro F1, Macro F1\n",
    "    - Precision, Recall (micro and macro)\n",
    "    - Accuracy\n",
    "    - Loss\n",
    "    \"\"\"\n",
    "    best_model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    test_start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating model\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = batch.pop('labels')\n",
    "            outputs = best_model(**batch)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().float().numpy())\n",
    "\n",
    "    test_time = time.time() - test_start_time\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.vstack(all_probs)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision_micro = precision_score(all_labels, all_preds, average='micro')\n",
    "    precision_macro = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall_micro = recall_score(all_labels, all_preds, average='micro')\n",
    "    recall_macro = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    try:\n",
    "        num_classes = len(np.unique(all_labels))\n",
    "        labels_one_hot = np.eye(num_classes)[all_labels]\n",
    "        roc_auc = roc_auc_score(labels_one_hot, all_probs, multi_class='ovr')\n",
    "\n",
    "        fpr = {}\n",
    "        tpr = {}\n",
    "        roc_auc_per_class = {}\n",
    "        for i in range(num_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(labels_one_hot[:, i], all_probs[:, i])\n",
    "            roc_auc_per_class[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for i in range(num_classes):\n",
    "            plt.plot(\n",
    "                fpr[i],\n",
    "                tpr[i],\n",
    "                lw=2,\n",
    "                label=f'{label_names[i]} (AUC = {roc_auc_per_class[i]:.2f})'\n",
    "            )\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curves (One-vs-Rest, Overall AUC = {roc_auc:.2f})')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig('roc_curves.png')\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not calculate ROC AUC due to {str(e)}\")\n",
    "        roc_auc = None\n",
    "        roc_auc_per_class = None\n",
    "\n",
    "    class_metrics = {}\n",
    "    for i, label in enumerate(label_names):\n",
    "        class_precision = precision_score(\n",
    "            [1 if l == i else 0 for l in all_labels],\n",
    "            [1 if p == i else 0 for p in all_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "        class_recall = recall_score(\n",
    "            [1 if l == i else 0 for l in all_labels],\n",
    "            [1 if p == i else 0 for p in all_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "        class_f1 = f1_score(\n",
    "            [1 if l == i else 0 for l in all_labels],\n",
    "            [1 if p == i else 0 for p in all_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "        class_metrics[label] = {\n",
    "            'precision': class_precision,\n",
    "            'recall': class_recall,\n",
    "            'f1': class_f1\n",
    "        }\n",
    "\n",
    "    print(f\"\\nComprehensive Model Evaluation Results:\")\n",
    "    print(f\"  Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: Micro={precision_micro:.4f}, Macro={precision_macro:.4f}\")\n",
    "    print(f\"  Recall: Micro={recall_micro:.4f}, Macro={recall_macro:.4f}\")\n",
    "    print(f\"  F1 Score: Micro={f1_micro:.4f}, Macro={f1_macro:.4f}\")\n",
    "    if roc_auc is not None:\n",
    "        print(f\"  ROC AUC (OVR): {roc_auc:.4f}\")\n",
    "    print(f\"  Evaluation time: {test_time:.2f} seconds\")\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "\n",
    "    results = {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_micro': precision_micro,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_micro': recall_micro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'roc_auc': roc_auc,\n",
    "        'roc_auc_per_class': {label_names[i]: roc_auc_per_class[i] for i in roc_auc_per_class} if roc_auc_per_class else None,\n",
    "        'confusion_matrix': conf_matrix.tolist(),\n",
    "        'class_metrics': class_metrics,\n",
    "        'test_time': test_time\n",
    "    }\n",
    "\n",
    "    return results, all_preds, all_labels\n",
    "\n",
    "\n",
    "############################################\n",
    "# 4. Training Function\n",
    "############################################\n",
    "def train_model(model_name, df_train, df_test, output_dir=\"model_outputs\", use_lora=True, epochs=10):\n",
    "    \"\"\"Train and evaluate a stance detection model.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # Set the number of labels to 3 for 'neutral', 'against', 'supportive'\n",
    "    num_labels = 3\n",
    "    batch_size = 1  # Adjust batch size as needed\n",
    "    grad_accum_steps = 16  # Effective batch size = batch_size * grad_accum_steps\n",
    "    learning_rate = 2e-5\n",
    "    max_length = 512\n",
    "    model_save_path = os.path.join(output_dir, f\"{model_name.split('/')[-1]}_stance_detector\")\n",
    "\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Training {model_name} model for Estonian stance detection\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"Set EOS token as padding token\")\n",
    "\n",
    "    print(\"Preparing datasets...\")\n",
    "    train_df, val_df = train_test_split(\n",
    "        df_train,\n",
    "        test_size=0.1,\n",
    "        random_state=42,\n",
    "        stratify=df_train['stance']\n",
    "    )\n",
    "    print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(df_test)}\")\n",
    "\n",
    "    # Create datasets using the updated StanceDataset\n",
    "    train_dataset = StanceDataset(train_df, tokenizer, max_length)\n",
    "    val_dataset = StanceDataset(val_df, tokenizer, max_length)\n",
    "    test_dataset = StanceDataset(df_test, tokenizer, max_length)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Model initialization\n",
    "    print(\"Initializing model...\")\n",
    "    if use_lora:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        if model.config.pad_token_id is None:\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            inference_mode=False,\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "        )\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        if model.config.pad_token_id is None:\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_loader) * epochs // grad_accum_steps\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=total_steps // 10,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_f1s = []\n",
    "    best_val_f1 = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\")):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / grad_accum_steps\n",
    "            loss.backward()\n",
    "            train_loss += loss.item() * grad_accum_steps\n",
    "\n",
    "            if (step + 1) % grad_accum_steps == 0 or step == len(train_loader) - 1:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        valid_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                labels = batch.pop('labels')\n",
    "                outputs = model(**batch)\n",
    "                if outputs.loss is not None:\n",
    "                    loss = outputs.loss\n",
    "                    val_loss += loss.item()\n",
    "                    valid_batches += 1\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        if valid_batches > 0:\n",
    "            val_loss /= valid_batches\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Accuracy: {val_acc:.4f}\")\n",
    "        print(f\"  Val F1 (macro): {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            model_to_save.save_pretrained(model_save_path)\n",
    "            tokenizer.save_pretrained(model_save_path)\n",
    "            print(f\"  Model saved to {model_save_path}\")\n",
    "\n",
    "        if epoch > 2 and val_losses[-1] > val_losses[-2] and val_losses[-2] > val_losses[-3]:\n",
    "            print(\"Early stopping triggered - validation loss increasing for 3 consecutive epochs\")\n",
    "            break\n",
    "\n",
    "    # Plot training curves (unchanged)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(val_losses, 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_f1s, 'g-', label='Validation F1')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Validation F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{model_name.split(\"/\")[-1]}_training_curve.png'))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    if use_lora:\n",
    "        config = PeftConfig.from_pretrained(model_save_path)\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            config.base_model_name_or_path,\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        if base_model.config.pad_token_id is None:\n",
    "            base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        best_model = PeftModel.from_pretrained(base_model, model_save_path)\n",
    "    else:\n",
    "        best_model = AutoModelForSequenceClassification.from_pretrained(model_save_path)\n",
    "        if best_model.config.pad_token_id is None:\n",
    "            best_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "\n",
    "    # Update label_names to match the dataset\n",
    "    label_names = ['neutral', 'against', 'supportive']\n",
    "    eval_results, test_preds, test_labels = evaluate_model_comprehensive(\n",
    "        best_model,\n",
    "        test_loader,\n",
    "        device,\n",
    "        label_names\n",
    "    )\n",
    "\n",
    "    with open(os.path.join(output_dir, f'{model_name.split(\"/\")[-1]}_results.json'), 'w') as f:\n",
    "        json.dump(eval_results, f, indent=2)\n",
    "\n",
    "    return eval_results\n",
    "\n",
    "\n",
    "############################################\n",
    "# 5. Main Pipeline Function\n",
    "############################################\n",
    "def run_pipeline(df_train, df_test, run_tokenization=True, epochs=10):\n",
    "    \"\"\"Run the stance detection pipeline for the Estonian dataset.\"\"\"\n",
    "    print(f\"Training set: {len(df_train)} sentences\")\n",
    "    print(\"Training stance distribution:\")\n",
    "    print(df_train['stance'].value_counts())\n",
    "    print(f\"\\nTest set: {len(df_test)} sentences\")\n",
    "    print(\"Test stance distribution:\")\n",
    "    print(df_test['stance'].value_counts())\n",
    "\n",
    "    if run_tokenization:\n",
    "        print(\"\\n=== Running Enhanced Tokenization Analysis ===\")\n",
    "        token_df = analyze_tokenization(df_train, df_test)\n",
    "\n",
    "    model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "    results = train_model(model_name, df_train, df_test, epochs=epochs)\n",
    "    return results\n",
    "\n",
    "\n",
    "############################################\n",
    "# 6. Run the Complete Pipeline\n",
    "############################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure df_train and df_test are already defined in your environment\n",
    "    results = run_pipeline(df_train, df_test, run_tokenization=True, epochs=10)\n",
    "\n",
    "    print(\"\\n=== Final Results Summary ===\")\n",
    "    print(f\"Model: meta-llama/Llama-3.1-8B\")\n",
    "    print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"F1 Score (macro): {results['f1_macro']:.4f}\")\n",
    "    print(f\"F1 Score (micro): {results['f1_micro']:.4f}\")\n",
    "    print(f\"Precision (macro): {results['precision_macro']:.4f}\")\n",
    "    print(f\"Recall (macro): {results['recall_macro']:.4f}\")\n",
    "    if results['roc_auc'] is not None:\n",
    "        print(f\"ROC AUC: {results['roc_auc']:.4f}\")\n",
    "\n",
    "    print(\"\\nClass-specific results:\")\n",
    "    for label, metrics in results['class_metrics'].items():\n",
    "        print(f\"  {label}: F1={metrics['f1']:.4f}, Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BRwSK7vQ9hHC",
    "outputId": "4236848d-b835-4c86-b8ea-67f5e303ca4b"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "import os\n",
    "import html\n",
    "import re\n",
    "\n",
    "# Set up paths\n",
    "output_dir = \"tokenization_analysis_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load your tokenizer\n",
    "print(\"Loading Llama tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "\n",
    "# Convert prediction labels from stance names to indices for Estonian stance detection\n",
    "def convert_labels_to_indices(labels):\n",
    "    \"\"\"Convert stance labels to numerical indices for Estonian stance detection.\"\"\"\n",
    "    label_map = {'neutral': 0, 'against': 1, 'supportive': 2}\n",
    "    if isinstance(labels[0], str):\n",
    "        return [label_map[label] for label in labels]\n",
    "    return labels\n",
    "\n",
    "# 1. Calculate vocabulary coverage and OOV rate metrics using df_train\n",
    "def analyze_vocabulary_coverage(df_train, sample_size=None, min_word_length=3):\n",
    "    \"\"\"\n",
    "    Analyze vocabulary coverage and OOV rates for Estonian text with Llama tokenizer.\n",
    "    Uses the \"sentence\" field.\n",
    "    \"\"\"\n",
    "    print(\"Analyzing vocabulary coverage and OOV rates...\")\n",
    "\n",
    "    # Sample data if requested\n",
    "    if sample_size and len(df_train) > sample_size:\n",
    "        sample_df = df_train.sample(sample_size, random_state=42)\n",
    "    else:\n",
    "        sample_df = df_train\n",
    "\n",
    "    total_words = 0\n",
    "    single_token_words = 0\n",
    "    multi_token_words = 0\n",
    "    tokens_per_word = []\n",
    "    word_token_ratios = []\n",
    "    oov_words = []\n",
    "    all_token_ids = []\n",
    "\n",
    "    for _, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Analyzing vocabulary coverage\"):\n",
    "        # Use the \"sentence\" field\n",
    "        sentence = html.unescape(row['sentence'])\n",
    "        sentence_words = sentence.split()\n",
    "        for word in sentence_words:\n",
    "            if len(word) >= min_word_length:\n",
    "                total_words += 1\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                all_token_ids.extend(word_tokens)\n",
    "                tokens_per_word.append(len(word_tokens))\n",
    "                word_token_ratios.append(len(word_tokens) / len(word))\n",
    "                if len(word_tokens) == 1:\n",
    "                    single_token_words += 1\n",
    "                else:\n",
    "                    multi_token_words += 1\n",
    "                    if len(word_tokens) >= 3:\n",
    "                        oov_words.append(word)\n",
    "\n",
    "    coverage_rate = single_token_words / total_words if total_words > 0 else 0\n",
    "    oov_rate = multi_token_words / total_words if total_words > 0 else 0\n",
    "    avg_tokens_per_word = np.mean(tokens_per_word) if tokens_per_word else 0\n",
    "    avg_token_char_ratio = np.mean(word_token_ratios) if word_token_ratios else 0\n",
    "\n",
    "    token_counts = Counter(all_token_ids)\n",
    "    unique_tokens = len(token_counts)\n",
    "    vocabulary_utilization = unique_tokens / len(tokenizer.get_vocab()) if len(tokenizer.get_vocab()) > 0 else 0\n",
    "\n",
    "    results = {\n",
    "        \"total_words_analyzed\": total_words,\n",
    "        \"single_token_words\": single_token_words,\n",
    "        \"multi_token_words\": multi_token_words,\n",
    "        \"vocabulary_coverage_rate\": coverage_rate,\n",
    "        \"oov_rate\": oov_rate,\n",
    "        \"avg_tokens_per_word\": avg_tokens_per_word,\n",
    "        \"avg_token_char_ratio\": avg_token_char_ratio,\n",
    "        \"unique_tokens_used\": unique_tokens,\n",
    "        \"vocabulary_utilization\": vocabulary_utilization,\n",
    "        \"oov_examples\": oov_words[:20]\n",
    "    }\n",
    "\n",
    "    print(f\"\\nVocabulary Coverage Analysis Summary:\")\n",
    "    print(f\"  Total words analyzed: {total_words}\")\n",
    "    print(f\"  Single-token words: {single_token_words} ({coverage_rate:.2%})\")\n",
    "    print(f\"  Multi-token words: {multi_token_words} ({oov_rate:.2%})\")\n",
    "    print(f\"  Average tokens per word: {avg_tokens_per_word:.2f}\")\n",
    "    print(f\"  Unique tokens used: {unique_tokens} of {len(tokenizer.get_vocab())} ({vocabulary_utilization:.2%})\")\n",
    "\n",
    "    with open(os.path.join(output_dir, \"vocabulary_coverage.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    labels_pie = ['Single-token words', 'Multi-token words']\n",
    "    values_pie = [single_token_words, multi_token_words]\n",
    "    plt.pie(values_pie, labels=labels_pie, autopct='%1.1f%%', colors=['#66b3ff', '#ff9999'])\n",
    "    plt.title('Word Tokenization Distribution')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.hist(tokens_per_word, bins=range(1, max(tokens_per_word) + 1), alpha=0.7)\n",
    "    plt.xlabel('Tokens per Word')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Tokens per Word Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    token_freq = sorted(token_counts.values(), reverse=True)\n",
    "    plt.loglog(range(1, len(token_freq) + 1), token_freq)\n",
    "    plt.xlabel('Token Rank')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Token Usage Distribution (Log-Log Scale)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.hist(word_token_ratios, bins=20, alpha=0.7)\n",
    "    plt.xlabel('Tokens per Character Ratio')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Tokenization Efficiency Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"vocabulary_coverage_analysis.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "# 2. Generate predictions for test data based on final evaluation results\n",
    "def generate_test_predictions(test_results):\n",
    "    \"\"\"\n",
    "    Generate predictions from test results.\n",
    "    Returns lists of predicted and true labels based on the confusion matrix.\n",
    "    \"\"\"\n",
    "    conf_matrix = np.array(test_results[\"confusion_matrix\"])\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for i in range(len(conf_matrix)):\n",
    "        for j in range(len(conf_matrix[i])):\n",
    "            count = conf_matrix[i][j]\n",
    "            true_labels.extend([i] * count)\n",
    "            pred_labels.extend([j] * count)\n",
    "    return pred_labels, true_labels\n",
    "\n",
    "# 3. Analyze correlation between tokenization quality and prediction errors using df_test\n",
    "def analyze_tokenization_error_correlation(df_train, df_test, test_predictions, test_labels, sample_size=None):\n",
    "    \"\"\"\n",
    "    Analyze correlation between tokenization quality and prediction errors for Estonian stance detection.\n",
    "    This function uses the test set (df_test) for sampling, as predictions correspond to the test data.\n",
    "    \"\"\"\n",
    "    print(\"\\nAnalyzing correlation between tokenization quality and prediction errors...\")\n",
    "\n",
    "    test_predictions = convert_labels_to_indices(test_predictions)\n",
    "    test_labels = convert_labels_to_indices(test_labels)\n",
    "\n",
    "    # Use the test set for sampling\n",
    "    if sample_size and len(df_test) > sample_size:\n",
    "        indices = np.random.choice(len(df_test), sample_size, replace=False)\n",
    "        sample_df = df_test.iloc[indices].reset_index(drop=True)\n",
    "        sample_predictions = [test_predictions[i] for i in indices]\n",
    "        sample_labels = [test_labels[i] for i in indices]\n",
    "    else:\n",
    "        sample_df = df_test.reset_index(drop=True)\n",
    "        sample_predictions = test_predictions\n",
    "        sample_labels = test_labels\n",
    "\n",
    "    tokenization_metrics = []\n",
    "    for i, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Calculating tokenization metrics\"):\n",
    "        sentence = html.unescape(row['sentence'])\n",
    "        sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        sentence_words = sentence.split()\n",
    "\n",
    "        avg_tokens_per_word = len(sentence_tokens) / len(sentence_words) if sentence_words else 0\n",
    "        tokens_per_char = len(sentence_tokens) / len(sentence) if len(sentence) > 0 else 0\n",
    "\n",
    "        highly_fragmented_count = 0\n",
    "        for word in sentence_words:\n",
    "            if len(word) >= 3:\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                if len(word_tokens) >= 3:\n",
    "                    highly_fragmented_count += 1\n",
    "        oov_ratio = highly_fragmented_count / len(sentence_words) if sentence_words else 0\n",
    "\n",
    "        # Ensure index i is within the bounds of sample_predictions/labels\n",
    "        is_correct = sample_predictions[i] == sample_labels[i]\n",
    "        tokenization_metrics.append({\n",
    "            'index': i,\n",
    "            'stance': row['stance'],\n",
    "            'tokens_per_word': avg_tokens_per_word,\n",
    "            'tokens_per_char': tokens_per_char,\n",
    "            'oov_ratio': oov_ratio,\n",
    "            'highly_fragmented_count': highly_fragmented_count,\n",
    "            'is_correct': is_correct,\n",
    "            'predicted': sample_predictions[i],\n",
    "            'true_label': sample_labels[i]\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(tokenization_metrics)\n",
    "    correct_df = metrics_df[metrics_df['is_correct']]\n",
    "    incorrect_df = metrics_df[~metrics_df['is_correct']]\n",
    "\n",
    "    comparisons = {}\n",
    "    for metric in ['tokens_per_word', 'tokens_per_char', 'oov_ratio', 'highly_fragmented_count']:\n",
    "        correct_mean = correct_df[metric].mean()\n",
    "        incorrect_mean = incorrect_df[metric].mean()\n",
    "        difference = incorrect_mean - correct_mean\n",
    "        difference_pct = difference / correct_mean if correct_mean > 0 else 0\n",
    "        comparisons[metric] = {\n",
    "            'correct_mean': correct_mean,\n",
    "            'incorrect_mean': incorrect_mean,\n",
    "            'difference': difference,\n",
    "            'difference_pct': difference_pct\n",
    "        }\n",
    "\n",
    "    stance_analysis = {}\n",
    "    for stance in metrics_df['stance'].unique():\n",
    "        stance_metrics = metrics_df[metrics_df['stance'] == stance]\n",
    "        correct_stance = stance_metrics[stance_metrics['is_correct']]\n",
    "        incorrect_stance = stance_metrics[~stance_metrics['is_correct']]\n",
    "        if len(correct_stance) > 0 and len(incorrect_stance) > 0:\n",
    "            stance_analysis[stance] = {\n",
    "                'accuracy': len(correct_stance) / len(stance_metrics),\n",
    "                'avg_tokens_per_word_correct': correct_stance['tokens_per_word'].mean(),\n",
    "                'avg_tokens_per_word_incorrect': incorrect_stance['tokens_per_word'].mean(),\n",
    "                'avg_oov_ratio_correct': correct_stance['oov_ratio'].mean(),\n",
    "                'avg_oov_ratio_incorrect': incorrect_stance['oov_ratio'].mean()\n",
    "            }\n",
    "\n",
    "    results = {\n",
    "        'overall_accuracy': len(correct_df) / len(metrics_df) if len(metrics_df) > 0 else 0,\n",
    "        'metric_comparisons': comparisons,\n",
    "        'stance_analysis': stance_analysis\n",
    "    }\n",
    "\n",
    "    print(\"\\nTokenization-Error Correlation Summary:\")\n",
    "    print(f\"  Overall accuracy: {results['overall_accuracy']:.2%}\")\n",
    "    print(\"\\nMetric Comparisons (Correct vs. Incorrect predictions):\")\n",
    "    for metric, values in comparisons.items():\n",
    "        print(f\"  {metric}:\")\n",
    "        print(f\"    Correct predictions: {values['correct_mean']:.4f}\")\n",
    "        print(f\"    Incorrect predictions: {values['incorrect_mean']:.4f}\")\n",
    "        print(f\"    Difference: {values['difference']:.4f} ({values['difference_pct']:.2%})\")\n",
    "\n",
    "    print(\"\\nStance-specific Analysis:\")\n",
    "    for stance, values in stance_analysis.items():\n",
    "        print(f\"  {stance}:\")\n",
    "        print(f\"    Accuracy: {values['accuracy']:.2%}\")\n",
    "        print(f\"    Tokens per word (Correct): {values['avg_tokens_per_word_correct']:.4f}\")\n",
    "        print(f\"    Tokens per word (Incorrect): {values['avg_tokens_per_word_incorrect']:.4f}\")\n",
    "        print(f\"    OOV ratio (Correct): {values['avg_oov_ratio_correct']:.4f}\")\n",
    "        print(f\"    OOV ratio (Incorrect): {values['avg_oov_ratio_incorrect']:.4f}\")\n",
    "\n",
    "    with open(os.path.join(output_dir, \"tokenization_error_correlation.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        results_json = json.dumps(results, default=lambda x: float(x) if isinstance(x, np.float32) else x)\n",
    "        f.write(results_json)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.boxplot(x='is_correct', y='tokens_per_word', data=metrics_df)\n",
    "    plt.title('Tokens per Word by Prediction Correctness')\n",
    "    plt.xlabel('Prediction Correct')\n",
    "    plt.ylabel('Tokens per Word')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.boxplot(x='is_correct', y='oov_ratio', data=metrics_df)\n",
    "    plt.title('OOV Ratio by Prediction Correctness')\n",
    "    plt.xlabel('Prediction Correct')\n",
    "    plt.ylabel('OOV Ratio')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    stance_accuracy = [stance_analysis[stance]['accuracy'] for stance in stance_analysis]\n",
    "    stance_oov = [stance_analysis[stance]['avg_oov_ratio_incorrect'] / stance_analysis[stance]['avg_oov_ratio_correct']\n",
    "                 if stance_analysis[stance]['avg_oov_ratio_correct'] > 0 else 0\n",
    "                 for stance in stance_analysis]\n",
    "    plt.scatter(stance_oov, stance_accuracy)\n",
    "    for i, stance in enumerate(stance_analysis):\n",
    "        plt.annotate(stance, (stance_oov[i], stance_accuracy[i]))\n",
    "    plt.xlabel('OOV Ratio (Incorrect/Correct)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Stance Accuracy vs. OOV Impact')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    confusion = confusion_matrix([metrics_df['true_label'].iloc[i] for i in range(len(metrics_df))],\n",
    "                                 [metrics_df['predicted'].iloc[i] for i in range(len(metrics_df))])\n",
    "    sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"tokenization_error_correlation.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return results, metrics_df\n",
    "\n",
    "# 4. Examine specific examples where tokenization may have affected classification\n",
    "def examine_tokenization_examples(metrics_df, df_test, top_n=10):\n",
    "    \"\"\"\n",
    "    Find and examine specific examples where tokenization may have affected classification.\n",
    "    Uses df_test as the source of examples.\n",
    "    \"\"\"\n",
    "    print(\"\\nFinding examples where tokenization may have affected classification...\")\n",
    "\n",
    "    incorrect_df = metrics_df[~metrics_df['is_correct']].copy()\n",
    "    if len(incorrect_df) == 0:\n",
    "        print(\"No incorrect predictions found in the sample.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    high_oov_incorrect = incorrect_df.sort_values(by='oov_ratio', ascending=False).head(top_n)\n",
    "    examples = []\n",
    "    for _, row in high_oov_incorrect.iterrows():\n",
    "        idx = int(row['index'])\n",
    "        if idx >= len(df_test):\n",
    "            print(f\"Warning: Index {idx} is out of bounds for df_test with length {len(df_test)}\")\n",
    "            continue\n",
    "        original_row = df_test.iloc[idx]\n",
    "        sentence = html.unescape(original_row['sentence'])\n",
    "        sentence_words = sentence.split()\n",
    "        tokenized_words = []\n",
    "        for word in sentence_words:\n",
    "            tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "            token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
    "            if len(tokens) > 2:\n",
    "                tokenized_words.append(f\"[{word} → {' '.join(token_texts)}]\")\n",
    "            else:\n",
    "                tokenized_words.append(word)\n",
    "        tokenized_sentence = \" \".join(tokenized_words)\n",
    "        stance_map = {0: 'neutral', 1: 'against', 2: 'supportive'}\n",
    "        true_label = int(row['true_label'])\n",
    "        predicted = int(row['predicted'])\n",
    "        examples.append({\n",
    "            'original_index': idx,\n",
    "            'sentence': sentence,\n",
    "            'tokenized_sentence': tokenized_sentence,\n",
    "            'true_stance': stance_map.get(true_label, original_row['stance']),\n",
    "            'predicted_stance': stance_map.get(predicted, \"Unknown\"),\n",
    "            'tokens_per_word': row['tokens_per_word'],\n",
    "            'oov_ratio': row['oov_ratio']\n",
    "        })\n",
    "    examples_df = pd.DataFrame(examples)\n",
    "    print(\"\\nExamples where tokenization may have affected classification:\")\n",
    "    for i, example in enumerate(examples):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"  Sentence: {example['sentence']}\")\n",
    "        print(f\"  Tokenized (highlighting problematic words): {example['tokenized_sentence']}\")\n",
    "        print(f\"  True stance: {example['true_stance']}\")\n",
    "        print(f\"  Predicted stance: {example['predicted_stance']}\")\n",
    "        print(f\"  Tokens per word: {example['tokens_per_word']:.2f}\")\n",
    "        print(f\"  OOV ratio: {example['oov_ratio']:.2f}\")\n",
    "    examples_df.to_csv(os.path.join(output_dir, \"tokenization_impact_examples.csv\"), index=False)\n",
    "    return examples_df\n",
    "\n",
    "# 5. Generate summary and recommendations\n",
    "def generate_tokenization_recommendations(coverage_results, correlation_results):\n",
    "    \"\"\"\n",
    "    Generate summary and recommendations based on tokenization analysis for Estonian stance detection.\n",
    "    \"\"\"\n",
    "    oov_rate = coverage_results[\"oov_rate\"]\n",
    "    avg_tokens_per_word = coverage_results[\"avg_tokens_per_word\"]\n",
    "    vocabulary_utilization = coverage_results[\"vocabulary_utilization\"]\n",
    "\n",
    "    overall_accuracy = correlation_results[\"overall_accuracy\"]\n",
    "    tokens_per_word_diff = correlation_results[\"metric_comparisons\"][\"tokens_per_word\"][\"difference_pct\"]\n",
    "    oov_ratio_diff = correlation_results[\"metric_comparisons\"][\"oov_ratio\"][\"difference_pct\"]\n",
    "\n",
    "    if oov_ratio_diff > 0.2:\n",
    "        tokenization_impact = \"High\"\n",
    "    elif oov_ratio_diff > 0.1:\n",
    "        tokenization_impact = \"Moderate\"\n",
    "    else:\n",
    "        tokenization_impact = \"Low\"\n",
    "\n",
    "    summary = f\"\"\"\n",
    "# Tokenization Impact Analysis for Estonian Stance Detection\n",
    "\n",
    "## Summary of Findings\n",
    "\n",
    "The analysis reveals that Llama 3.1's tokenization of Estonian text has a **{tokenization_impact} impact** on stance detection performance.\n",
    "\n",
    "### Key Metrics:\n",
    "- **OOV Rate**: {oov_rate:.2%} of Estonian words require multiple tokens\n",
    "- **Average Tokens per Word**: {avg_tokens_per_word:.2f}\n",
    "- **Vocabulary Utilization**: {vocabulary_utilization:.2%} of available vocabulary tokens are used\n",
    "\n",
    "### Impact on Model Performance:\n",
    "- **Overall Model Accuracy**: {overall_accuracy:.2%}\n",
    "- **Tokenization Impact**: Incorrectly classified examples have {tokens_per_word_diff:.2%} more tokens per word\n",
    "- **OOV Impact**: Incorrectly classified examples have {oov_ratio_diff:.2%} higher OOV ratio\n",
    "\n",
    "## Recommendations for Estonian NLP Tokenization\n",
    "\n",
    "Based on these findings, we recommend:\n",
    "1. **Tokenizer Adaptation:**\n",
    "   - Consider using an Estonian-specific tokenizer or augmenting the vocabulary with common Estonian word pieces if tokenization impact is high.\n",
    "2. **Pre-processing Enhancements:**\n",
    "   - Normalize Estonian text (e.g., handle diacritics and case) before tokenization.\n",
    "3. **Model Adjustments:**\n",
    "   - Further fine-tuning or domain-adaptive pretraining may improve handling of Estonian text tokens.\n",
    "4. **Vocabulary Optimization:**\n",
    "   - If vocabulary utilization is low, explore methods to optimize token usage for Estonian.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This analysis shows that tokenization quality has a {('significant' if tokenization_impact == 'High' else 'moderate' if tokenization_impact == 'Moderate' else 'modest')} impact on Estonian stance detection using Llama 3.1. Despite tokenization challenges, model performance is {('strong' if overall_accuracy > 0.8 else 'reasonable' if overall_accuracy > 0.7 else 'moderate')}, though targeted improvements in tokenization may further enhance accuracy.\n",
    "\"\"\"\n",
    "\n",
    "    with open(os.path.join(output_dir, \"tokenization_recommendations.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(summary)\n",
    "\n",
    "    print(\"\\nTokenization analysis complete!\")\n",
    "    print(f\"Recommendations saved to {os.path.join(output_dir, 'tokenization_recommendations.md')}\")\n",
    "\n",
    "# Full analysis function using df_train and df_test\n",
    "def analyze_tokenization_impact(df_train, df_test, test_predictions=None, test_labels=None, test_results=None):\n",
    "    \"\"\"\n",
    "    Run comprehensive tokenization impact analysis for Estonian stance detection.\n",
    "    Uses df_train for vocabulary analysis and df_test for error correlation.\n",
    "    \"\"\"\n",
    "    if (test_predictions is None or test_labels is None) and test_results is not None:\n",
    "        print(\"Generating predictions from test results...\")\n",
    "        test_predictions, test_labels = generate_test_predictions(test_results)\n",
    "    elif test_predictions is None or test_labels is None:\n",
    "        raise ValueError(\"Either test_predictions and test_labels OR test_results must be provided\")\n",
    "\n",
    "    coverage_results = analyze_vocabulary_coverage(df_train, sample_size=500)\n",
    "    correlation_results, metrics_df = analyze_tokenization_error_correlation(df_train, df_test, test_predictions, test_labels)\n",
    "    examples_df = examine_tokenization_examples(metrics_df, df_test)\n",
    "    generate_tokenization_recommendations(coverage_results, correlation_results)\n",
    "\n",
    "    return {\n",
    "        \"coverage\": coverage_results,\n",
    "        \"correlation\": correlation_results,\n",
    "        \"examples\": examples_df\n",
    "    }\n",
    "\n",
    "############################################\n",
    "# MAIN EXECUTION BLOCK - RUN THIS\n",
    "############################################\n",
    "\n",
    "# Specify the model's test results from your previous run (for 3 classes)\n",
    "test_results = {\n",
    "    \"accuracy\": 0.6677,\n",
    "    \"f1_macro\": 0.5982,\n",
    "    \"precision_macro\": 0.6188,\n",
    "    \"recall_macro\": 0.5884,\n",
    "    \"confusion_matrix\": [\n",
    "        [243, 48, 29],      # neutral predictions\n",
    "        [65, 162, 8],       # against predictions\n",
    "        [56, 11, 31]        # supportive predictions\n",
    "    ],\n",
    "    \"class_metrics\": {\n",
    "        \"neutral\": {\"precision\": 0.6676, \"recall\": 0.7594, \"f1\": 0.7105},\n",
    "        \"against\": {\"precision\": 0.7330, \"recall\": 0.6894, \"f1\": 0.7105},\n",
    "        \"supportive\": {\"precision\": 0.4559, \"recall\": 0.3163, \"f1\": 0.3735}\n",
    "    }\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting tokenization impact analysis...\")\n",
    "\n",
    "    # Ensure df_train and df_test are defined from your preprocessed Estonian dataset.\n",
    "    try:\n",
    "        if 'df_train' in globals() and len(df_train) > 0 and 'df_test' in globals() and len(df_test) > 0:\n",
    "            print(f\"Found existing data: {len(df_train)} training samples, {len(df_test)} test samples\")\n",
    "            results = analyze_tokenization_impact(\n",
    "                df_train=df_train,\n",
    "                df_test=df_test,\n",
    "                test_results=test_results\n",
    "            )\n",
    "            print(\"Analysis complete. Check the 'tokenization_analysis_results' directory for outputs.\")\n",
    "        else:\n",
    "            print(\"ERROR: Required data not found. Make sure df_train and df_test are defined and not empty.\")\n",
    "    except NameError:\n",
    "        print(\"ERROR: Required variables not found. Run your data preprocessing code to create df_train and df_test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pAnuChcX4nV7",
    "outputId": "9dd3c207-6ab1-4df9-e613-dda0fd29f277"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import html\n",
    "\n",
    "def extract_words_from_dataset(dataset_df, text_columns=['sentence']):\n",
    "    \"\"\"Extract all words from the dataset and count their frequency.\n",
    "\n",
    "    This function now works with the Estonian classification task\n",
    "    where the primary text field is 'sentence'.\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    for _, row in dataset_df.iterrows():\n",
    "        for col in text_columns:\n",
    "            if col in row:\n",
    "                # Clean the text from HTML entities and split into words\n",
    "                text = html.unescape(row[col])\n",
    "                words = text.split()\n",
    "                all_words.extend(words)\n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(all_words)\n",
    "    return word_counts\n",
    "\n",
    "# Combine the train and test DataFrames (assumed to be defined as df_train and df_test)\n",
    "combined_df = pd.concat([df_train, df_test])\n",
    "\n",
    "# Extract word counts using the \"sentence\" column\n",
    "word_counts = extract_words_from_dataset(combined_df, text_columns=['sentence'])\n",
    "\n",
    "# Get the most common words (top 1000)\n",
    "most_common_words = word_counts.most_common(1000)\n",
    "print(f\"Found {len(word_counts)} unique words in the dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZcPG1FqO48uc",
    "outputId": "385cdd89-f8c6-4360-ffa3-556f9eb78424"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_word_tokenization(word_counts, top_n=1000, min_freq=5, tokenizer_name=\"meta-llama/Llama-3.1-8B\"):\n",
    "    \"\"\"Analyze how efficiently common words are tokenized.\n",
    "\n",
    "    This function uses the tokenizer for our Estonian classification task.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    # Filter words by minimum frequency\n",
    "    common_words = [word for word, count in word_counts.most_common() if count >= min_freq]\n",
    "    if top_n:\n",
    "        common_words = common_words[:top_n]\n",
    "\n",
    "    results = []\n",
    "    for word in tqdm(common_words, desc=\"Analyzing word tokenization\"):\n",
    "        # Tokenize the word (without adding special tokens)\n",
    "        tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "        token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "        results.append({\n",
    "            'word': word,\n",
    "            'frequency': word_counts[word],\n",
    "            'token_count': len(tokens),\n",
    "            'tokens': token_texts,\n",
    "            'chars_per_token': len(word) / len(tokens) if len(tokens) > 0 else 0\n",
    "        })\n",
    "\n",
    "    # Convert results to a DataFrame for easier analysis\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# Analyze tokenization for common words using the previously computed word_counts\n",
    "tokenization_df = analyze_word_tokenization(word_counts, top_n=5000, min_freq=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3LQh6cKs5P0T",
    "outputId": "b2d866da-3c19-4c10-8a0f-eb6655a2c52f"
   },
   "outputs": [],
   "source": [
    "def find_inefficient_tokens(tokenization_df, min_token_count=3, min_frequency=5):\n",
    "    \"\"\"Find words that are inefficiently tokenized (split into many tokens) and occur frequently.\"\"\"\n",
    "    # Filter for words that get split into at least min_token_count tokens\n",
    "    # and occur at least min_frequency times.\n",
    "    inefficient_df = tokenization_df[\n",
    "        (tokenization_df['token_count'] >= min_token_count) &\n",
    "        (tokenization_df['frequency'] >= min_frequency)\n",
    "    ].sort_values(by=['frequency', 'token_count'], ascending=False)\n",
    "\n",
    "    return inefficient_df\n",
    "\n",
    "# Find inefficiently tokenized words\n",
    "inefficient_words = find_inefficient_tokens(tokenization_df, min_token_count=3, min_frequency=5)\n",
    "\n",
    "print(f\"Found {len(inefficient_words)} inefficiently tokenized words\")\n",
    "print(\"\\nTop 20 most frequent words split into 3+ tokens:\")\n",
    "for _, row in inefficient_words.head(20).iterrows():\n",
    "    print(f\"{row['word']} (freq: {row['frequency']}): split into {row['token_count']} tokens: {row['tokens']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mofEFSQm5bS-",
    "outputId": "201c875d-e997-45c9-a6da-fa25e242b0fd"
   },
   "outputs": [],
   "source": [
    "def find_common_substrings(word_list, min_length=2, min_freq=10, prefix_only=False, suffix_only=False):\n",
    "    \"\"\"Find common prefixes or suffixes in a list of words.\"\"\"\n",
    "    substrings = Counter()\n",
    "\n",
    "    for word in word_list:\n",
    "        if len(word) < min_length:\n",
    "            continue\n",
    "\n",
    "        # Find all potential substrings\n",
    "        if prefix_only:\n",
    "            # Only consider prefixes (limit length to a reasonable maximum, e.g. 5)\n",
    "            for i in range(min_length, min(len(word) + 1, 6)):\n",
    "                substrings[word[:i]] += 1\n",
    "        elif suffix_only:\n",
    "            # Only consider suffixes (limit length similarly)\n",
    "            for i in range(min_length, min(len(word) + 1, 6)):\n",
    "                substrings[word[-i:]] += 1\n",
    "        else:\n",
    "            # Consider all substrings (limit substring length to 5)\n",
    "            for i in range(len(word) - min_length + 1):\n",
    "                for j in range(i + min_length, min(i + 6, len(word) + 1)):\n",
    "                    substrings[word[i:j]] += 1\n",
    "\n",
    "    # Filter substrings by frequency threshold\n",
    "    common_substrings = {s: f for s, f in substrings.items() if f >= min_freq}\n",
    "    return common_substrings\n",
    "\n",
    "# Use the list of inefficiently tokenized words from your previous analysis\n",
    "word_list = inefficient_words['word'].tolist()\n",
    "\n",
    "# Find common prefixes and suffixes with the specified minimum frequency.\n",
    "common_prefixes = find_common_substrings(word_list, min_length=2, min_freq=5, prefix_only=True)\n",
    "common_suffixes = find_common_substrings(word_list, min_length=2, min_freq=5, suffix_only=True)\n",
    "\n",
    "print(\"\\nCommon prefixes in inefficiently tokenized words:\")\n",
    "for prefix, freq in sorted(common_prefixes.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
    "    print(f\"{prefix}: {freq} occurrences\")\n",
    "\n",
    "print(\"\\nCommon suffixes in inefficiently tokenized words:\")\n",
    "for suffix, freq in sorted(common_suffixes.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
    "    print(f\"{suffix}: {freq} occurrences\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "pAJxratByskA",
    "j9WCYLo914AM"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
