{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_01ScrXYsbmF"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qn1C6ZdqATlp",
    "outputId": "4febe1f9-c4ca-4e4a-8aa8-20800ea7d1c7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "data_path = \"/content/drive/MyDrive/AraStance\"\n",
    "train_file = os.path.join(data_path, \"train.jsonl\")\n",
    "test_file = os.path.join(data_path, \"test.jsonl\")\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "df_train = load_jsonl(train_file)\n",
    "df_test = load_jsonl(test_file)\n",
    "\n",
    "df_train = pd.DataFrame(df_train)\n",
    "df_test = pd.DataFrame(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1GbzYQDX9sG"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2oLJrwpPA2wE",
    "outputId": "c9c99575-3601-41ba-c141-17423b29eb7c"
   },
   "outputs": [],
   "source": [
    "import html\n",
    "import random\n",
    "\n",
    "pairs = []\n",
    "for i, row in df_train.iterrows():\n",
    "    claim = row['claim']\n",
    "    for j, (article, stance) in enumerate(zip(row['article'], row['stance'])):\n",
    "        article_title = row['article_title'][j] if isinstance(row['article_title'], list) else row['article_title']\n",
    "        pairs.append({\n",
    "            'claim': claim,\n",
    "            'article': article,\n",
    "            'article_title': article_title,\n",
    "            'stance': stance\n",
    "        })\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs)\n",
    "\n",
    "# Add these lines to see the output\n",
    "print(f\"Total claim-article pairs: {len(pairs_df)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(pairs_df.head())\n",
    "\n",
    "# Count the stance labels\n",
    "print(\"\\nStance distribution:\")\n",
    "print(pairs_df['stance'].value_counts())\n",
    "\n",
    "# Show one example of each stance\n",
    "print(\"\\nExamples of each stance:\")\n",
    "for stance in [\"Agree\", \"Disagree\", \"Discuss\", \"Unrelated\"]:\n",
    "    if stance in pairs_df['stance'].values:\n",
    "        example = pairs_df[pairs_df['stance'] == stance].iloc[0]\n",
    "        print(f\"\\n--- {stance} Example ---\")\n",
    "        print(f\"Claim: {example['claim'][:100]}...\")\n",
    "        print(f\"Article Title: {example['article_title']}\")\n",
    "\n",
    "# Explode the test dataset into claim-article pairs\n",
    "test_pairs = []\n",
    "for i, row in df_test.iterrows():\n",
    "    claim = row['claim']\n",
    "    for j, (article, stance) in enumerate(zip(row['article'], row['stance'])):\n",
    "        article_title = row['article_title'][j] if isinstance(row['article_title'], list) else row['article_title']\n",
    "        test_pairs.append({\n",
    "            'claim': claim,\n",
    "            'article': article,\n",
    "            'article_title': article_title,\n",
    "            'stance': stance\n",
    "        })\n",
    "\n",
    "test_pairs_df = pd.DataFrame(test_pairs)\n",
    "print(f\"Total claim-article pairs in test set: {len(test_pairs_df)}\")\n",
    "print(\"\\nStance distribution in test set:\")\n",
    "print(test_pairs_df['stance'].value_counts())\n",
    "\n",
    "# Clean HTML entities from a sample\n",
    "for stance in [\"Agree\", \"Disagree\", \"Discuss\", \"Unrelated\"]:\n",
    "    stance_examples = pairs_df[pairs_df['stance'] == stance]\n",
    "\n",
    "    if len(stance_examples) > 0:\n",
    "        random_index = random.randint(0, len(stance_examples) - 1)\n",
    "        example = stance_examples.iloc[random_index]\n",
    "\n",
    "        # Clean HTML entities\n",
    "        cleaned_title = html.unescape(example['article_title'])\n",
    "        cleaned_article = html.unescape(example['article'][:150])\n",
    "\n",
    "        print(f\"\\n--- Random {stance} Example (Cleaned) ---\")\n",
    "        print(f\"Claim: {example['claim']}\")\n",
    "        print(f\"Article Title: {cleaned_title}\")\n",
    "        print(f\"Article Excerpt: {cleaned_article}...\")\n",
    "        print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-ZEhbRfYCLU"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZd85AKXA3L9",
    "outputId": "c04fd914-b70c-4b6c-8daa-2f7e0b37ead5"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade huggingface_hub\n",
    "!pip install transformers==4.49.0\n",
    "!pip install accelerate\n",
    "!pip install peft==0.5.0\n",
    "!pip install datasets\n",
    "!pip install bitsandbytes==0.38.2\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i_u3Ec2tBC4n",
    "outputId": "87263468-1df5-4648-d819-c3a2eabff327"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login\n",
    "\n",
    "# 3. Import libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, roc_curve, auc\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import html\n",
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    print(\"CUDA is available! Using GPU.\")\n",
    "else:\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "#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\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wxl-CCQYYpjN",
    "outputId": "5d2e367e-f026-4d2d-f8f1-8519ef8d03fd"
   },
   "outputs": [],
   "source": [
    "# Clone the repository into the current Colab directory\n",
    "!git clone https://github.com/DariusFeher/dynamic-tokenization.git\n",
    "\n",
    "!pip install absl-py accelerate adapters aiohttp aiosignal anyio appdirs argon2-cffi argon2-cffi-bindings arrow asttokens async-lru attrs Babel beautifulsoup4 bleach chex click comm contourpy cryptography cycler datasets debugpy decorator defusedxml dill etils evaluate executing fastjsonschema filelock flax fonttools fqdn frozenlist fsspec gitdb GitPython h11 h5py httpcore httpx huggingface-hub idna importlib_resources ipykernel ipython isoduration jax jaxlib jedi Jinja2 joblib json5 jsonpatch jsonpointer jsonschema jsonschema-specifications jupyter-events jupyter-lsp jupyter_client jupyter_core jupyter_server jupyter_server_terminals jupyterlab jupyterlab_pygments jupyterlab_server kiwisolver lightning-utilities markdown-it-py MarkupSafe matplotlib matplotlib-inline maturin mdurl menuinst mistune ml-dtypes mpmath msgpack multidict multiprocess nbclient nbconvert nbformat nest-asyncio networkx notebook_shim numpy nvidia-cublas-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-cuda-nvrtc-cu12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pDYqDCAvYvpD",
    "outputId": "53e4c3b9-2fb8-4309-f4ad-95ef666938a5"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/reemmadell19/zett\n",
    "\n",
    "!pip install ./zett"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oe0cecEyYydQ"
   },
   "outputs": [],
   "source": [
    "!find /usr/local/lib/python3.11/dist-packages/zett -type f -exec sed -i '/madlad400_metadata.csv/ s/^/#/' {} \\;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DLPe5_qXY1oX",
    "outputId": "0834a50a-32ab-455c-d9e8-cc76deac91b0"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path to the dynamic-tokenization folder\n",
    "repo_path = os.path.join(os.getcwd(), 'dynamic-tokenization')\n",
    "\n",
    "# Add the repository path to sys.path if not already present\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "\n",
    "# Optionally, check the contents to verify the folder structure\n",
    "!ls -l {repo_path}/tokenizations/\n",
    "\n",
    "# Now import the DynamicBPE class from dynamic_bpe_v2.py.\n",
    "# Adjust the module name to exactly match the file name and its case.\n",
    "from tokenizations.dynamic_bpe import Dynamic_BPE\n",
    "\n",
    "print(\"DynamicBPE imported successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tI1WeyAZkr8U"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import copy\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "from typing import Tuple\n",
    "from datasets.formatting.formatting import LazyBatch\n",
    "from tokenizations.tokenizers_utils import tokenize  # Use your existing tokenize utility\n",
    "from tokenizers import pre_tokenizers\n",
    "\n",
    "class ArabicDynamicBPE:\n",
    "    \"\"\"\n",
    "    A dynamic BPE tokenizer tailored for Arabic.\n",
    "\n",
    "    Design choices:\n",
    "      - Normalization: Applies NFKC normalization, removes diacritics (nonspacing marks)\n",
    "        and tatweel (ـ) to standardize Arabic text.\n",
    "      - Merging Criterion: Two tokens are mergeable if neither is a special token and if their\n",
    "        normalized concatenation contains no whitespace. The merged token is allowed if re-tokenizing\n",
    "        via the underlying pre-tokenizer yields at most two segments.\n",
    "      - Merging Process: Repeatedly merge the most frequent valid pair up to a specified maximum.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, tokenizer_boundary: str = \"pretokens\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_boundary = tokenizer_boundary\n",
    "        # Fix: Convert special_tokens_map values to a set of individual tokens.\n",
    "        self.special_token_map = set()\n",
    "        for value in tokenizer.special_tokens_map.values():\n",
    "            if isinstance(value, list):\n",
    "                self.special_token_map.update(value)\n",
    "            else:\n",
    "                self.special_token_map.add(value)\n",
    "\n",
    "        # Use a raw string (r\"\") to avoid invalid escape sequence warnings.\n",
    "        self.punctuation_tokens = {\n",
    "            token for token in tokenizer.vocab\n",
    "            if any(c in token for c in r\"\"\".,!?;:()-\"'`$%&*+<=>@[\\]^_{|}~،؟؛\"\"\")\n",
    "            and not any(c.isdigit() for c in token)\n",
    "        }\n",
    "        self.debug = False\n",
    "\n",
    "    def normalize(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize Arabic text by:\n",
    "          - Applying NFKC normalization.\n",
    "          - Removing diacritics (nonspacing marks).\n",
    "          - Removing tatweel (ـ).\n",
    "        \"\"\"\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "        text = ''.join(ch for ch in text if unicodedata.category(ch) != 'Mn')\n",
    "        text = text.replace(\"ـ\", \"\")\n",
    "        return text\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def is_valid_pair(self, pair: Tuple[str, str]) -> bool:\n",
    "        token1, token2 = pair\n",
    "        # Do not merge if either token is a special token.\n",
    "        if token1 in self.special_token_map or token2 in self.special_token_map:\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            # For \"sentence\" boundary, disallow merging of special tokens.\n",
    "            if self.tokenizer_boundary == \"sentence\":\n",
    "                return token1 not in self.special_token_map and token2 not in self.special_token_map\n",
    "\n",
    "            # Create the merged string using native UTF-8 encoding.\n",
    "            merged_string = (token1 + token2).encode(\"utf-8\").decode(\"utf-8\")\n",
    "\n",
    "            # Relax the merging criteria:\n",
    "            # Allow merge if re-tokenizing the merged string yields at most 2 segments.\n",
    "            if self.tokenizer_boundary == \"pretokens\":\n",
    "                tokens = self.tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(merged_string)\n",
    "                return len(tokens) <= 2\n",
    "            elif self.tokenizer_boundary == \"word\":\n",
    "                tokens = pre_tokenizers.WhitespaceSplit().pre_tokenize_str(merged_string)\n",
    "                return len(tokens) <= 2 and token1 not in self.special_token_map and token2 not in self.special_token_map\n",
    "            elif self.tokenizer_boundary == \"word_hyphen\":\n",
    "                tokens = pre_tokenizers.WhitespaceSplit().pre_tokenize_str(merged_string)\n",
    "                cond1 = len(tokens) <= 2 and token1 not in self.special_token_map and token2 not in self.special_token_map\n",
    "                if cond1 and token1 not in self.punctuation_tokens and token2 in self.punctuation_tokens:\n",
    "                    return token2 == \"-\"\n",
    "                return cond1\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def get_most_frequent_pair(self, batch_tokens) -> Tuple[str, str]:\n",
    "        pair_freqs = Counter()\n",
    "        for token_sequence in batch_tokens:\n",
    "            pairs = list(zip(token_sequence, token_sequence[1:]))\n",
    "            valid_pairs = [pair for pair in pairs if self.is_valid_pair(pair)]\n",
    "            pair_freqs.update(valid_pairs)\n",
    "        if pair_freqs:\n",
    "            return max(pair_freqs, key=pair_freqs.get)\n",
    "        return \"\"\n",
    "\n",
    "    def merge_pair(self, a: str, b: str, batch_tokens, ner: bool = False, batch_word_ids: list = []):\n",
    "        for idx, token_seq in enumerate(batch_tokens):\n",
    "            i = 0\n",
    "            new_token_seq = []\n",
    "            new_word_ids = []\n",
    "            while i < len(token_seq):\n",
    "                if i < len(token_seq) - 1 and token_seq[i] == a and token_seq[i+1] == b:\n",
    "                    new_token_seq.append(a + b)\n",
    "                    if ner:\n",
    "                        new_word_ids.append(batch_word_ids[idx][i])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_token_seq.append(token_seq[i])\n",
    "                    if ner:\n",
    "                        new_word_ids.append(batch_word_ids[idx][i])\n",
    "                    i += 1\n",
    "            batch_tokens[idx] = new_token_seq\n",
    "            if ner:\n",
    "                batch_word_ids[idx] = new_word_ids\n",
    "        return batch_tokens, batch_word_ids\n",
    "\n",
    "    def tokenize_base_case(self, batch_examples, mlm: bool = False, max_length: int = 128,\n",
    "                             ner: bool = False, nli: bool = False, mmlu: bool = False):\n",
    "        batch_tokens = []\n",
    "        unique_tokens_original = set()\n",
    "        batch_word_tokens = []\n",
    "        batch_word_ids = []\n",
    "        if mmlu:\n",
    "            if isinstance(batch_examples, list):\n",
    "                for batch_example in batch_examples:\n",
    "                    tokens = [\"<s>\"] + tokenize(\n",
    "                        batch_example,\n",
    "                        self.tokenizer,\n",
    "                        max_length=max_length,\n",
    "                        truncation=True,\n",
    "                    )\n",
    "                    if len(tokens) > max_length:\n",
    "                        tokens = tokens[:max_length]\n",
    "                    unique_tokens_original.update(tokens)\n",
    "                    batch_tokens.append(tokens)\n",
    "            else:\n",
    "                for idx, _ in enumerate(batch_examples[\"prompt\"]):\n",
    "                    tokens = [\"<s>\"] + tokenize(\n",
    "                        batch_examples[\"prompt\"][idx],\n",
    "                        self.tokenizer,\n",
    "                        max_length=max_length,\n",
    "                        truncation=True,\n",
    "                    )\n",
    "                    if len(tokens) > max_length:\n",
    "                        tokens = tokens[:max_length]\n",
    "                    unique_tokens_original.update(tokens)\n",
    "                    batch_tokens.append(tokens)\n",
    "        elif ner:\n",
    "            if isinstance(batch_examples, list):\n",
    "                for batch_example in batch_examples:\n",
    "                    tokens = [\"<s>\"]\n",
    "                    word_ids = [None]\n",
    "                    for word_index, word in enumerate(batch_example[\"tokens\"]):\n",
    "                        subtokens = self.tokenizer.tokenize(word, max_length=max_length)\n",
    "                        tokens.extend(subtokens)\n",
    "                        word_ids.extend([word_index] * len(subtokens))\n",
    "                    if len(tokens) >= max_length:\n",
    "                        tokens = tokens[: max_length - 1]\n",
    "                        word_ids = word_ids[: max_length - 1]\n",
    "                    tokens.append(\"</s>\")\n",
    "                    word_ids.append(None)\n",
    "                    batch_tokens.append(tokens)\n",
    "                    unique_tokens_original.update(tokens)\n",
    "                    batch_word_tokens.append(tokens)\n",
    "                    batch_word_ids.append(word_ids)\n",
    "            else:\n",
    "                for idx, _ in enumerate(batch_examples[\"tokens\"]):\n",
    "                    tokens = [\"<s>\"]\n",
    "                    word_ids = [None]\n",
    "                    for word_index, word in enumerate(batch_examples[\"tokens\"][idx]):\n",
    "                        subtokens = self.tokenizer.tokenize(word, max_length=max_length)\n",
    "                        tokens.extend(subtokens)\n",
    "                        word_ids.extend([word_index] * len(subtokens))\n",
    "                    if len(tokens) >= max_length:\n",
    "                        tokens = tokens[: max_length - 1]\n",
    "                        word_ids = word_ids[: max_length - 1]\n",
    "                    tokens.append(\"</s>\")\n",
    "                    word_ids.append(None)\n",
    "                    unique_tokens_original.update(tokens)\n",
    "                    batch_tokens.append(tokens)\n",
    "                    batch_word_ids.append(word_ids)\n",
    "        elif nli:\n",
    "            if isinstance(batch_examples, list):\n",
    "                for batch_example in batch_examples:\n",
    "                    tokens = (\n",
    "                        [\"<s>\"]\n",
    "                        + tokenize(batch_example[\"premise\"], self.tokenizer)\n",
    "                        + [\"</s>\", \"</s>\"]\n",
    "                        + tokenize(batch_example[\"hypothesis\"], self.tokenizer)\n",
    "                        + [\"</s>\"]\n",
    "                    )\n",
    "                    batch_tokens.append(tokens)\n",
    "                    unique_tokens_original.update(tokens)\n",
    "                    if self.debug:\n",
    "                        tokens_word = (\n",
    "                            [\"<s>\"]\n",
    "                            + pretokenize(batch_example[\"premise\"], self.tokenizer)\n",
    "                            + [\"</s>\", \"</s>\"]\n",
    "                            + pretokenize(batch_example[\"hypothesis\"], self.tokenizer)\n",
    "                            + [\"</s>\"]\n",
    "                        )\n",
    "                        batch_word_tokens.append(tokens_word)\n",
    "            else:\n",
    "                for idx, _ in enumerate(batch_examples[\"premise\"]):\n",
    "                    tokens = (\n",
    "                        [\"<s>\"]\n",
    "                        + tokenize(batch_examples[\"premise\"][idx], self.tokenizer)\n",
    "                        + [\"</s>\", \"</s>\"]\n",
    "                        + tokenize(batch_examples[\"hypothesis\"][idx], self.tokenizer)\n",
    "                        + [\"</s>\"]\n",
    "                    )\n",
    "                    batch_tokens.append(tokens)\n",
    "                    unique_tokens_original.update(tokens)\n",
    "                    if self.debug:\n",
    "                        tokens_word = (\n",
    "                            [\"<s>\"]\n",
    "                            + pretokenize(batch_examples[\"premise\"][idx], self.tokenizer)\n",
    "                            + [\"</s>\", \"</s>\"]\n",
    "                            + pretokenize(batch_examples[\"hypothesis\"][idx], self.tokenizer)\n",
    "                            + [\"</s>\"]\n",
    "                        )\n",
    "                        batch_word_tokens.append(tokens_word)\n",
    "        elif mlm:\n",
    "            if isinstance(batch_examples, list):\n",
    "                for batch_example in batch_examples:\n",
    "                    tokens = (\n",
    "                        [\"<s>\"]\n",
    "                        + tokenize(\n",
    "                            batch_example[\"text\"],\n",
    "                            self.tokenizer,\n",
    "                            max_length=max_length - 2,\n",
    "                        )\n",
    "                        + [\"</s>\"]\n",
    "                    )\n",
    "                    tokens = tokens[:max_length]\n",
    "                    batch_tokens.append(tokens)\n",
    "                    unique_tokens_original.update(tokens)\n",
    "                    if self.debug:\n",
    "                        tokens_word = (\n",
    "                            [\"<s>\"]\n",
    "                            + pretokenize(batch_example[\"text\"], self.tokenizer)\n",
    "                            + [\"</s>\"]\n",
    "                        )\n",
    "                        batch_word_tokens.append(tokens_word)\n",
    "            else:\n",
    "                for idx, _ in enumerate(batch_examples[\"text\"]):\n",
    "                    tokens = (\n",
    "                        [\"<s>\"]\n",
    "                        + tokenize(\n",
    "                            batch_examples[\"text\"][idx],\n",
    "                            max_length=max_length - 2,\n",
    "                            truncation=True,\n",
    "                            tokenizer=self.tokenizer,\n",
    "                        )\n",
    "                        + [\"</s>\"]\n",
    "                    )\n",
    "                    if self.debug:\n",
    "                        tokens_word = (\n",
    "                            [\"<s>\"]\n",
    "                            + pretokenize(batch_examples[\"text\"][idx], self.tokenizer)\n",
    "                            + [\"</s>\"]\n",
    "                        )\n",
    "                        batch_word_tokens.append(tokens_word)\n",
    "        return unique_tokens_original, batch_tokens, batch_word_tokens, batch_word_ids\n",
    "\n",
    "    def tokenize_batch(self, batch_examples: LazyBatch, max_nr_merges: int = 1000,\n",
    "                       mlm: bool = False, max_length: int = 128, ner: bool = False,\n",
    "                       nli: bool = False, mmlu: bool = False):\n",
    "        unique_tokens_original, batch_tokens, batch_word_tokens, batch_word_ids = (\n",
    "            self.tokenize_base_case(\n",
    "                batch_examples=batch_examples,\n",
    "                mlm=mlm,\n",
    "                max_length=max_length,\n",
    "                ner=ner,\n",
    "                nli=nli,\n",
    "                mmlu=mmlu,\n",
    "            )\n",
    "        )\n",
    "        total_merges = 0\n",
    "        while total_merges < max_nr_merges:\n",
    "            best_pair = self.get_most_frequent_pair(batch_tokens=batch_tokens)\n",
    "            if best_pair == \"\":\n",
    "\n",
    "                break\n",
    "            total_merges += 1\n",
    "            batch_tokens, batch_word_ids = self.merge_pair(\n",
    "                a=best_pair[0],\n",
    "                b=best_pair[1],\n",
    "                batch_tokens=batch_tokens,\n",
    "                ner=ner,\n",
    "                batch_word_ids=batch_word_ids,\n",
    "            )\n",
    "        unique_tokens_bpe = set()\n",
    "        batch_seq_lengths = []\n",
    "        for tokenised_text in batch_tokens:\n",
    "            unique_tokens_bpe.update(tokenised_text)\n",
    "            batch_seq_lengths.append(len(tokenised_text))\n",
    "        if self.debug:\n",
    "            for i in range(32):\n",
    "                if i < len(batch_tokens) and batch_tokens[i] != batch_word_tokens[i]:\n",
    "                    print(i)\n",
    "                    print(batch_tokens[i])\n",
    "                    print(batch_word_tokens[i])\n",
    "        return batch_tokens, unique_tokens_bpe, batch_seq_lengths, batch_word_ids\n",
    "\n",
    "    def tokenize_batch_for_seq_len(self, batch_examples: LazyBatch, max_nr_merges: int = 20000,\n",
    "                                   mlm: bool = False, max_length: int = 128, ner: bool = False,\n",
    "                                   nli: bool = False, mmlu: bool = False):\n",
    "        batch_tokens = []\n",
    "        total_merges = 0\n",
    "        _, batch_tokens, _, _ = self.tokenize_base_case(\n",
    "            batch_examples=batch_examples,\n",
    "            mlm=False,\n",
    "            max_length=max_length,\n",
    "            ner=False,\n",
    "            nli=False,\n",
    "            mmlu=True,\n",
    "        )\n",
    "        import copy\n",
    "        init_batch_tokens = copy.deepcopy(batch_tokens)\n",
    "        if total_merges not in self.merges2seqLen:\n",
    "            self.merges2seqLen[total_merges] = 0\n",
    "        for tokenised_text in batch_tokens:\n",
    "            self.merges2seqLen[total_merges] += len(tokenised_text)\n",
    "        while total_merges < max_nr_merges:\n",
    "            best_pair = self.get_most_frequent_pair(batch_tokens=batch_tokens)\n",
    "            if best_pair == \"\":\n",
    "                for i in range(total_merges + 1, max_nr_merges):\n",
    "                    if i not in self.merges2seqLen:\n",
    "                        self.merges2seqLen[i] = 0\n",
    "                    for tokenised_text in batch_tokens:\n",
    "                        self.merges2seqLen[i] += len(tokenised_text)\n",
    "                break\n",
    "            total_merges += 1\n",
    "            batch_tokens, _ = self.merge_pair(\n",
    "                a=best_pair[0], b=best_pair[1], batch_tokens=batch_tokens\n",
    "            )\n",
    "            if total_merges not in self.merges2seqLen:\n",
    "                self.merges2seqLen[total_merges] = 0\n",
    "            for tokenised_text in batch_tokens:\n",
    "                self.merges2seqLen[total_merges] += len(tokenised_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIAArpw6kuor"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ArabicDynamicBPETokenizer:\n",
    "    \"\"\"\n",
    "    A wrapper that exposes a Hugging Face tokenizer interface while\n",
    "    applying dynamic merging via the ArabicDynamicBPE class.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_tokenizer, arabic_dynamic_bpe):\n",
    "        self.base_tokenizer = base_tokenizer\n",
    "        self.arabic_dynamic_bpe = arabic_dynamic_bpe\n",
    "        # Save the vocabulary for later use.\n",
    "        self._vocab = base_tokenizer.get_vocab()\n",
    "        self.pad_token = base_tokenizer.pad_token\n",
    "        self.eos_token = base_tokenizer.eos_token\n",
    "        self.pad_token_id = base_tokenizer.pad_token_id\n",
    "        self.eos_token_id = base_tokenizer.eos_token_id\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self._vocab\n",
    "    # Add get_vocab() method\n",
    "    def get_vocab(self):\n",
    "        return self.base_tokenizer.get_vocab()\n",
    "\n",
    "\n",
    "    def __call__(self, text, max_length=512, truncation=True, return_tensors=None, max_nr_merges=1000, **kwargs):\n",
    "        \"\"\"\n",
    "        Tokenizes the input text without padding.\n",
    "\n",
    "        It applies dynamic merging using ArabicDynamicBPE and then converts the merged tokens to IDs.\n",
    "        No extra pad tokens are added.\n",
    "        \"\"\"\n",
    "        # Use the ArabicDynamicBPE's tokenize_batch method on a single-sample batch.\n",
    "        batch_tokens, _, batch_seq_lengths, _ = self.arabic_dynamic_bpe.tokenize_batch(\n",
    "            [text],\n",
    "            max_nr_merges=max_nr_merges,\n",
    "            mlm=False,\n",
    "            max_length=max_length,\n",
    "            ner=False,\n",
    "            nli=False,\n",
    "            mmlu=False\n",
    "        )\n",
    "        # Fallback: If no merges occur, fall back to the base tokenizer.\n",
    "        if not batch_tokens or len(batch_tokens) == 0 or len(batch_tokens[0]) == 0:\n",
    "            output = self.base_tokenizer(\n",
    "                text, max_length=max_length, truncation=truncation, return_tensors=return_tensors, **kwargs\n",
    "            )\n",
    "            return output\n",
    "\n",
    "        # Remove any special boundary markers if desired.\n",
    "        merged_tokens = [t for t in batch_tokens[0] if t not in {\"<|begin_of_text|>\", \"<|end_of_text|>\"}]\n",
    "        # Convert the merged tokens to IDs.\n",
    "        input_ids = self.base_tokenizer.convert_tokens_to_ids(merged_tokens)\n",
    "\n",
    "        # Apply truncation if necessary.\n",
    "        if truncation:\n",
    "            input_ids = input_ids[:max_length]\n",
    "        # Note: We are NOT performing any padding here.\n",
    "        # Create an attention mask that is 1 for each token in the unpadded sequence.\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        if return_tensors == \"pt\":\n",
    "            input_ids = torch.tensor(input_ids)\n",
    "            attention_mask = torch.tensor(attention_mask)\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "    def encode(self, text, **kwargs):\n",
    "        return self.__call__(text, **kwargs)[\"input_ids\"]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return self.base_tokenizer.convert_ids_to_tokens(ids)\n",
    "\n",
    "    def save_pretrained(self, path):\n",
    "        \"\"\"\n",
    "        Delegate saving to the wrapped base tokenizer.\n",
    "        \"\"\"\n",
    "        return self.base_tokenizer.save_pretrained(path)\n",
    "\n",
    "def get_arabic_dynamic_tokenizer(model_name=\"google/gemma-7b\", use_fast=True, tokenizer_boundary=\"pretokens\"):\n",
    "    \"\"\"\n",
    "    Loads the base Hugging Face tokenizer and wraps it with your ArabicDynamicBPE.\n",
    "    This function always applies Arabic-specific normalization and merging criteria.\n",
    "    \"\"\"\n",
    "    from transformers import AutoTokenizer\n",
    "    base_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=use_fast)\n",
    "    if base_tokenizer.pad_token is None:\n",
    "        base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "    # Instantiate your ArabicDynamicBPE (using your provided code).\n",
    "    arabic_dynamic_bpe = ArabicDynamicBPE(base_tokenizer, tokenizer_boundary=tokenizer_boundary)\n",
    "    # Return the wrapped tokenizer.\n",
    "    return ArabicDynamicBPETokenizer(base_tokenizer, arabic_dynamic_bpe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFIqg7gmrx5j"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import copy\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "from typing import Tuple\n",
    "from datasets.formatting.formatting import LazyBatch\n",
    "from tokenizations.tokenizers_utils import tokenize  # Use your existing tokenize utility\n",
    "from tokenizers import pre_tokenizers\n",
    "import torch\n",
    "\n",
    "class ArabicDynamicBPE:\n",
    "    \"\"\"\n",
    "    A dynamic BPE tokenizer tailored for Arabic.\n",
    "\n",
    "    Design choices:\n",
    "      - Normalization: Applies NFKC normalization, removes diacritics (nonspacing marks)\n",
    "        and tatweel (ـ) to standardize Arabic text.\n",
    "      - Merging Criterion: Two tokens are mergeable if neither is a special token and if their\n",
    "        normalized concatenation contains no whitespace. The merged token is allowed if re-tokenizing\n",
    "        via the underlying pre-tokenizer yields at most two segments.\n",
    "      - Merging Process: Repeatedly merge the most frequent valid pair up to a specified maximum.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, tokenizer_boundary: str = \"pretokens\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_boundary = tokenizer_boundary\n",
    "        # Fix: Convert special_tokens_map values to a set of individual tokens.\n",
    "        self.special_token_map = set()\n",
    "        for value in tokenizer.special_tokens_map.values():\n",
    "            if isinstance(value, list):\n",
    "                self.special_token_map.update(value)\n",
    "            else:\n",
    "                self.special_token_map.add(value)\n",
    "\n",
    "        # Use a raw string (r\"\") to avoid invalid escape sequence warnings.\n",
    "        self.punctuation_tokens = {\n",
    "            token for token in tokenizer.vocab\n",
    "            if any(c in token for c in r\"\"\".,!?;:()-\"'`$%&*+<=>@[\\]^_{|}~،؟؛\"\"\")\n",
    "            and not any(c.isdigit() for c in token)\n",
    "        }\n",
    "        self.debug = False\n",
    "\n",
    "    def normalize(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize Arabic text by:\n",
    "          - Applying NFKC normalization.\n",
    "          - Removing diacritics (nonspacing marks).\n",
    "          - Removing tatweel (ـ).\n",
    "        \"\"\"\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "        text = ''.join(ch for ch in text if unicodedata.category(ch) != 'Mn')\n",
    "        text = text.replace(\"ـ\", \"\")\n",
    "        return text\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def is_valid_pair(self, pair: Tuple[str, str]) -> bool:\n",
    "        token1, token2 = pair\n",
    "        # Do not merge if either token is a special token.\n",
    "        if token1 in self.special_token_map or token2 in self.special_token_map:\n",
    "            return False\n",
    "        try:\n",
    "            # For \"sentence\" boundary, disallow merging of special tokens.\n",
    "            if self.tokenizer_boundary == \"sentence\":\n",
    "                return token1 not in self.special_token_map and token2 not in self.special_token_map\n",
    "            # Create the merged string using native UTF-8 encoding.\n",
    "            merged_string = (token1 + token2).encode(\"utf-8\").decode(\"utf-8\")\n",
    "            # Relax the merging criteria:\n",
    "            # Allow merge if re-tokenizing the merged string yields at most 2 segments.\n",
    "            if self.tokenizer_boundary == \"pretokens\":\n",
    "                tokens = self.tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(merged_string)\n",
    "                return len(tokens) <= 2\n",
    "            elif self.tokenizer_boundary == \"word\":\n",
    "                tokens = pre_tokenizers.WhitespaceSplit().pre_tokenize_str(merged_string)\n",
    "                return len(tokens) <= 2 and token1 not in self.special_token_map and token2 not in self.special_token_map\n",
    "            elif self.tokenizer_boundary == \"word_hyphen\":\n",
    "                tokens = pre_tokenizers.WhitespaceSplit().pre_tokenize_str(merged_string)\n",
    "                cond1 = len(tokens) <= 2 and token1 not in self.special_token_map and token2 not in self.special_token_map\n",
    "                if cond1 and token1 not in self.punctuation_tokens and token2 in self.punctuation_tokens:\n",
    "                    return token2 == \"-\"\n",
    "                return cond1\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def get_most_frequent_pair(self, batch_tokens) -> Tuple[str, str]:\n",
    "        pair_freqs = Counter()\n",
    "        for token_sequence in batch_tokens:\n",
    "            pairs = list(zip(token_sequence, token_sequence[1:]))\n",
    "            valid_pairs = [pair for pair in pairs if self.is_valid_pair(pair)]\n",
    "            pair_freqs.update(valid_pairs)\n",
    "        if pair_freqs:\n",
    "            return max(pair_freqs, key=pair_freqs.get)\n",
    "        return \"\"\n",
    "\n",
    "    def merge_pair(self, a: str, b: str, batch_tokens, ner: bool = False, batch_word_ids: list = []):\n",
    "        for idx, token_seq in enumerate(batch_tokens):\n",
    "            i = 0\n",
    "            new_token_seq = []\n",
    "            new_word_ids = []\n",
    "            while i < len(token_seq):\n",
    "                if i < len(token_seq) - 1 and token_seq[i] == a and token_seq[i+1] == b:\n",
    "                    new_token_seq.append(a + b)\n",
    "                    if ner:\n",
    "                        new_word_ids.append(batch_word_ids[idx][i])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_token_seq.append(token_seq[i])\n",
    "                    if ner:\n",
    "                        new_word_ids.append(batch_word_ids[idx][i])\n",
    "                    i += 1\n",
    "            batch_tokens[idx] = new_token_seq\n",
    "            if ner:\n",
    "                batch_word_ids[idx] = new_word_ids\n",
    "        return batch_tokens, batch_word_ids\n",
    "\n",
    "    def tokenize_base_case(self, batch_examples, mlm: bool = False, max_length: int = 128,\n",
    "                             ner: bool = False, nli: bool = False, mmlu: bool = False):\n",
    "        batch_tokens = []\n",
    "        unique_tokens_original = set()\n",
    "        batch_word_tokens = []\n",
    "        batch_word_ids = []\n",
    "        # Here we assume batch_examples is a list of texts (for mlm or other modes)\n",
    "        if isinstance(batch_examples, list):\n",
    "            for batch_example in batch_examples:\n",
    "                tokens = [\"<s>\"] + tokenize(\n",
    "                    batch_example,\n",
    "                    self.tokenizer,\n",
    "                    max_length=max_length,\n",
    "                    truncation=True,\n",
    "                )\n",
    "                if len(tokens) > max_length:\n",
    "                    tokens = tokens[:max_length]\n",
    "                unique_tokens_original.update(tokens)\n",
    "                batch_tokens.append(tokens)\n",
    "        else:\n",
    "            # For backward compatibility if batch_examples is not a list.\n",
    "            raise ValueError(\"batch_examples must be a list of texts\")\n",
    "        return unique_tokens_original, batch_tokens, batch_word_tokens, batch_word_ids\n",
    "\n",
    "    def tokenize_batch(self, batch_examples, max_nr_merges: int = 1000,\n",
    "                       mlm: bool = False, max_length: int = 128, ner: bool = False,\n",
    "                       nli: bool = False, mmlu: bool = False):\n",
    "        unique_tokens_original, batch_tokens, batch_word_tokens, batch_word_ids = (\n",
    "            self.tokenize_base_case(\n",
    "                batch_examples=batch_examples,\n",
    "                mlm=mlm,\n",
    "                max_length=max_length,\n",
    "                ner=ner,\n",
    "                nli=nli,\n",
    "                mmlu=mmlu,\n",
    "            )\n",
    "        )\n",
    "        total_merges = 0\n",
    "        if self.debug:\n",
    "            print(\"Dynamic tokenizer: Starting tokenize_batch with dynamic merging...\")\n",
    "        while total_merges < max_nr_merges:\n",
    "            best_pair = self.get_most_frequent_pair(batch_tokens=batch_tokens)\n",
    "            if best_pair == \"\":\n",
    "                if self.debug:\n",
    "                    print(f\"No valid pair found after {total_merges} merges.\")\n",
    "                break\n",
    "            total_merges += 1\n",
    "            if self.debug:\n",
    "                print(f\"Merging pair {best_pair} at merge count {total_merges}.\")\n",
    "            batch_tokens, batch_word_ids = self.merge_pair(\n",
    "                a=best_pair[0],\n",
    "                b=best_pair[1],\n",
    "                batch_tokens=batch_tokens,\n",
    "                ner=ner,\n",
    "                batch_word_ids=batch_word_ids,\n",
    "            )\n",
    "        if self.debug:\n",
    "            print(f\"Dynamic tokenizer: Completed merges. Total merges: {total_merges}\")\n",
    "            for i in range(min(32, len(batch_tokens))):\n",
    "                print(f\"Sample {i} tokens: {batch_tokens[i]}\")\n",
    "        unique_tokens_bpe = set()\n",
    "        batch_seq_lengths = []\n",
    "        for tokenised_text in batch_tokens:\n",
    "            unique_tokens_bpe.update(tokenised_text)\n",
    "            batch_seq_lengths.append(len(tokenised_text))\n",
    "        return batch_tokens, unique_tokens_bpe, batch_seq_lengths, batch_word_ids\n",
    "\n",
    "    def tokenize_batch_for_seq_len(self, batch_examples, max_nr_merges: int = 20000,\n",
    "                                   mlm: bool = False, max_length: int = 128, ner: bool = False,\n",
    "                                   nli: bool = False, mmlu: bool = False):\n",
    "        # (Not modified here.)\n",
    "        pass  # Omitted for brevity\n",
    "\n",
    "class ArabicDynamicBPETokenizer:\n",
    "    \"\"\"\n",
    "    A wrapper that exposes a Hugging Face tokenizer interface while\n",
    "    applying dynamic merging via the ArabicDynamicBPE class.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_tokenizer, arabic_dynamic_bpe):\n",
    "        self.base_tokenizer = base_tokenizer\n",
    "        self.arabic_dynamic_bpe = arabic_dynamic_bpe\n",
    "        # Save the vocabulary for later use.\n",
    "        self._vocab = base_tokenizer.get_vocab()\n",
    "        self.pad_token = base_tokenizer.pad_token\n",
    "        self.eos_token = base_tokenizer.eos_token\n",
    "        self.pad_token_id = base_tokenizer.pad_token_id\n",
    "        self.eos_token_id = base_tokenizer.eos_token_id\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self._vocab\n",
    "    # Add get_vocab() method\n",
    "    def get_vocab(self):\n",
    "        return self.base_tokenizer.get_vocab()\n",
    "\n",
    "    def __call__(self, text, max_length=512, truncation=True, return_tensors=None, max_nr_merges=1000, **kwargs):\n",
    "        \"\"\"\n",
    "        Tokenizes a single input text without padding.\n",
    "\n",
    "        It applies dynamic merging using ArabicDynamicBPE and then converts the merged tokens to IDs.\n",
    "        No extra pad tokens are added.\n",
    "        \"\"\"\n",
    "        # Process a single example as a batch of one.\n",
    "        batch_tokens, _, batch_seq_lengths, _ = self.arabic_dynamic_bpe.tokenize_batch(\n",
    "            [text],\n",
    "            max_nr_merges=max_nr_merges,\n",
    "            mlm=False,\n",
    "            max_length=max_length,\n",
    "            ner=False,\n",
    "            nli=False,\n",
    "            mmlu=False\n",
    "        )\n",
    "        # Fallback: If no merges occur, fall back to the base tokenizer.\n",
    "        if not batch_tokens or len(batch_tokens) == 0 or len(batch_tokens[0]) == 0:\n",
    "            output = self.base_tokenizer(\n",
    "                text, max_length=max_length, truncation=truncation, return_tensors=return_tensors, **kwargs\n",
    "            )\n",
    "            return output\n",
    "\n",
    "        merged_tokens = [t for t in batch_tokens[0] if t not in {\"<|begin_of_text|>\", \"<|end_of_text|>\"}]\n",
    "        input_ids = self.base_tokenizer.convert_tokens_to_ids(merged_tokens)\n",
    "\n",
    "        if truncation:\n",
    "            input_ids = input_ids[:max_length]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        if return_tensors == \"pt\":\n",
    "            input_ids = torch.tensor(input_ids)\n",
    "            attention_mask = torch.tensor(attention_mask)\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "    def encode(self, text, **kwargs):\n",
    "        return self.__call__(text, **kwargs)[\"input_ids\"]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return self.base_tokenizer.convert_ids_to_tokens(ids)\n",
    "\n",
    "    def save_pretrained(self, path):\n",
    "        return self.base_tokenizer.save_pretrained(path)\n",
    "\n",
    "    # New method for encoding batches.\n",
    "    def encode_batch(self, texts, max_length=512, truncation=True, return_tensors=None, max_nr_merges=1000, **kwargs):\n",
    "        \"\"\"\n",
    "        Tokenizes a batch of input texts using dynamic merging.\n",
    "        Returns a list of dictionaries with input_ids and attention_mask.\n",
    "        \"\"\"\n",
    "        # texts should be a list of strings.\n",
    "        batch_tokens, _, batch_seq_lengths, _ = self.arabic_dynamic_bpe.tokenize_batch(\n",
    "            texts,\n",
    "            max_nr_merges=max_nr_merges,\n",
    "            mlm=False,\n",
    "            max_length=max_length,\n",
    "            ner=False,\n",
    "            nli=False,\n",
    "            mmlu=False\n",
    "        )\n",
    "        outputs = []\n",
    "        for tokens in batch_tokens:\n",
    "            merged_tokens = [t for t in tokens if t not in {\"<|begin_of_text|>\", \"<|end_of_text|>\"}]\n",
    "            input_ids = self.base_tokenizer.convert_tokens_to_ids(merged_tokens)\n",
    "            if truncation:\n",
    "                input_ids = input_ids[:max_length]\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "            if return_tensors == \"pt\":\n",
    "                input_ids = torch.tensor(input_ids)\n",
    "                attention_mask = torch.tensor(attention_mask)\n",
    "            outputs.append({\"input_ids\": input_ids, \"attention_mask\": attention_mask})\n",
    "        return outputs\n",
    "\n",
    "def get_arabic_dynamic_tokenizer(model_name=\"google/gemma-7b\", use_fast=True, tokenizer_boundary=\"pretokens\"):\n",
    "    from transformers import AutoTokenizer\n",
    "    base_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=use_fast)\n",
    "    if base_tokenizer.pad_token is None:\n",
    "        base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "    arabic_dynamic_bpe = ArabicDynamicBPE(base_tokenizer, tokenizer_boundary=tokenizer_boundary)\n",
    "    return ArabicDynamicBPETokenizer(base_tokenizer, arabic_dynamic_bpe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gf66PkCpY2nY"
   },
   "source": [
    "# Dynamic Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3643d1ae28904ccead408cc8dea206c8",
      "6302e3be93714546b497561777c14086",
      "94c8cf8889a04d199e2986251630a5d5",
      "cca68107240c4fd18ae24eef080b349b",
      "a1ef1ba9efcc44388fe51b862ff00bd6",
      "f06a7eeb2b3b4d3e8946817f27687f69",
      "e36791e02bb14c6ba03102f10a05a78c",
      "837941b3367441498ba4960e3799da2f",
      "ac43280336eb455585e689fe82fd7ab0",
      "a5c45855c509473b9d87ad3d1b06b797",
      "312bc78c042449d2a9e30b7b32511e94",
      "19f8b381b3fb4cffa6246095033fc32e",
      "fa5c1595f5a64ceb8d5fcbbf6091f33e",
      "cf1019607119457cb8b385b575bfce00",
      "91cf0637f04547a0a94f6dcc7a371d66",
      "d4b46e5f7bf143f5b5eb234d38883e66",
      "2409e8f8360245c9ba0c61e95bbd9e9e",
      "abbfabff075d46f9afaf9cbb3a66a2be",
      "518325493bba4960ac22b59fba60c84a",
      "62f79e52dc3345b0b8f4612a08af6373",
      "423e05d2001c441290ad06efcbf5d53a",
      "ae31000c6a0341b6a3d241a0710c30c2"
     ]
    },
    "id": "NGUIX6spABRC",
    "outputId": "5ba141e9-c459-4041-ee73-caae5e3ec0d2"
   },
   "outputs": [],
   "source": [
    "class StanceDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Map stance labels to integers\n",
    "        self.label_map = {'Agree': 0, 'Disagree': 1, 'Discuss': 2, 'Unrelated': 3}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        claim = html.unescape(row['claim'])\n",
    "        article = html.unescape(row['article'])\n",
    "\n",
    "        # Truncate article if needed to fit within max_length\n",
    "        if len(article) > 5000:  # Arbitrary limit to avoid very long sequences\n",
    "            article = article[:5000]\n",
    "\n",
    "        # Combine claim and article\n",
    "        text = f\"Claim: {claim} Article: {article}\"\n",
    "\n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Remove batch dimension added by tokenizer\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "\n",
    "        # Add label\n",
    "        encoding['labels'] = torch.tensor(self.label_map[row['stance']])\n",
    "\n",
    "        return encoding\n",
    "\n",
    "def analyze_tokenization(pairs_df, test_pairs_df):\n",
    "    \"\"\"\n",
    "    Analyze tokenization for Gemma with 5 specific metrics:\n",
    "    - Token Count (Token Fertility)\n",
    "    - Token Length Distribution\n",
    "    - Compression Ratio\n",
    "    - Vocabulary Size\n",
    "    - OOV Rate\n",
    "\n",
    "    Uses the entire dataset.\n",
    "    \"\"\"\n",
    "    print(\"Loading dynamic tokenizer for tokenization analysis...\")\n",
    "    tokenizer = get_arabic_dynamic_tokenizer(\"google/gemma-7b\", use_fast=True, tokenizer_boundary=\"pretokens\")\n",
    "\n",
    "    # Use full dataset\n",
    "    combined_df = pd.concat([pairs_df, test_pairs_df])\n",
    "    print(f\"Analyzing tokenization on full dataset ({len(combined_df)} samples)...\")\n",
    "\n",
    "    # Initialize metrics\n",
    "    results = []\n",
    "    total_chars = 0\n",
    "    total_tokens = 0\n",
    "    token_lengths = []\n",
    "    tokens_per_char_values = []\n",
    "    oov_count = 0\n",
    "    unique_tokens = set()\n",
    "    vocabulary_size = len(tokenizer.get_vocab())\n",
    "    highly_fragmented_words = []\n",
    "    words_analyzed = 0\n",
    "\n",
    "    # Use tqdm for progress tracking with a reasonable chunk size\n",
    "    chunk_size = 100  # Process in chunks to avoid memory issues\n",
    "    total_chunks = (len(combined_df) + chunk_size - 1) // chunk_size\n",
    "\n",
    "    for chunk_idx in tqdm(range(total_chunks), desc=\"Analyzing tokenization\"):\n",
    "        chunk_start = chunk_idx * chunk_size\n",
    "        chunk_end = min((chunk_idx + 1) * chunk_size, len(combined_df))\n",
    "        chunk_df = combined_df.iloc[chunk_start:chunk_end]\n",
    "\n",
    "        for _, row in chunk_df.iterrows():\n",
    "            # Clean HTML entities\n",
    "            claim = html.unescape(row['claim'])\n",
    "            article = html.unescape(row['article'][:500])  # Still truncate article for speed\n",
    "\n",
    "            # Process claim\n",
    "            claim_chars = len(claim)\n",
    "            claim_tokens = tokenizer.encode(claim, add_special_tokens=False)\n",
    "            claim_tokens_texts = tokenizer.convert_ids_to_tokens(claim_tokens)\n",
    "\n",
    "            # Token fertility (tokens per character)\n",
    "            claim_fertility = len(claim_tokens) / claim_chars if claim_chars > 0 else 0\n",
    "            tokens_per_char_values.append(claim_fertility)\n",
    "\n",
    "            # Track token lengths for distribution\n",
    "            for token in claim_tokens_texts:\n",
    "                token_lengths.append(len(token))\n",
    "                unique_tokens.add(token)\n",
    "\n",
    "            # Update counts\n",
    "            total_chars += claim_chars\n",
    "            total_tokens += len(claim_tokens)\n",
    "\n",
    "            # Check word-level fragmentation (OOV estimation)\n",
    "            claim_words = claim.split()\n",
    "            for word in claim_words:\n",
    "                if len(word) >= 3:  # Only check non-trivial words\n",
    "                    words_analyzed += 1\n",
    "                    word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                    if len(word_tokens) >= 3:  # If a word is broken into 3+ tokens\n",
    "                        highly_fragmented_words.append(word)\n",
    "                        oov_count += 1\n",
    "\n",
    "            # Store results for this sample\n",
    "            results.append({\n",
    "                'text_length': len(claim) + len(article[:500]),\n",
    "                'gemma_tokens': len(claim_tokens) + len(tokenizer.encode(article[:500])),\n",
    "                'fertility': claim_fertility,\n",
    "                'stance': row['stance']\n",
    "            })\n",
    "\n",
    "    token_df = pd.DataFrame(results)\n",
    "\n",
    "    # 1. Token Fertility (tokens per character)\n",
    "    avg_token_fertility = np.mean(tokens_per_char_values)\n",
    "\n",
    "    # 2. Token Length Distribution\n",
    "    token_length_distribution = Counter(token_lengths)\n",
    "    avg_token_length = np.mean(token_lengths)\n",
    "    median_token_length = np.median(token_lengths)\n",
    "\n",
    "    # 3. Compression Ratio (characters per token)\n",
    "    compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "    # 4. Vocabulary Size\n",
    "    vocabulary_coverage = len(unique_tokens) / vocabulary_size\n",
    "\n",
    "    # 5. OOV Rate\n",
    "    oov_rate = oov_count / words_analyzed if words_analyzed > 0 else 0\n",
    "\n",
    "    # Display statistics\n",
    "    print(\"\\n===== Tokenization Analysis Results =====\")\n",
    "    print(f\"1. Token Fertility (tokens/char): {avg_token_fertility:.4f}\")\n",
    "    print(f\"2. Token Length: Mean={avg_token_length:.2f}, Median={median_token_length:.2f}\")\n",
    "    print(f\"3. Compression Ratio (chars/token): {compression_ratio:.4f}\")\n",
    "    print(f\"4. Vocabulary: Used {len(unique_tokens)} of {vocabulary_size} tokens ({vocabulary_coverage:.2%})\")\n",
    "    print(f\"5. OOV Rate: {oov_rate:.4f} ({oov_count}/{words_analyzed} words)\")\n",
    "\n",
    "    # Print some example highly fragmented words (potential OOVs)\n",
    "    if highly_fragmented_words:\n",
    "        print(\"\\nExample highly fragmented words (potential OOVs):\")\n",
    "        sample_oov = random.sample(highly_fragmented_words, min(10, len(highly_fragmented_words)))\n",
    "        for word in sample_oov:\n",
    "            tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "            token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
    "            print(f\"  '{word}' → {len(tokens)} tokens: {token_texts}\")\n",
    "\n",
    "    # Visualize tokenization\n",
    "    plt.figure(figsize=(16, 12))\n",
    "\n",
    "    # Plot 1: Token Count vs Text Length\n",
    "    plt.subplot(2, 2, 1)\n",
    "    # Sample for visualization if dataset is large\n",
    "    plot_df = token_df.sample(min(1000, len(token_df)), random_state=42) if len(token_df) > 1000 else token_df\n",
    "    plt.scatter(plot_df['text_length'], plot_df['gemma_tokens'], alpha=0.7)\n",
    "    plt.xlabel('Text Length (characters)')\n",
    "    plt.ylabel('Gemma Token Count')\n",
    "    plt.title('Tokenization Analysis: Gemma')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Token Fertility by Stance\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.boxplot(x='stance', y='fertility', data=token_df)\n",
    "    plt.title('Token Fertility by Stance')\n",
    "    plt.ylabel('Tokens per Character')\n",
    "\n",
    "    # Plot 3: Token Length Distribution\n",
    "    plt.subplot(2, 2, 3)\n",
    "    # For better visualization, limit to tokens with frequency > threshold if too many unique lengths\n",
    "    if len(token_length_distribution) > 30:\n",
    "        filtered_dist = {k: v for k, v in token_length_distribution.items()\n",
    "                         if v > max(token_length_distribution.values()) * 0.01}\n",
    "        plt.bar(filtered_dist.keys(), filtered_dist.values())\n",
    "        plt.title('Token Length Distribution (Filtered)')\n",
    "    else:\n",
    "        plt.bar(token_length_distribution.keys(), token_length_distribution.values())\n",
    "        plt.title('Token Length Distribution')\n",
    "    plt.xlabel('Token Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Plot 4: Compression Ratio Distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    compression_values = [1/f if f > 0 else 0 for f in token_df['fertility']]\n",
    "    plt.hist(compression_values, bins=20)\n",
    "    plt.xlabel('Compression Ratio (chars/token)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Compression Ratio Distribution')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tokenization_analysis.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Save results to file\n",
    "    tokenization_metrics = {\n",
    "        'token_fertility': avg_token_fertility,\n",
    "        'token_length_mean': avg_token_length,\n",
    "        'token_length_median': median_token_length,\n",
    "        'compression_ratio': compression_ratio,\n",
    "        'vocabulary_size': vocabulary_size,\n",
    "        'vocabulary_coverage': vocabulary_coverage,\n",
    "        'oov_rate': oov_rate,\n",
    "        'token_length_distribution': {str(k): v for k, v in token_length_distribution.items()}\n",
    "    }\n",
    "\n",
    "    with open('tokenization_metrics.json', 'w') as f:\n",
    "        json.dump(tokenization_metrics, f, indent=2)\n",
    "\n",
    "    return token_df\n",
    "\n",
    "# Enhanced Model Evaluation function with all requested metrics\n",
    "def evaluate_model_comprehensive(best_model, test_loader, device, label_names=['Agree', 'Disagree', 'Discuss', 'Unrelated']):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with comprehensive metrics:\n",
    "    - AUC, ROC curves\n",
    "    - Micro F1, Macro F1\n",
    "    - Precision, Recall (micro and macro)\n",
    "    - Accuracy\n",
    "    - Loss\n",
    "    \"\"\"\n",
    "    best_model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Use CrossEntropyLoss for loss calculation\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    test_start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating model\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = batch.pop('labels')\n",
    "\n",
    "            outputs = best_model(**batch)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Get predictions and probabilities\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().float().numpy())\n",
    "\n",
    "    test_time = time.time() - test_start_time\n",
    "\n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "\n",
    "    # Convert predictions and labels to numpy arrays\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.vstack(all_probs)\n",
    "\n",
    "    # 1. Accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    # 2-3. Precision (Micro and Macro)\n",
    "    precision_micro = precision_score(all_labels, all_preds, average='micro')\n",
    "    precision_macro = precision_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    # 4-5. Recall (Micro and Macro)\n",
    "    recall_micro = recall_score(all_labels, all_preds, average='micro')\n",
    "    recall_macro = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    # 6-7. F1 Score (Micro and Macro)\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # 8. AUC-ROC (One-vs-Rest for multiclass)\n",
    "    try:\n",
    "        # Convert labels to one-hot encoding for ROC AUC calculation\n",
    "        num_classes = len(np.unique(all_labels))\n",
    "        labels_one_hot = np.eye(num_classes)[all_labels]\n",
    "\n",
    "        # Calculate ROC AUC\n",
    "        roc_auc = roc_auc_score(labels_one_hot, all_probs, multi_class='ovr')\n",
    "\n",
    "        # Calculate ROC curves for plotting\n",
    "        fpr = {}\n",
    "        tpr = {}\n",
    "        roc_auc_per_class = {}\n",
    "\n",
    "        for i in range(num_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(labels_one_hot[:, i], all_probs[:, i])\n",
    "            roc_auc_per_class[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        # Plot ROC curves\n",
    "        plt.figure(figsize=(10, 8))\n",
    "\n",
    "        for i in range(num_classes):\n",
    "            plt.plot(\n",
    "                fpr[i],\n",
    "                tpr[i],\n",
    "                lw=2,\n",
    "                label=f'{label_names[i]} (AUC = {roc_auc_per_class[i]:.2f})'\n",
    "            )\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curves (One-vs-Rest, Overall AUC = {roc_auc:.2f})')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig('roc_curves.png')\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not calculate ROC AUC due to {str(e)}\")\n",
    "        roc_auc = None\n",
    "        roc_auc_per_class = None\n",
    "\n",
    "    # Class-wise metrics\n",
    "    class_metrics = {}\n",
    "    for i, label in enumerate(label_names):\n",
    "        class_precision = precision_score(\n",
    "            [1 if l == i else 0 for l in all_labels],\n",
    "            [1 if p == i else 0 for p in all_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "        class_recall = recall_score(\n",
    "            [1 if l == i else 0 for l in all_labels],\n",
    "            [1 if p == i else 0 for p in all_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "        class_f1 = f1_score(\n",
    "            [1 if l == i else 0 for l in all_labels],\n",
    "            [1 if p == i else 0 for p in all_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "\n",
    "        class_metrics[label] = {\n",
    "            'precision': class_precision,\n",
    "            'recall': class_recall,\n",
    "            'f1': class_f1\n",
    "        }\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nComprehensive Model Evaluation Results:\")\n",
    "    print(f\"  Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: Micro={precision_micro:.4f}, Macro={precision_macro:.4f}\")\n",
    "    print(f\"  Recall: Micro={recall_micro:.4f}, Macro={recall_macro:.4f}\")\n",
    "    print(f\"  F1 Score: Micro={f1_micro:.4f}, Macro={f1_macro:.4f}\")\n",
    "    if roc_auc is not None:\n",
    "        print(f\"  ROC AUC (OVR): {roc_auc:.4f}\")\n",
    "    print(f\"  Evaluation time: {test_time:.2f} seconds\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_micro': precision_micro,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_micro': recall_micro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'roc_auc': roc_auc,\n",
    "        'roc_auc_per_class': {label_names[i]: auc_val for i, auc_val in roc_auc_per_class.items()} if roc_auc_per_class else None,\n",
    "        'confusion_matrix': conf_matrix.tolist(),\n",
    "        'class_metrics': class_metrics,\n",
    "        'test_time': test_time\n",
    "    }\n",
    "\n",
    "    return results, all_preds, all_labels\n",
    "\n",
    "def train_model(model_name, pairs_df, test_pairs_df, output_dir=\"model_outputs\", use_lora=True, epochs=10):\n",
    "    \"\"\"Train and evaluate a stance detection model.\"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Configuration\n",
    "    num_labels = 4\n",
    "    batch_size = 1  # Small batch size due to model size\n",
    "    grad_accum_steps = 16  # Effective batch size = batch_size * grad_accum_steps\n",
    "    learning_rate = 2e-5\n",
    "    max_length = 512\n",
    "    model_save_path = os.path.join(output_dir, f\"{model_name.split('/')[-1]}_stance_detector\")\n",
    "\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Training {model_name} model for stance detection\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = get_arabic_dynamic_tokenizer(model_name)\n",
    "\n",
    "    # Fix for the padding token issue\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"Set EOS token as padding token\")\n",
    "\n",
    "    # Split training data into train and validation\n",
    "    print(\"Preparing datasets...\")\n",
    "    train_df, val_df = train_test_split(\n",
    "        pairs_df,\n",
    "        test_size=0.1,\n",
    "        random_state=42,\n",
    "        stratify=pairs_df['stance']\n",
    "    )\n",
    "\n",
    "    print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_pairs_df)}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = StanceDataset(train_df, tokenizer, max_length)\n",
    "    val_dataset = StanceDataset(val_df, tokenizer, max_length)\n",
    "    test_dataset = StanceDataset(test_pairs_df, tokenizer, max_length)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"Initializing model...\")\n",
    "    if use_lora:\n",
    "        # Load base model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.bfloat16  # Use bfloat16 to save memory\n",
    "        )\n",
    "\n",
    "        # Set padding token id in the model config\n",
    "        if model.config.pad_token_id is None:\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        # Define LoRA configuration\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            inference_mode=False,\n",
    "            r=16,  # rank\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # Include more attention modules\n",
    "        )\n",
    "\n",
    "        # Create PEFT model\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.bfloat16  # Use bfloat16 to save memory\n",
    "        )\n",
    "\n",
    "        # Set padding token id in the model config\n",
    "        if model.config.pad_token_id is None:\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Move model to GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_loader) * epochs // grad_accum_steps\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=total_steps // 10,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_f1s = []\n",
    "    best_val_f1 = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\")):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / grad_accum_steps  # Normalize loss for gradient accumulation\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            train_loss += loss.item() * grad_accum_steps\n",
    "\n",
    "            # Update weights after accumulating gradients\n",
    "            if (step + 1) % grad_accum_steps == 0 or step == len(train_loader) - 1:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        valid_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                # Don't pop labels here\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # For predictions, still get labels separately\n",
    "                labels = batch['labels']\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Avoid division by zero if all batches had None loss\n",
    "        if valid_batches > 0:\n",
    "            val_loss /= valid_batches\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Calculate metrics\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        val_precision = precision_score(all_labels, all_preds, average='macro')\n",
    "        val_recall = recall_score(all_labels, all_preds, average='macro')\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Accuracy: {val_acc:.4f}\")\n",
    "        print(f\"  Val F1 (macro): {val_f1:.4f}\")\n",
    "        print(f\"  Val Precision: {val_precision:.4f}\")\n",
    "        print(f\"  Val Recall: {val_recall:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            # Save model\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            model_to_save.save_pretrained(model_save_path)\n",
    "            tokenizer.save_pretrained(model_save_path)\n",
    "            print(f\"  Model saved to {model_save_path}\")\n",
    "\n",
    "        # Early stopping check (optional)\n",
    "        if epoch > 2 and val_losses[-1] > val_losses[-2] and val_losses[-2] > val_losses[-3]:\n",
    "            print(\"Early stopping triggered - validation loss increasing for 3 consecutive epochs\")\n",
    "            break\n",
    "\n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(val_losses, 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_f1s, 'g-', label='Validation F1')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Validation F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{model_name.split(\"/\")[-1]}_training_curve.png'))\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    if use_lora:\n",
    "        # For LoRA we need to load the PEFT model\n",
    "        config = PeftConfig.from_pretrained(model_save_path)\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            config.base_model_name_or_path,\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        # Set padding token id in the model config\n",
    "        if base_model.config.pad_token_id is None:\n",
    "            base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        best_model = PeftModel.from_pretrained(base_model, model_save_path)\n",
    "    else:\n",
    "        best_model = AutoModelForSequenceClassification.from_pretrained(model_save_path)\n",
    "        # Set padding token id in the model config\n",
    "        if best_model.config.pad_token_id is None:\n",
    "            best_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "\n",
    "    # Evaluate with comprehensive metrics\n",
    "    label_names = ['Agree', 'Disagree', 'Discuss', 'Unrelated']\n",
    "    eval_results, test_preds, test_labels = evaluate_model_comprehensive(\n",
    "        best_model,\n",
    "        test_loader,\n",
    "        device,\n",
    "        label_names\n",
    "    )\n",
    "\n",
    "    # Save results to file\n",
    "    with open(os.path.join(output_dir, f'{model_name.split(\"/\")[-1]}_results.json'), 'w') as f:\n",
    "        json.dump(eval_results, f, indent=2)\n",
    "\n",
    "    return eval_results\n",
    "\n",
    "# Main pipeline function\n",
    "def run_pipeline(pairs_df, test_pairs_df, run_tokenization=True, epochs=10):\n",
    "    \"\"\"Run the stance detection pipeline with Llama 3.1.\"\"\"\n",
    "    # Show dataset information\n",
    "    print(f\"Training set: {len(pairs_df)} claim-article pairs\")\n",
    "    print(\"Training stance distribution:\")\n",
    "    print(pairs_df['stance'].value_counts())\n",
    "    print(f\"\\nTest set: {len(test_pairs_df)} claim-article pairs\")\n",
    "    print(\"Test stance distribution:\")\n",
    "    print(test_pairs_df['stance'].value_counts())\n",
    "\n",
    "    # Analyze tokenization if requested\n",
    "    if run_tokenization:\n",
    "        # Enhanced tokenization analysis with all 5 metrics\n",
    "        print(\"\\n=== Running Enhanced Tokenization Analysis ===\")\n",
    "        token_df = analyze_tokenization(pairs_df, test_pairs_df)\n",
    "\n",
    "    # Train and evaluate Llama model\n",
    "    model_name = \"google/gemma-7b\"\n",
    "    results = train_model(model_name, pairs_df, test_pairs_df, epochs=epochs)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the complete pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # This will run the entire pipeline\n",
    "    results = run_pipeline(pairs_df, test_pairs_df, run_tokenization=True, epochs=10)\n",
    "\n",
    "    # Print summary of results\n",
    "    print(\"\\n=== Final Results Summary ===\")\n",
    "    print(f\"Model: google/gemma-7b\")\n",
    "    print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"F1 Score (macro): {results['f1_macro']:.4f}\")\n",
    "    print(f\"F1 Score (micro): {results['f1_micro']:.4f}\")\n",
    "    print(f\"Precision (macro): {results['precision_macro']:.4f}\")\n",
    "    print(f\"Recall (macro): {results['recall_macro']:.4f}\")\n",
    "    if results['roc_auc'] is not None:\n",
    "        print(f\"ROC AUC: {results['roc_auc']:.4f}\")\n",
    "\n",
    "    print(\"\\nClass-specific results:\")\n",
    "    for label, metrics in results['class_metrics'].items():\n",
    "        # Fix: Remove the extra space in 'precision ' key\n",
    "        print(f\"  {label}: F1={metrics['f1']:.4f}, Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}\")\n",
    "\n",
    "\n",
    "# Enhanced tokenization analysis function for Arabic with all five requested metrics\n",
    "def analyze_tokenization_fertility(pairs_df, test_pairs_df, sample_size=None):\n",
    "    \"\"\"\n",
    "    Analyze tokenization with all five specific metrics for Arabic text with Llama:\n",
    "    - Token Count (Token Fertility)\n",
    "    - Token Length Distribution\n",
    "    - Compression Ratio\n",
    "    - Vocabulary Size\n",
    "    - Out-of-Vocabulary (OOV) Rate\n",
    "    \"\"\"\n",
    "    print(\"Loading tokenizer for detailed tokenization analysis...\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = get_arabic_dynamic_tokenizer(\"meta-llama/Llama-3.1-8B\", use_fast=True, tokenizer_boundary=\"pretokens\")\n",
    "\n",
    "    # Sample data for analysis\n",
    "    combined_df = pd.concat([pairs_df, test_pairs_df])\n",
    "    sample_df = combined_df.sample(sample_size, random_state=42)\n",
    "\n",
    "    # Initialize metrics\n",
    "    results = []\n",
    "\n",
    "    # For vocabulary size metrics\n",
    "    vocabulary_size = len(tokenizer.get_vocab())\n",
    "    unique_tokens_used = set()\n",
    "\n",
    "    # For token length distribution\n",
    "    token_lengths = []\n",
    "\n",
    "    # For overall metrics\n",
    "    total_chars = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for _, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Analyzing token metrics\"):\n",
    "        # Clean text\n",
    "        claim = html.unescape(row['claim'])\n",
    "        article = html.unescape(row['article'][:500])  # Truncate for speed\n",
    "\n",
    "        # Tokenize\n",
    "        claim_tokens = tokenizer.encode(claim, add_special_tokens=False)\n",
    "        article_tokens = tokenizer.encode(article, add_special_tokens=False)\n",
    "\n",
    "        # Get token texts\n",
    "        claim_token_texts = tokenizer.convert_ids_to_tokens(claim_tokens)\n",
    "        article_token_texts = tokenizer.convert_ids_to_tokens(article_tokens)\n",
    "\n",
    "        # Add to unique tokens set\n",
    "        unique_tokens_used.update(claim_tokens + article_tokens)\n",
    "\n",
    "        # Track token lengths\n",
    "        for token in claim_token_texts + article_token_texts:\n",
    "            token_lengths.append(len(token))\n",
    "\n",
    "        # Update total counts\n",
    "        total_chars += len(claim) + len(article)\n",
    "        total_tokens += len(claim_tokens) + len(article_tokens)\n",
    "\n",
    "        # Calculate fertility (tokens per character)\n",
    "        claim_fertility = len(claim_tokens) / len(claim) if len(claim) > 0 else 0\n",
    "        article_fertility = len(article_tokens) / len(article) if len(article) > 0 else 0\n",
    "\n",
    "        # Calculate fragmentation (tokens per word) - for OOV estimation\n",
    "        claim_words = len(claim.split())\n",
    "        article_words = len(article.split())\n",
    "        claim_fragmentation = len(claim_tokens) / claim_words if claim_words > 0 else 0\n",
    "        article_fragmentation = len(article_tokens) / article_words if article_words > 0 else 0\n",
    "\n",
    "        # Estimate OOV by checking for multi-token words\n",
    "        # In Arabic, words broken into many tokens often indicate OOV issues\n",
    "        claim_words_list = claim.split()\n",
    "        article_words_list = article.split()\n",
    "\n",
    "        # Track highly fragmented words (potential OOVs)\n",
    "        highly_fragmented_words = []\n",
    "        for word in claim_words_list + article_words_list:\n",
    "            if len(word) >= 3:  # Only check non-trivial words\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                if len(word_tokens) >= 3:  # If a word is broken into 3+ tokens\n",
    "                    highly_fragmented_words.append(word)\n",
    "\n",
    "        results.append({\n",
    "            'claim_tokens': len(claim_tokens),\n",
    "            'article_tokens': len(article_tokens),\n",
    "            'claim_fertility': claim_fertility,\n",
    "            'article_fertility': article_fertility,\n",
    "            'claim_fragmentation': claim_fragmentation,\n",
    "            'article_fragmentation': article_fragmentation,\n",
    "            'highly_fragmented_words': len(highly_fragmented_words),\n",
    "            'highly_fragmented_word_examples': highly_fragmented_words[:5] if highly_fragmented_words else [],\n",
    "            'stance': row['stance']\n",
    "        })\n",
    "\n",
    "    token_df = pd.DataFrame(results)\n",
    "\n",
    "    # Calculate final metrics\n",
    "\n",
    "    # 1. Token Fertility (tokens per character)\n",
    "    avg_claim_fertility = token_df['claim_fertility'].mean()\n",
    "    avg_article_fertility = token_df['article_fertility'].mean()\n",
    "    avg_fertility = (avg_claim_fertility + avg_article_fertility) / 2\n",
    "\n",
    "    # 2. Token Length Distribution\n",
    "    token_length_dist = Counter(token_lengths)\n",
    "    avg_token_length = np.mean(token_lengths)\n",
    "\n",
    "    # 3. Compression Ratio (characters per token)\n",
    "    compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "    # 4. Vocabulary Size metrics\n",
    "    vocabulary_coverage = len(unique_tokens_used) / vocabulary_size\n",
    "\n",
    "    # 5. OOV Rate\n",
    "    avg_oov_count = token_df['highly_fragmented_words'].mean()\n",
    "    total_words = sum(len(row['claim'].split()) + len(row['article'][:500].split()) for _, row in sample_df.iterrows())\n",
    "    total_highly_fragmented = sum(row['highly_fragmented_words'] for _, row in token_df.iterrows())\n",
    "    oov_rate = total_highly_fragmented / total_words if total_words > 0 else 0\n",
    "\n",
    "    # Display statistics\n",
    "    print(\"\\n=== Tokenization Metrics Analysis ===\")\n",
    "    print(f\"1. Token Fertility (tokens/char):\")\n",
    "    print(f\"   - Claim: {avg_claim_fertility:.4f} tokens/char\")\n",
    "    print(f\"   - Article: {avg_article_fertility:.4f} tokens/char\")\n",
    "    print(f\"   - Overall: {avg_fertility:.4f} tokens/char\")\n",
    "\n",
    "    print(f\"\\n2. Token Length Distribution:\")\n",
    "    print(f\"   - Average token length: {avg_token_length:.2f} characters\")\n",
    "    print(f\"   - Median token length: {np.median(token_lengths):.2f} characters\")\n",
    "\n",
    "    print(f\"\\n3. Compression Ratio: {compression_ratio:.4f} chars/token\")\n",
    "\n",
    "    print(f\"\\n4. Vocabulary Size:\")\n",
    "    print(f\"   - Total vocabulary size: {vocabulary_size} tokens\")\n",
    "    print(f\"   - Unique tokens used: {len(unique_tokens_used)} tokens\")\n",
    "    print(f\"   - Vocabulary coverage: {vocabulary_coverage:.2%}\")\n",
    "\n",
    "    print(f\"\\n5. OOV Rate:\")\n",
    "    print(f\"   - Average highly fragmented words per sample: {avg_oov_count:.2f}\")\n",
    "    print(f\"   - OOV rate: {oov_rate:.2%} ({total_highly_fragmented}/{total_words} words)\")\n",
    "\n",
    "    # Print some example highly fragmented words (potential OOVs)\n",
    "    if sum(len(row) for row in token_df['highly_fragmented_word_examples']) > 0:\n",
    "        print(\"\\nExample highly fragmented words (potential OOVs):\")\n",
    "        all_examples = [w for row in token_df['highly_fragmented_word_examples'] for w in row]\n",
    "        for word in random.sample(all_examples, min(10, len(all_examples))):\n",
    "            tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "            token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
    "            print(f\"  '{word}' → {len(tokens)} tokens: {token_texts}\")\n",
    "\n",
    "    # Visualize fertility metrics\n",
    "    plt.figure(figsize=(16, 12))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.boxplot(x='stance', y='claim_fertility', data=token_df)\n",
    "    plt.title('Token Fertility (Claim) by Stance')\n",
    "    plt.ylabel('Tokens per Character')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.boxplot(x='stance', y='article_fertility', data=token_df)\n",
    "    plt.title('Token Fertility (Article) by Stance')\n",
    "    plt.ylabel('Tokens per Character')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.bar(token_length_dist.keys(), token_length_dist.values())\n",
    "    plt.xlabel('Token Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Token Length Distribution')\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.boxplot(x='stance', y='highly_fragmented_words', data=token_df)\n",
    "    plt.title('OOV Words by Stance')\n",
    "    plt.ylabel('Count of Highly Fragmented Words')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tokenization_metrics_analysis.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Save results to file\n",
    "    metrics_results = {\n",
    "        'token_fertility': {\n",
    "            'claim': avg_claim_fertility,\n",
    "            'article': avg_article_fertility,\n",
    "            'overall': avg_fertility\n",
    "        },\n",
    "        'token_length_distribution': {\n",
    "            'mean': avg_token_length,\n",
    "            'median': float(np.median(token_lengths)),\n",
    "            'distribution': {str(k): v for k, v in token_length_dist.items()}\n",
    "        },\n",
    "        'compression_ratio': compression_ratio,\n",
    "        'vocabulary_size': {\n",
    "            'total': vocabulary_size,\n",
    "            'used': len(unique_tokens_used),\n",
    "            'coverage': vocabulary_coverage\n",
    "        },\n",
    "        'oov_rate': {\n",
    "            'avg_highly_fragmented_per_sample': avg_oov_count,\n",
    "            'overall_rate': oov_rate\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open('tokenization_metrics_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics_results, f, indent=2)\n",
    "\n",
    "    return token_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7xtV-CNbndn"
   },
   "source": [
    "### Extend Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LpJDPOsbeynD",
    "outputId": "00d9e66a-1175-4bfd-9126-d2764a007a3b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "data_path = \"/content/drive/MyDrive/AraStance\"\n",
    "train_file = os.path.join(data_path, \"train.jsonl\")\n",
    "test_file = os.path.join(data_path, \"test.jsonl\")\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "df_train = load_jsonl(train_file)\n",
    "df_test = load_jsonl(test_file)\n",
    "\n",
    "df_train = pd.DataFrame(df_train)\n",
    "df_test = pd.DataFrame(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "n18Z-a2oe0uo",
    "outputId": "1487dcbc-f1ae-4b80-8e01-7cec2f9345e8"
   },
   "outputs": [],
   "source": [
    "import html\n",
    "import random\n",
    "\n",
    "pairs = []\n",
    "for i, row in df_train.iterrows():\n",
    "    claim = row['claim']\n",
    "    for j, (article, stance) in enumerate(zip(row['article'], row['stance'])):\n",
    "        article_title = row['article_title'][j] if isinstance(row['article_title'], list) else row['article_title']\n",
    "        pairs.append({\n",
    "            'claim': claim,\n",
    "            'article': article,\n",
    "            'article_title': article_title,\n",
    "            'stance': stance\n",
    "        })\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs)\n",
    "\n",
    "# Add these lines to see the output\n",
    "print(f\"Total claim-article pairs: {len(pairs_df)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(pairs_df.head())\n",
    "\n",
    "# Count the stance labels\n",
    "print(\"\\nStance distribution:\")\n",
    "print(pairs_df['stance'].value_counts())\n",
    "\n",
    "# Show one example of each stance\n",
    "print(\"\\nExamples of each stance:\")\n",
    "for stance in [\"Agree\", \"Disagree\", \"Discuss\", \"Unrelated\"]:\n",
    "    if stance in pairs_df['stance'].values:\n",
    "        example = pairs_df[pairs_df['stance'] == stance].iloc[0]\n",
    "        print(f\"\\n--- {stance} Example ---\")\n",
    "        print(f\"Claim: {example['claim'][:100]}...\")\n",
    "        print(f\"Article Title: {example['article_title']}\")\n",
    "\n",
    "# Explode the test dataset into claim-article pairs\n",
    "test_pairs = []\n",
    "for i, row in df_test.iterrows():\n",
    "    claim = row['claim']\n",
    "    for j, (article, stance) in enumerate(zip(row['article'], row['stance'])):\n",
    "        article_title = row['article_title'][j] if isinstance(row['article_title'], list) else row['article_title']\n",
    "        test_pairs.append({\n",
    "            'claim': claim,\n",
    "            'article': article,\n",
    "            'article_title': article_title,\n",
    "            'stance': stance\n",
    "        })\n",
    "\n",
    "test_pairs_df = pd.DataFrame(test_pairs)\n",
    "print(f\"Total claim-article pairs in test set: {len(test_pairs_df)}\")\n",
    "print(\"\\nStance distribution in test set:\")\n",
    "print(test_pairs_df['stance'].value_counts())\n",
    "\n",
    "# Clean HTML entities from a sample\n",
    "for stance in [\"Agree\", \"Disagree\", \"Discuss\", \"Unrelated\"]:\n",
    "    stance_examples = pairs_df[pairs_df['stance'] == stance]\n",
    "\n",
    "    if len(stance_examples) > 0:\n",
    "        random_index = random.randint(0, len(stance_examples) - 1)\n",
    "        example = stance_examples.iloc[random_index]\n",
    "\n",
    "        # Clean HTML entities\n",
    "        cleaned_title = html.unescape(example['article_title'])\n",
    "        cleaned_article = html.unescape(example['article'][:150])\n",
    "\n",
    "        print(f\"\\n--- Random {stance} Example (Cleaned) ---\")\n",
    "        print(f\"Claim: {example['claim']}\")\n",
    "        print(f\"Article Title: {cleaned_title}\")\n",
    "        print(f\"Article Excerpt: {cleaned_article}...\")\n",
    "        print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abQOpv0Ne4ns",
    "outputId": "458864ad-1da3-46aa-cc2f-d80a82b8a8d5"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade huggingface_hub\n",
    "!pip install transformers==4.49.0\n",
    "!pip install accelerate\n",
    "!pip install peft==0.5.0\n",
    "!pip install datasets\n",
    "!pip install bitsandbytes==0.38.2\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uigf3AA2e_mC",
    "outputId": "609b04c2-b916-473c-9bed-5feac286ba9a"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login\n",
    "\n",
    "# 3. Import libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, roc_curve, auc\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import html\n",
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    print(\"CUDA is available! Using GPU.\")\n",
    "else:\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IfDqJPKEgpp7",
    "outputId": "ca11259d-3529-44fa-cf44-141d8c380a07"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import html\n",
    "import os\n",
    "\n",
    "def find_inefficient_tokens(pairs_df, test_pairs_df, tokenizer, min_word_length=3, min_token_count=3):\n",
    "    \"\"\"Find words that are inefficiently tokenized.\"\"\"\n",
    "    print(\"Analyzing tokenization efficiency for Arabic words...\")\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_df = pd.concat([pairs_df, test_pairs_df])\n",
    "    word_data = []\n",
    "\n",
    "    for _, row in tqdm(combined_df.iterrows(), total=len(combined_df), desc=\"Analyzing words\"):\n",
    "        # Clean text and split into words\n",
    "        claim = html.unescape(row['claim'])\n",
    "        article = html.unescape(row['article'][:500])  # Limit article length for speed\n",
    "\n",
    "        for text in [claim, article]:\n",
    "            words = text.split()\n",
    "\n",
    "            # Analyze each word\n",
    "            for word in words:\n",
    "                if len(word) >= min_word_length:  # Only analyze substantial words\n",
    "                    # Skip words with non-Arabic characters\n",
    "                    if any(ord(c) < 1536 or ord(c) > 1791 for c in word):\n",
    "                        continue\n",
    "\n",
    "                    tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                    token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "                    if len(tokens) >= min_token_count:\n",
    "                        word_data.append({\n",
    "                            'word': word,\n",
    "                            'token_count': len(tokens),\n",
    "                            'tokens': token_texts,\n",
    "                            'chars_per_token': len(word) / len(tokens)\n",
    "                        })\n",
    "\n",
    "    # Count word frequencies\n",
    "    word_counts = {}\n",
    "    for item in word_data:\n",
    "        word = item['word']\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "\n",
    "    # Add frequency to each entry\n",
    "    for item in word_data:\n",
    "        item['frequency'] = word_counts[item['word']]\n",
    "\n",
    "    # Convert to DataFrame and remove duplicates\n",
    "    result_df = pd.DataFrame(word_data).drop_duplicates(subset=['word'])\n",
    "\n",
    "    # Sort by frequency and token count\n",
    "    result_df = result_df.sort_values(['frequency', 'token_count'], ascending=[False, False])\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Run the analysis\n",
    "print(\"Loading tokenizer for analysis...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\n",
    "\n",
    "# Find inefficient tokens\n",
    "inefficient_tokens_df = find_inefficient_tokens(pairs_df, test_pairs_df, tokenizer)\n",
    "\n",
    "# Display the top candidates\n",
    "print(f\"Found {len(inefficient_tokens_df)} inefficiently tokenized words\")\n",
    "print(\"\\nTop 20 candidates to add to vocabulary:\")\n",
    "for _, row in inefficient_tokens_df.head(20).iterrows():\n",
    "    print(f\"{row['word']} (freq: {row['frequency']}): {row['token_count']} tokens: {row['tokens']}\")\n",
    "\n",
    "# Save to CSV\n",
    "inefficient_tokens_df.to_csv(\"inefficient_arabic_tokens.csv\", index=False)\n",
    "print(\"Saved inefficient tokens to 'inefficient_arabic_tokens.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8a71516417434628ab679a70fb400782",
      "9dea22469fc648e5ae76376718e88a0c",
      "8d60396a4bd448e0ac35d6c8c05d97c1",
      "7b9771f9ca6a453d97d6b0fb9ed4af5a",
      "3cd95460aae84e9c9c331fa881c17e66",
      "6ee45a3fc8024a61bba690ee4a3e6bd2",
      "b4757d6825a24891936b4a1ea23fb805",
      "945ea045082f4be7bdc9a23ed7204c35",
      "9195154d618a4d8083163445656e7bf3",
      "3c888cc664494c80af5dac86497f1302",
      "58ef4189e6a94d9b956a498b409516f6",
      "90314b97984b4fa0bf5876ef23e36c4e",
      "018849184fc14cc187a7f426a6e9ff7a",
      "95ceab04f25645edad22ce9edf26baae",
      "70a232adb2e74f63acf45f6208f3160e",
      "e22e2f500f954b41bac5e61c72674364",
      "6fd9195c50514fa3910185d4bf4bdefb",
      "2ed4ea4677d045bfa6244ed7dea6b8bb",
      "bfbd5a3aa4d8449dbedab637dd92608a",
      "58ec5b00e0e045e19bd6c4765ed04d84",
      "3ed54307c66c4b2182226bcf0a5223e6",
      "2aaa8d9d9a104ccd866087c6b826cf54",
      "bb3b95931fed450ca2058b72d5a0965d",
      "0e9a76e8433449e8a76b76a34446ec92",
      "7086ce935f8a4152a448bcfa87e835a8",
      "ca73e9c0bc0e42b99152f4828cf3950c",
      "fffbe6e7c5fb414db86729f13c48cd10",
      "c5a3c76b6d0741c68463e1d2ca499c86",
      "9e34639bbb824faa8ab15a54fb33136b",
      "d8d98e90f1694e21ac910f57c8709388",
      "a9b04c293c1643ecb97a6c0d7f8fe3cc",
      "5391fb137aec43deaefa46ea03e96f77",
      "7d883ec6995741f5b52cc51d12c2e24d",
      "baf14bfe64584f259762f5fb719fc0bb",
      "db402e313d1041fe8ffb1f7dd99f646d",
      "e77ca58a0e5249199035d9c4627395c5",
      "0ddc592732d444d0ac111cc4880d60c1",
      "f88992721ed64bcabd116fbc35da1814",
      "d7d481ddbf184c24a888b248d2f5f624",
      "272989957d23429fbded766937b98113",
      "fee170de1d264e949bfaade5cd583207",
      "17c9bc9a2dd14550b29f6e3c5792ca76",
      "8b65af3179254d5197d88919aa2200fa",
      "103a84233e344f21bdcaa06ed0e35a5e",
      "48e264b5730142e58e3798922ee5d3ba",
      "7a7ba6ce21be49939420bb4de32d22c9",
      "cd75631af2aa4a279ae2b46095d3eb80",
      "b2e9551e707849f0b330385dc5d5230b",
      "5f8b7f3e8e9f4160bcf0dda6bdf158b5",
      "871c7ddea3074d1bbde8f8c1dd5355cb",
      "415a751fda1c41dfbcd035a83aae8045",
      "e462b4619c48433ca5883a722ec7be71",
      "b5808d9f6ef548cab0b7c38e9e907612",
      "06e93cc80911405d9795a9825f6df7df",
      "8a7ec314a690415e9636b20414c18c46",
      "0a4847f6198b45efb616534f43b981db",
      "e24878ee1bef49fe86ff98813113ed96",
      "627db2f67f17477cb600805ab6f739bf",
      "90d96e0d6288444a8c58dde05ad2c87e",
      "22ce5689f02748e48d4461e2f3766200",
      "02b7976a015e4f89afa2d4b491ef1ef5",
      "26456532766d4b04a18a6b88eb390e38",
      "a1a6310dedf340fdb35884db1d57b97a",
      "c01832ada9b24638b1bb5eed7f1f9fb4",
      "988d062f17d04a169256ca3a41532456",
      "2b966addfd2b4f4ba99024545798bc69",
      "ba81e0dac0464b5a98077f37310a5b2e",
      "86ad44b53ce14ea6a2e6c805b889f9e3",
      "f03255d0bb604cb9a938c931bfc71fbb",
      "c9ffc96b017f42b38b3964290e331ec7",
      "bb691f3471bf47f59afac0dbcff42178",
      "c9aee66a1f094a63b7af9fb892a517e6",
      "7efddc397da1415686497fb6afac8f2f",
      "8d9f3cdd554d4b0983e78991e1ac1c2d",
      "ab140944422a47fa9acdef838f6ab944",
      "83ed596b8f5d4cff958be6c9f5054dd5",
      "a8f212b522e54fa79fda33a938ac52b0",
      "ec24db0628404a9a93653a1a1e6fa6e7",
      "894006db7d394ebfaf1e0c6ea5670701",
      "a748b66a71194e9d8c67b19878625414",
      "1b9db91d992947388ef1346e04c0c127",
      "4081596497ac4761b3b972388ffb5fe9",
      "97d3288b16ad46d187fb55296bc941bc",
      "39a3da2cef1f4ab9be4eb2d9845b76cd",
      "2d218c04c9f94d238952abb9362ae6c6",
      "45ea88b652cc4c0aa55d2ce5283b77b6",
      "790ef253c1ce444ab93e6eb3d8492806",
      "c54de58985d54ba581f4742ea1027786"
     ]
    },
    "id": "CGdSGYMsFtpS",
    "outputId": "6771fa04-a478-4de0-dadb-ea8421b1ff46"
   },
   "outputs": [],
   "source": [
    "def fix_gemma_tokenizer_for_arabic():\n",
    "    # Load original tokenizer and model\n",
    "    print(\"Loading original tokenizer and model...\")\n",
    "    model_name = \"google/gemma-7b\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Load inefficient tokens from our analysis\n",
    "    print(\"Loading inefficient tokens...\")\n",
    "    try:\n",
    "        inefficient_tokens_df = pd.read_csv(\"inefficient_arabic_tokens.csv\")\n",
    "    except:\n",
    "        print(\"File not found. Please run the analysis first.\")\n",
    "        return\n",
    "\n",
    "    # Step 1: Prepare tokens with and without spaces\n",
    "    space_aware_tokens = []\n",
    "\n",
    "    for _, row in tqdm(inefficient_tokens_df.head(500).iterrows(), desc=\"Preparing tokens\"):\n",
    "        word = row['word']\n",
    "\n",
    "        # Add versions with and without space prefixes\n",
    "        space_aware_tokens.append(word)  # No space\n",
    "        space_aware_tokens.append(\" \" + word)  # With space\n",
    "\n",
    "        # Also add with the tokenizer's actual space representation\n",
    "        space_aware_tokens.append(\"▁\" + word)  # With Gemma-style space\n",
    "\n",
    "    # Remove duplicates\n",
    "    space_aware_tokens = list(set(space_aware_tokens))\n",
    "\n",
    "    # Also add specific high-frequency multi-token sequences from the inefficient analysis\n",
    "    # These are directly from the tokens analysis\n",
    "    for _, row in inefficient_tokens_df.head(500).iterrows():\n",
    "        token_texts = eval(row['tokens'])  # Convert string representation to list\n",
    "        if len(token_texts) >= 3:\n",
    "            # Try to reconstruct different variations of how this might be tokenized\n",
    "            joined = ''.join(token_texts).replace('▁', ' ').strip()\n",
    "            if joined not in space_aware_tokens:\n",
    "                space_aware_tokens.append(joined)\n",
    "\n",
    "            # Add version with Gemma-style space\n",
    "            if not joined.startswith(' '):\n",
    "                space_aware_tokens.append(\"▁\" + joined)\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=4,\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    # Fix padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    if model.config.pad_token_id is None:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Print original size\n",
    "    print(f\"Original vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "    # Add tokens with detailed tracking\n",
    "    orig_vocab_size = len(tokenizer)\n",
    "\n",
    "    # Add tokens and verify which ones actually get added\n",
    "    new_tokens = []\n",
    "    for token in tqdm(space_aware_tokens, desc=\"Verifying tokens\"):\n",
    "        # Skip very short tokens (length 1-2) as they're likely already in the vocab\n",
    "        if len(token.strip()) <= 2:\n",
    "            continue\n",
    "\n",
    "        # Check if this exact token exists with this exact spelling\n",
    "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "        if token_id == tokenizer.unk_token_id:\n",
    "            new_tokens.append(token)\n",
    "\n",
    "    print(f\"Adding {len(new_tokens)} verified new tokens...\")\n",
    "    num_added = tokenizer.add_tokens(new_tokens)\n",
    "    print(f\"Successfully added {num_added} new tokens\")\n",
    "\n",
    "    # Resize embeddings\n",
    "    output_dir = \"gemma-7b-arabic-optimized\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Resize the model's embedding matrix\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"New vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "    # Save the extended tokenizer and model\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    model.save_pretrained(output_dir)\n",
    "    print(f\"Saved to {output_dir}\")\n",
    "\n",
    "    # Test the improvement\n",
    "    test_texts = [\n",
    "        \"نائب وزير الخارجية الإيراني يشكك في سقوط نظام الرئيس السوري\",\n",
    "        \"التلفاز المصري :احالة الإعلامي أحمد موسى إلى نيابة أمن الدولة\",\n",
    "        \"الخوف يتملك المدنيين في سوريا مع اقتراب الحرب من المناطق المكتظة\",\n",
    "        \"وفاة وزير الإعلام المصري الأسبق منصور حسن\"\n",
    "    ]\n",
    "\n",
    "    # Load the original tokenizer for comparison\n",
    "    original_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Test each text\n",
    "    print(\"\\nTesting optimized tokenizer:\")\n",
    "    for text in test_texts:\n",
    "        orig_tokens = original_tokenizer.encode(text, add_special_tokens=False)\n",
    "        new_tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "        # Convert to readable form\n",
    "        orig_token_texts = original_tokenizer.convert_ids_to_tokens(orig_tokens)\n",
    "        new_token_texts = tokenizer.convert_ids_to_tokens(new_tokens)\n",
    "\n",
    "        # Print comparative results\n",
    "        print(f\"\\nText: {text[:40]}...\")\n",
    "        print(f\"Original: {len(orig_tokens)} tokens\")\n",
    "        print(f\"First few tokens: {orig_token_texts[:10]}\")\n",
    "        print(f\"Optimized: {len(new_tokens)} tokens\")\n",
    "        print(f\"First few tokens: {new_token_texts[:10]}\")\n",
    "\n",
    "        diff = len(orig_tokens) - len(new_tokens)\n",
    "        percent = (diff / len(orig_tokens)) * 100 if len(orig_tokens) > 0 else 0\n",
    "        print(f\"Change: {diff} tokens ({percent:.1f}%)\")\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "# Run the optimization\n",
    "optimized_tokenizer, optimized_model = fix_gemma_tokenizer_for_arabic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lGyVd5TRiONn",
    "outputId": "e5712c63-1dfb-43f9-9186-6a8a9241b934"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import html\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_tokenization_metrics(tokenizer_path, pairs_df, test_pairs_df):\n",
    "    \"\"\"\n",
    "    Analyze tokenization fertility metrics for the given tokenizer\n",
    "    \"\"\"\n",
    "    print(f\"Loading tokenizer from {tokenizer_path}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "    # Combine datasets for comprehensive analysis\n",
    "    combined_df = pd.concat([pairs_df, test_pairs_df])\n",
    "    total_samples = len(combined_df)\n",
    "    print(f\"Analyzing {total_samples} samples...\")\n",
    "\n",
    "    # Track metrics\n",
    "    total_chars = 0\n",
    "    total_tokens = 0\n",
    "    tokens_per_char_values = []\n",
    "    token_lengths = []\n",
    "    unique_tokens = set()\n",
    "    vocabulary_size = len(tokenizer.get_vocab())\n",
    "    oov_count = 0\n",
    "    words_analyzed = 0\n",
    "\n",
    "    # Process each sample\n",
    "    for _, row in tqdm(combined_df.iterrows(), total=total_samples, desc=\"Analyzing samples\"):\n",
    "        # Clean text\n",
    "        claim = html.unescape(row['claim'])\n",
    "        article = html.unescape(row['article'][:500])  # Truncate for speed\n",
    "\n",
    "        # Process claim and article\n",
    "        for text in [claim, article]:\n",
    "            # Tokenize\n",
    "            text_tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "            text_token_texts = tokenizer.convert_ids_to_tokens(text_tokens)\n",
    "\n",
    "            # Update metrics\n",
    "            chars = len(text)\n",
    "            total_chars += chars\n",
    "            total_tokens += len(text_tokens)\n",
    "\n",
    "            # Calculate fertility (tokens per character)\n",
    "            if chars > 0:\n",
    "                tokens_per_char = len(text_tokens) / chars\n",
    "                tokens_per_char_values.append(tokens_per_char)\n",
    "\n",
    "            # Track token lengths for distribution\n",
    "            for token in text_token_texts:\n",
    "                token_lengths.append(len(token))\n",
    "                unique_tokens.add(token)\n",
    "\n",
    "            # Check word-level fragmentation (OOV estimation)\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                if len(word) >= 3:  # Only check non-trivial words\n",
    "                    words_analyzed += 1\n",
    "                    word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                    if len(word_tokens) >= 3:  # If a word is broken into 3+ tokens\n",
    "                        oov_count += 1\n",
    "\n",
    "    # Calculate final metrics\n",
    "    avg_tokens_per_char = np.mean(tokens_per_char_values)\n",
    "    avg_token_length = np.mean(token_lengths)\n",
    "    median_token_length = np.median(token_lengths)\n",
    "    compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0\n",
    "    vocabulary_coverage = len(unique_tokens) / vocabulary_size\n",
    "    oov_rate = oov_count / words_analyzed if words_analyzed > 0 else 0\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\n===== Full Dataset Tokenization Analysis Results =====\")\n",
    "    print(f\"Total samples analyzed: {total_samples}\")\n",
    "    print(f\"1. Token Fertility (tokens/char): {avg_tokens_per_char:.4f}\")\n",
    "    print(f\"2. Token Length: Mean={avg_token_length:.2f}, Median={median_token_length:.2f}\")\n",
    "    print(f\"3. Compression Ratio (chars/token): {compression_ratio:.4f}\")\n",
    "    print(f\"4. Vocabulary: Used {len(unique_tokens)} of {vocabulary_size} tokens ({vocabulary_coverage:.2%})\")\n",
    "    print(f\"5. OOV Rate: {oov_rate:.4f} ({oov_count}/{words_analyzed} words)\")\n",
    "\n",
    "    # Return metrics as a dictionary for comparison\n",
    "    return {\n",
    "        \"tokenizer\": tokenizer_path,\n",
    "        \"total_samples\": total_samples,\n",
    "        \"token_fertility\": avg_tokens_per_char,\n",
    "        \"token_length_mean\": avg_token_length,\n",
    "        \"token_length_median\": median_token_length,\n",
    "        \"compression_ratio\": compression_ratio,\n",
    "        \"vocabulary_size\": vocabulary_size,\n",
    "        \"vocabulary_used\": len(unique_tokens),\n",
    "        \"vocabulary_coverage\": vocabulary_coverage,\n",
    "        \"oov_rate\": oov_rate,\n",
    "        \"oov_count\": oov_count,\n",
    "        \"words_analyzed\": words_analyzed\n",
    "    }\n",
    "\n",
    "# Run the analysis for your optimized tokenizer\n",
    "metrics = analyze_tokenization_metrics(\"gemma-7b-arabic-optimized\", pairs_df, test_pairs_df)\n",
    "\n",
    "# Optionally save the metrics to a file\n",
    "import json\n",
    "with open(\"optimized_tokenizer_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"\\nAnalysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JadeoATsfvao"
   },
   "outputs": [],
   "source": [
    "class StanceDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Map stance labels to integers\n",
    "        self.label_map = {'Agree': 0, 'Disagree': 1, 'Discuss': 2, 'Unrelated': 3}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        claim = html.unescape(row['claim'])\n",
    "        article = html.unescape(row['article'])\n",
    "\n",
    "        # Truncate article if needed to fit within max_length\n",
    "        if len(article) > 5000:  # Arbitrary limit to avoid very long sequences\n",
    "            article = article[:5000]\n",
    "\n",
    "        # Combine claim and article\n",
    "        text = f\"Claim: {claim} Article: {article}\"\n",
    "\n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Remove batch dimension added by tokenizer\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "\n",
    "        # Add label\n",
    "        encoding['labels'] = torch.tensor(self.label_map[row['stance']])\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8e8AtkKf4LM"
   },
   "outputs": [],
   "source": [
    "def evaluate_model_comprehensive(best_model, test_loader, device, label_names=['Agree', 'Disagree', 'Discuss', 'Unrelated']):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with comprehensive metrics:\n",
    "    - AUC, ROC curves\n",
    "    - Micro F1, Macro F1\n",
    "    - Precision, Recall (micro and macro)\n",
    "    - Accuracy\n",
    "    - Loss\n",
    "    \"\"\"\n",
    "    best_model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Use CrossEntropyLoss for loss calculation\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    test_start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating model\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = batch.pop('labels')\n",
    "\n",
    "            outputs = best_model(**batch)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Get predictions and probabilities\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().float().numpy())\n",
    "\n",
    "    test_time = time.time() - test_start_time\n",
    "\n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "\n",
    "    # Convert predictions and labels to numpy arrays\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.vstack(all_probs)\n",
    "\n",
    "    # 1. Accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    # 2-3. Precision (Micro and Macro)\n",
    "    precision_micro = precision_score(all_labels, all_preds, average='micro')\n",
    "    precision_macro = precision_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    # 4-5. Recall (Micro and Macro)\n",
    "    recall_micro = recall_score(all_labels, all_preds, average='micro')\n",
    "    recall_macro = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    # 6-7. F1 Score (Micro and Macro)\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # 8. AUC-ROC (One-vs-Rest for multiclass)\n",
    "    try:\n",
    "        # Convert labels to one-hot encoding for ROC AUC calculation\n",
    "        num_classes = len(np.unique(all_labels))\n",
    "        labels_one_hot = np.eye(num_classes)[all_labels]\n",
    "\n",
    "        # Calculate ROC AUC\n",
    "        roc_auc = roc_auc_score(labels_one_hot, all_probs, multi_class='ovr')\n",
    "\n",
    "        # Calculate ROC curves for plotting\n",
    "        fpr = {}\n",
    "        tpr = {}\n",
    "        roc_auc_per_class = {}\n",
    "\n",
    "        for i in range(num_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(labels_one_hot[:, i], all_probs[:, i])\n",
    "            roc_auc_per_class[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        # Plot ROC curves\n",
    "        plt.figure(figsize=(10, 8))\n",
    "\n",
    "        for i in range(num_classes):\n",
    "            plt.plot(\n",
    "                fpr[i],\n",
    "                tpr[i],\n",
    "                lw=2,\n",
    "                label=f'{label_names[i]} (AUC = {roc_auc_per_class[i]:.2f})'\n",
    "            )\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curves (One-vs-Rest, Overall AUC = {roc_auc:.2f})')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig('roc_curves.png')\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not calculate ROC AUC due to {str(e)}\")\n",
    "        roc_auc = None\n",
    "        roc_auc_per_class = None\n",
    "\n",
    "    # Class-wise metrics\n",
    "    class_metrics = {}\n",
    "    for i, label in enumerate(label_names):\n",
    "        class_precision = precision_score(\n",
    "            [1 if l == i else 0 for l in all_labels],\n",
    "            [1 if p == i else 0 for p in all_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "        class_recall = recall_score(\n",
    "            [1 if l == i else 0 for l in all_labels],\n",
    "            [1 if p == i else 0 for p in all_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "        class_f1 = f1_score(\n",
    "            [1 if l == i else 0 for l in all_labels],\n",
    "            [1 if p == i else 0 for p in all_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "\n",
    "        class_metrics[label] = {\n",
    "            'precision': class_precision,\n",
    "            'recall': class_recall,\n",
    "            'f1': class_f1\n",
    "        }\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nComprehensive Model Evaluation Results:\")\n",
    "    print(f\"  Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: Micro={precision_micro:.4f}, Macro={precision_macro:.4f}\")\n",
    "    print(f\"  Recall: Micro={recall_micro:.4f}, Macro={recall_macro:.4f}\")\n",
    "    print(f\"  F1 Score: Micro={f1_micro:.4f}, Macro={f1_macro:.4f}\")\n",
    "    if roc_auc is not None:\n",
    "        print(f\"  ROC AUC (OVR): {roc_auc:.4f}\")\n",
    "    print(f\"  Evaluation time: {test_time:.2f} seconds\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_micro': precision_micro,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_micro': recall_micro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'roc_auc': roc_auc,\n",
    "        'roc_auc_per_class': {label_names[i]: auc_val for i, auc_val in roc_auc_per_class.items()} if roc_auc_per_class else None,\n",
    "        'confusion_matrix': conf_matrix.tolist(),\n",
    "        'class_metrics': class_metrics,\n",
    "        'test_time': test_time\n",
    "    }\n",
    "\n",
    "    return results, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6290a9f71c6f4e21a84dda8def4d9407",
      "4947a99872634fe9b7232fff4db0e243",
      "e0043d7828db4c3184425e6a89225ede",
      "dd878bb904ac4f4396031069abfa5d8e",
      "f050669fe883409ca015a7c4d3fdb828",
      "ff9e87cae8554248b596e70f232703f0",
      "01e5fd7f1855490888826fb7cdaa245e",
      "4df9f88c127246e4a432fa309fc31401",
      "2be268a865df4c42bd938c8cd731cc35",
      "107e322e5fc44da4be91f4133857a9a3",
      "f5cc034b84054495a0514e825fe94bf0",
      "53222d12156544e5a3a786f3198ca3ec",
      "620cf79deafc4d4992468e1f67c7172b",
      "5acf4a0d11524af69870fc80f4f0270a",
      "ffec78a9e5cf47cdbf9821d08c7ee02e",
      "296872540cb34cd481c5d44d0d6c3563",
      "c39eb70c6c694cfa879f765904339c2d",
      "d0a28a396abe4f8381c671a69263dec8",
      "fee9854272c5492dbfb5cd5f7e82d98f",
      "e1598fe182ae40eaaeed05c171e6ca0a",
      "c914c9efc43c46048b8a6cc472640030",
      "b191a23ae6ae4360aece30686504046b"
     ]
    },
    "id": "zILJW-Y_f5st",
    "outputId": "86595745-b2b0-4d5e-a870-173673a025cb"
   },
   "outputs": [],
   "source": [
    "def train_model(pairs_df, test_pairs_df, output_dir=\"optimized_model_outputs\", epochs=10):\n",
    "    \"\"\"Train and evaluate a stance detection model with optimized Arabic tokenizer.\"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Configuration\n",
    "    num_labels = 4\n",
    "    batch_size = 1  # Small batch size due to model size\n",
    "    grad_accum_steps = 16  # Effective batch size = batch_size * grad_accum_steps\n",
    "    learning_rate = 2e-5\n",
    "    max_length = 512\n",
    "    model_save_path = os.path.join(output_dir, \"gemma_arabic_stance_detector\")\n",
    "\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Training Gemma model with optimized Arabic tokenizer\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    # Load optimized tokenizer\n",
    "    print(\"Loading optimized tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gemma-7b-arabic-optimized\")\n",
    "\n",
    "    # Fix for the padding token issue\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"Set EOS token as padding token\")\n",
    "\n",
    "    # Split training data into train and validation\n",
    "    print(\"Preparing datasets...\")\n",
    "    train_df, val_df = train_test_split(\n",
    "        pairs_df,\n",
    "        test_size=0.1,\n",
    "        random_state=42,\n",
    "        stratify=pairs_df['stance']\n",
    "    )\n",
    "\n",
    "    print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_pairs_df)}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = StanceDataset(train_df, tokenizer, max_length)\n",
    "    val_dataset = StanceDataset(val_df, tokenizer, max_length)\n",
    "    test_dataset = StanceDataset(test_pairs_df, tokenizer, max_length)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize model with optimized tokenizer\n",
    "    print(\"Initializing model with optimized tokenizer...\")\n",
    "    # Load base model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"gemma-7b-arabic-optimized\",\n",
    "        num_labels=num_labels,\n",
    "        torch_dtype=torch.bfloat16  # Use bfloat16 to save memory\n",
    "    )\n",
    "\n",
    "    # Set padding token id in the model config\n",
    "    if model.config.pad_token_id is None:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Define LoRA configuration\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False,\n",
    "        r=16,  # rank\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # Include more attention modules\n",
    "    )\n",
    "\n",
    "    # Create PEFT model\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # Move model to GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_loader) * epochs // grad_accum_steps\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=total_steps // 10,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_f1s = []\n",
    "    best_val_f1 = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\")):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / grad_accum_steps  # Normalize loss for gradient accumulation\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            train_loss += loss.item() * grad_accum_steps\n",
    "\n",
    "            # Update weights after accumulating gradients\n",
    "            if (step + 1) % grad_accum_steps == 0 or step == len(train_loader) - 1:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        valid_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                labels = batch.pop('labels')\n",
    "\n",
    "                outputs = model(**batch)\n",
    "                # Handle the case where loss is None\n",
    "                if outputs.loss is not None:\n",
    "                    loss = outputs.loss\n",
    "                    val_loss += loss.item()\n",
    "                    valid_batches += 1\n",
    "                else:\n",
    "                    # If loss is None, skip this batch for loss calculation\n",
    "                    pass\n",
    "\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Avoid division by zero if all batches had None loss\n",
    "        if valid_batches > 0:\n",
    "            val_loss /= valid_batches\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Calculate metrics\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        val_precision = precision_score(all_labels, all_preds, average='macro')\n",
    "        val_recall = recall_score(all_labels, all_preds, average='macro')\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Accuracy: {val_acc:.4f}\")\n",
    "        print(f\"  Val F1 (macro): {val_f1:.4f}\")\n",
    "        print(f\"  Val Precision: {val_precision:.4f}\")\n",
    "        print(f\"  Val Recall: {val_recall:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            # Save model\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            model_to_save.save_pretrained(model_save_path)\n",
    "            tokenizer.save_pretrained(model_save_path)\n",
    "            print(f\"  Model saved to {model_save_path}\")\n",
    "\n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(val_losses, 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_f1s, 'g-', label='Validation F1')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Validation F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'training_curve.png'))\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    # Load the best model\n",
    "    config = PeftConfig.from_pretrained(model_save_path)\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"gemma-7b-arabic-optimized\",\n",
    "        num_labels=num_labels,\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    # Set padding token id in the model config\n",
    "    if base_model.config.pad_token_id is None:\n",
    "        base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    best_model = PeftModel.from_pretrained(base_model, model_save_path)\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "\n",
    "    # Evaluate with comprehensive metrics\n",
    "    label_names = ['Agree', 'Disagree', 'Discuss', 'Unrelated']\n",
    "    eval_results, test_preds, test_labels = evaluate_model_comprehensive(\n",
    "        best_model,\n",
    "        test_loader,\n",
    "        device,\n",
    "        label_names\n",
    "    )\n",
    "\n",
    "    # Save results to file\n",
    "    with open(os.path.join(output_dir, 'results.json'), 'w') as f:\n",
    "        json.dump(eval_results, f, indent=2)\n",
    "\n",
    "    return eval_results\n",
    "\n",
    "results = train_model(pairs_df, test_pairs_df, epochs=10)\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\n=== Final Results Summary ===\")\n",
    "print(f\"Model: Gemma-7B with optimized Arabic tokenizer\")\n",
    "print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "print(f\"F1 Score (macro): {results['f1_macro']:.4f}\")\n",
    "print(f\"F1 Score (micro): {results['f1_micro']:.4f}\")\n",
    "print(f\"Precision (macro): {results['precision_macro']:.4f}\")\n",
    "print(f\"Recall (macro): {results['recall_macro']:.4f}\")\n",
    "if results['roc_auc'] is not None:\n",
    "    print(f\"ROC AUC: {results['roc_auc']:.4f}\")\n",
    "\n",
    "# Class-specific results\n",
    "print(\"\\nClass-specific results:\")\n",
    "for label, metrics in results['class_metrics'].items():\n",
    "    print(f\"  {label}: F1={metrics['f1']:.4f}, Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
