{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9WCYLo914AM"
   },
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5jZKAP40NYx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CiN55Oma0Q-S",
    "outputId": "38835a40-2a86-4dd0-ffa1-ad32f287b786"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3ahqPze0azt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "data_path = \"/content/drive/MyDrive/AraStance\"\n",
    "train_file = os.path.join(data_path, \"train.jsonl\")\n",
    "test_file = os.path.join(data_path, \"test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXPWqVHx0kHV"
   },
   "outputs": [],
   "source": [
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "df_train = load_jsonl(train_file)\n",
    "df_test = load_jsonl(test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "DKM1tp8h1OOq",
    "outputId": "6ac9de6b-2d11-40d1-9fb3-96f2b78a13f8"
   },
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(df_train)\n",
    "df_test = pd.DataFrame(df_test)\n",
    "\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VBf_ZUQ61kJx",
    "outputId": "78ab3a44-030d-4309-ebc8-d41e24881469"
   },
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "ka7kIBRl2H4S",
    "outputId": "e5bf7616-ea74-4303-87dd-fee4f4fb9898"
   },
   "outputs": [],
   "source": [
    "df_train['stance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vc1c0sVx2NYu",
    "outputId": "cabd9919-04b1-49d0-e9ab-8d9faa4def7d"
   },
   "outputs": [],
   "source": [
    "# First, let's check if the stance values are actually lists\n",
    "print(type(df_train['stance'].iloc[0]))\n",
    "\n",
    "# Check what's in the article column too\n",
    "print(type(df_train['article'].iloc[0]))\n",
    "\n",
    "# Let's look at a complete example\n",
    "example = df_train.iloc[0]\n",
    "print(f\"Claim: {example['claim']}\")\n",
    "print(f\"Stance values: {example['stance']}\")\n",
    "print(f\"Number of articles: {len(example['article']) if isinstance(example['article'], list) else 1}\")\n",
    "\n",
    "# If you want to extract pairs where the stance is \"Agree\"\n",
    "for i, row in df_train.iterrows():\n",
    "    stances = row['stance']\n",
    "    if not isinstance(stances, list):\n",
    "        stances = [stances]  # Handle case where it's not a list\n",
    "\n",
    "    if \"Agree\" in stances:\n",
    "        print(f\"\\n--- Example with 'Agree' stance ---\")\n",
    "        print(f\"Claim: {row['claim'][:100]}...\")\n",
    "\n",
    "        # Find which article(s) have \"Agree\" stance\n",
    "        if isinstance(row['article'], list) and len(row['article']) == len(stances):\n",
    "            for j, (article, stance) in enumerate(zip(row['article'], stances)):\n",
    "                if stance == \"Agree\":\n",
    "                    print(f\"Article {j+1} (Stance: {stance}): {row['article_title'][j] if isinstance(row['article_title'], list) else row['article_title']}\")\n",
    "        break  # Just show one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jR2lQBI2tSx",
    "outputId": "22f3c748-9dc9-4c1a-922f-998311721d35"
   },
   "outputs": [],
   "source": [
    "all_stances = []\n",
    "for stance_list in df_train['stance']:\n",
    "    all_stances.extend(stance_list)\n",
    "\n",
    "stance_counts = pd.Series(all_stances).value_counts()\n",
    "print(stance_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73sld6H94mdX",
    "outputId": "c70a27b3-ce74-429a-850f-0ca4e5805ce1"
   },
   "outputs": [],
   "source": [
    "# Count how many articles are paired with each claim\n",
    "claim_article_counts = pairs_df.groupby('claim').size()\n",
    "print(claim_article_counts.describe())\n",
    "print(\"\\nSample of claims with different numbers of paired articles:\")\n",
    "print(claim_article_counts.value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aHeOdJ3953Bu",
    "outputId": "bcdb985a-1496-4ae1-dcd4-663b66e553fd"
   },
   "outputs": [],
   "source": [
    "# Get a random claim-article pair with its stance\n",
    "import random\n",
    "\n",
    "# Select a random index\n",
    "random_index = random.randint(0, len(pairs_df) - 1)\n",
    "random_pair = pairs_df.iloc[random_index]\n",
    "\n",
    "# Display the information\n",
    "print(f\"Claim: {random_pair['claim']}\")\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "print(f\"Article Title: {random_pair['article_title']}\")\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "print(f\"Article Excerpt (first 300 chars): {random_pair['article'][:300]}...\")\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "print(f\"Stance: {random_pair['stance']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WiAWr9qv6PCI",
    "outputId": "7a51cabb-c1b0-4c38-895f-d318afb3e76e"
   },
   "outputs": [],
   "source": [
    "# Show a random example for each stance\n",
    "for stance in [\"Agree\", \"Disagree\", \"Discuss\", \"Unrelated\"]:\n",
    "    stance_examples = pairs_df[pairs_df['stance'] == stance]\n",
    "\n",
    "    if len(stance_examples) > 0:\n",
    "        random_index = random.randint(0, len(stance_examples) - 1)\n",
    "        example = stance_examples.iloc[random_index]\n",
    "\n",
    "        print(f\"\\n--- Random {stance} Example ---\")\n",
    "        print(f\"Claim: {example['claim']}\")\n",
    "        print(f\"Article Title: {example['article_title']}\")\n",
    "        print(f\"Article Excerpt: {example['article'][:150]}...\")\n",
    "        print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eGpQaHT4vTl"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hV8L0-i64xD1",
    "outputId": "5457097d-513f-456d-c401-83d230630527"
   },
   "outputs": [],
   "source": [
    "import html\n",
    "import random\n",
    "\n",
    "pairs = []\n",
    "for i, row in df_train.iterrows():\n",
    "    claim = row['claim']\n",
    "    for j, (article, stance) in enumerate(zip(row['article'], row['stance'])):\n",
    "        article_title = row['article_title'][j] if isinstance(row['article_title'], list) else row['article_title']\n",
    "        pairs.append({\n",
    "            'claim': claim,\n",
    "            'article': article,\n",
    "            'article_title': article_title,\n",
    "            'stance': stance\n",
    "        })\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs)\n",
    "\n",
    "# Add these lines to see the output\n",
    "print(f\"Total claim-article pairs: {len(pairs_df)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(pairs_df.head())\n",
    "\n",
    "# Count the stance labels\n",
    "print(\"\\nStance distribution:\")\n",
    "print(pairs_df['stance'].value_counts())\n",
    "\n",
    "# Show one example of each stance\n",
    "print(\"\\nExamples of each stance:\")\n",
    "for stance in [\"Agree\", \"Disagree\", \"Discuss\", \"Unrelated\"]:\n",
    "    if stance in pairs_df['stance'].values:\n",
    "        example = pairs_df[pairs_df['stance'] == stance].iloc[0]\n",
    "        print(f\"\\n--- {stance} Example ---\")\n",
    "        print(f\"Claim: {example['claim'][:100]}...\")\n",
    "        print(f\"Article Title: {example['article_title']}\")\n",
    "\n",
    "# Explode the test dataset into claim-article pairs\n",
    "test_pairs = []\n",
    "for i, row in df_test.iterrows():\n",
    "    claim = row['claim']\n",
    "    for j, (article, stance) in enumerate(zip(row['article'], row['stance'])):\n",
    "        article_title = row['article_title'][j] if isinstance(row['article_title'], list) else row['article_title']\n",
    "        test_pairs.append({\n",
    "            'claim': claim,\n",
    "            'article': article,\n",
    "            'article_title': article_title,\n",
    "            'stance': stance\n",
    "        })\n",
    "\n",
    "test_pairs_df = pd.DataFrame(test_pairs)\n",
    "print(f\"Total claim-article pairs in test set: {len(test_pairs_df)}\")\n",
    "print(\"\\nStance distribution in test set:\")\n",
    "print(test_pairs_df['stance'].value_counts())\n",
    "\n",
    "# Clean HTML entities from a sample\n",
    "for stance in [\"Agree\", \"Disagree\", \"Discuss\", \"Unrelated\"]:\n",
    "    stance_examples = pairs_df[pairs_df['stance'] == stance]\n",
    "\n",
    "    if len(stance_examples) > 0:\n",
    "        random_index = random.randint(0, len(stance_examples) - 1)\n",
    "        example = stance_examples.iloc[random_index]\n",
    "\n",
    "        # Clean HTML entities\n",
    "        cleaned_title = html.unescape(example['article_title'])\n",
    "        cleaned_article = html.unescape(example['article'][:150])\n",
    "\n",
    "        print(f\"\\n--- Random {stance} Example (Cleaned) ---\")\n",
    "        print(f\"Claim: {example['claim']}\")\n",
    "        print(f\"Article Title: {cleaned_title}\")\n",
    "        print(f\"Article Excerpt: {cleaned_article}...\")\n",
    "        print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmxtGM8L_9t0"
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NO2P7VmkAFDC",
    "outputId": "0e5e821f-a8ea-43ff-9e2c-e470f228432e"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade huggingface_hub\n",
    "!pip install transformers==4.49.0\n",
    "!pip install accelerate\n",
    "!pip install peft==0.5.0\n",
    "!pip install datasets\n",
    "!pip install bitsandbytes==0.38.2\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5pJzM0AB4KB",
    "outputId": "ebd97c91-5b09-4668-f4b8-094f72a6c79f"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login\n",
    "\n",
    "# 3. Import libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, roc_curve, auc\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import html\n",
    "import random\n",
    "import time\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    print(\"CUDA is available! Using GPU.\")\n",
    "else:\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "#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\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHMKOFnW6ig9"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lC0OfD6kJZMo",
    "outputId": "30e55372-c608-4135-aa14-1bdd25d5cb58"
   },
   "outputs": [],
   "source": [
    "# Clone the repository into the current Colab directory\n",
    "!git clone https://github.com/DariusFeher/dynamic-tokenization.git\n",
    "\n",
    "!pip install absl-py accelerate adapters aiohttp aiosignal anyio appdirs argon2-cffi argon2-cffi-bindings arrow asttokens async-lru attrs Babel beautifulsoup4 bleach chex click comm contourpy cryptography cycler datasets debugpy decorator defusedxml dill etils evaluate executing fastjsonschema filelock flax fonttools fqdn frozenlist fsspec gitdb GitPython h11 h5py httpcore httpx huggingface-hub idna importlib_resources ipykernel ipython isoduration jax jaxlib jedi Jinja2 joblib json5 jsonpatch jsonpointer jsonschema jsonschema-specifications jupyter-events jupyter-lsp jupyter_client jupyter_core jupyter_server jupyter_server_terminals jupyterlab jupyterlab_pygments jupyterlab_server kiwisolver lightning-utilities markdown-it-py MarkupSafe matplotlib matplotlib-inline maturin mdurl menuinst mistune ml-dtypes mpmath msgpack multidict multiprocess nbclient nbconvert nbformat nest-asyncio networkx notebook_shim numpy nvidia-cublas-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-cuda-nvrtc-cu12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NTzBpQyVMHei",
    "outputId": "ae25f4b0-22f1-431c-ecd9-4cac9eb78c6c"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/reemmadell19/zett\n",
    "\n",
    "!pip install ./zett"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZip6OVlEHNg",
    "outputId": "4311f8d4-1beb-4884-ac49-aad147b6ef10"
   },
   "outputs": [],
   "source": [
    "!pip install -U git+https://github.com/reemmadell19/zett.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFkc18OIOHMB"
   },
   "outputs": [],
   "source": [
    "!find /usr/local/lib/python3.11/dist-packages/zett -type f -exec sed -i '/madlad400_metadata.csv/ s/^/#/' {} \\;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ycJigBqWMBxu",
    "outputId": "3cccadbf-4acc-4df0-d61e-16a84e62cdad"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path to the dynamic-tokenization folder\n",
    "repo_path = os.path.join(os.getcwd(), 'dynamic-tokenization')\n",
    "\n",
    "# Add the repository path to sys.path if not already present\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "\n",
    "# Optionally, check the contents to verify the folder structure\n",
    "!ls -l {repo_path}/tokenizations/\n",
    "\n",
    "# Now import the DynamicBPE class from dynamic_bpe_v2.py.\n",
    "# Adjust the module name to exactly match the file name and its case.\n",
    "from tokenizations.dynamic_bpe import Dynamic_BPE\n",
    "\n",
    "print(\"DynamicBPE imported successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLfbhr33E_Wa",
    "outputId": "062d6d2a-a575-4a9f-a34b-ffc60e0c315d"
   },
   "outputs": [],
   "source": [
    "!python -c \"from zett.utils import CHARS_TO_BYTES; print({ch: val for ch, val in CHARS_TO_BYTES.items() if 0x0600 <= ord(ch) <= 0x06FF})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K77GV0tTwlop"
   },
   "source": [
    "# Arabic Dynamic BPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6Ukt_EuwHt1"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import copy\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "from typing import Tuple\n",
    "from datasets.formatting.formatting import LazyBatch\n",
    "from tokenizations.tokenizers_utils import tokenize  # Use your existing tokenize utility\n",
    "from tokenizers import pre_tokenizers\n",
    "\n",
    "class ArabicDynamicBPE:\n",
    "    \"\"\"\n",
    "    A dynamic BPE tokenizer tailored for Arabic.\n",
    "\n",
    "    Design choices:\n",
    "      - Normalization: Applies NFKC normalization, removes diacritics (nonspacing marks)\n",
    "        and tatweel (ـ) to standardize Arabic text.\n",
    "      - Merging Criterion: Two tokens are mergeable if neither is a special token and if their\n",
    "        normalized concatenation contains no whitespace. The merged token is allowed if re-tokenizing\n",
    "        via the underlying pre-tokenizer yields at most two segments.\n",
    "      - Merging Process: Repeatedly merge the most frequent valid pair up to a specified maximum.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, tokenizer_boundary: str = \"pretokens\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_boundary = tokenizer_boundary\n",
    "        self.special_token_map = set(tokenizer.special_tokens_map.values())\n",
    "        # Use a raw string (r\"\") to avoid invalid escape sequence warnings.\n",
    "        self.punctuation_tokens = {\n",
    "            token for token in tokenizer.vocab\n",
    "            if any(c in token for c in r\"\"\".,!?;:()-\"'`$%&*+<=>@[\\]^_{|}~،؟؛\"\"\")\n",
    "            and not any(c.isdigit() for c in token)\n",
    "        }\n",
    "        self.debug = False\n",
    "\n",
    "    def normalize(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize Arabic text by:\n",
    "          - Applying NFKC normalization.\n",
    "          - Removing diacritics (nonspacing marks).\n",
    "          - Removing tatweel (ـ).\n",
    "        \"\"\"\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "        text = ''.join(ch for ch in text if unicodedata.category(ch) != 'Mn')\n",
    "        text = text.replace(\"ـ\", \"\")\n",
    "        return text\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def is_valid_pair(self, pair: Tuple[str, str]) -> bool:\n",
    "        token1, token2 = pair\n",
    "        # Do not merge if either token is a special token.\n",
    "        if token1 in self.special_token_map or token2 in self.special_token_map:\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            # For \"sentence\" boundary, disallow merging of special tokens.\n",
    "            if self.tokenizer_boundary == \"sentence\":\n",
    "                return token1 not in self.special_token_map and token2 not in self.special_token_map\n",
    "\n",
    "            # Create the merged string using native UTF-8 encoding.\n",
    "            merged_string = (token1 + token2).encode(\"utf-8\").decode(\"utf-8\")\n",
    "\n",
    "            # Relax the merging criteria:\n",
    "            # Allow merge if re-tokenizing the merged string yields at most 2 segments.\n",
    "            if self.tokenizer_boundary == \"pretokens\":\n",
    "                tokens = self.tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(merged_string)\n",
    "                return len(tokens) <= 2\n",
    "            elif self.tokenizer_boundary == \"word\":\n",
    "                tokens = pre_tokenizers.WhitespaceSplit().pre_tokenize_str(merged_string)\n",
    "                return len(tokens) <= 2 and token1 not in self.special_token_map and token2 not in self.special_token_map\n",
    "            elif self.tokenizer_boundary == \"word_hyphen\":\n",
    "                tokens = pre_tokenizers.WhitespaceSplit().pre_tokenize_str(merged_string)\n",
    "                cond1 = len(tokens) <= 2 and token1 not in self.special_token_map and token2 not in self.special_token_map\n",
    "                if cond1 and token1 not in self.punctuation_tokens and token2 in self.punctuation_tokens:\n",
    "                    return token2 == \"-\"\n",
    "                return cond1\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def get_most_frequent_pair(self, batch_tokens) -> Tuple[str, str]:\n",
    "        pair_freqs = Counter()\n",
    "        for token_sequence in batch_tokens:\n",
    "            pairs = list(zip(token_sequence, token_sequence[1:]))\n",
    "            valid_pairs = [pair for pair in pairs if self.is_valid_pair(pair)]\n",
    "            pair_freqs.update(valid_pairs)\n",
    "        if pair_freqs:\n",
    "            return max(pair_freqs, key=pair_freqs.get)\n",
    "        return \"\"\n",
    "\n",
    "    def merge_pair(self, a: str, b: str, batch_tokens, ner: bool = False, batch_word_ids: list = []):\n",
    "        for idx, token_seq in enumerate(batch_tokens):\n",
    "            i = 0\n",
    "            new_token_seq = []\n",
    "            new_word_ids = []\n",
    "            while i < len(token_seq):\n",
    "                if i < len(token_seq) - 1 and token_seq[i] == a and token_seq[i+1] == b:\n",
    "                    new_token_seq.append(a + b)\n",
    "                    if ner:\n",
    "                        new_word_ids.append(batch_word_ids[idx][i])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_token_seq.append(token_seq[i])\n",
    "                    if ner:\n",
    "                        new_word_ids.append(batch_word_ids[idx][i])\n",
    "                    i += 1\n",
    "            batch_tokens[idx] = new_token_seq\n",
    "            if ner:\n",
    "                batch_word_ids[idx] = new_word_ids\n",
    "        return batch_tokens, batch_word_ids\n",
    "\n",
    "    def tokenize_base_case(self, batch_examples, mlm: bool = False, max_length: int = 128,\n",
    "                             ner: bool = False, nli: bool = False, mmlu: bool = False):\n",
    "        batch_tokens = []\n",
    "        unique_tokens_original = set()\n",
    "        batch_word_tokens = []\n",
    "        batch_word_ids = []\n",
    "        if mmlu:\n",
    "            if isinstance(batch_examples, list):\n",
    "                for batch_example in batch_examples:\n",
    "                    tokens = [\"<s>\"] + tokenize(\n",
    "                        batch_example,\n",
    "                        self.tokenizer,\n",
    "                        max_length=max_length,\n",
    "                        truncation=True,\n",
    "                    )\n",
    "                    if len(tokens) > max_length:\n",
    "                        tokens = tokens[:max_length]\n",
    "                    unique_tokens_original.update(tokens)\n",
    "                    batch_tokens.append(tokens)\n",
    "            else:\n",
    "                for idx, _ in enumerate(batch_examples[\"prompt\"]):\n",
    "                    tokens = [\"<s>\"] + tokenize(\n",
    "                        batch_examples[\"prompt\"][idx],\n",
    "                        self.tokenizer,\n",
    "                        max_length=max_length,\n",
    "                        truncation=True,\n",
    "                    )\n",
    "                    if len(tokens) > max_length:\n",
    "                        tokens = tokens[:max_length]\n",
    "                    unique_tokens_original.update(tokens)\n",
    "                    batch_tokens.append(tokens)\n",
    "        elif ner:\n",
    "            if isinstance(batch_examples, list):\n",
    "                for batch_example in batch_examples:\n",
    "                    tokens = [\"<s>\"]\n",
    "                    word_ids = [None]\n",
    "                    for word_index, word in enumerate(batch_example[\"tokens\"]):\n",
    "                        subtokens = self.tokenizer.tokenize(word, max_length=max_length)\n",
    "                        tokens.extend(subtokens)\n",
    "                        word_ids.extend([word_index] * len(subtokens))\n",
    "                    if len(tokens) >= max_length:\n",
    "                        tokens = tokens[: max_length - 1]\n",
    "                        word_ids = word_ids[: max_length - 1]\n",
    "                    tokens.append(\"</s>\")\n",
    "                    word_ids.append(None)\n",
    "                    batch_tokens.append(tokens)\n",
    "                    unique_tokens_original.update(tokens)\n",
    "                    batch_word_tokens.append(tokens)\n",
    "                    batch_word_ids.append(word_ids)\n",
    "            else:\n",
    "                for idx, _ in enumerate(batch_examples[\"tokens\"]):\n",
    "                    tokens = [\"<s>\"]\n",
    "                    word_ids = [None]\n",
    "                    for word_index, word in enumerate(batch_examples[\"tokens\"][idx]):\n",
    "                        subtokens = self.tokenizer.tokenize(word, max_length=max_length)\n",
    "                        tokens.extend(subtokens)\n",
    "                        word_ids.extend([word_index] * len(subtokens))\n",
    "                    if len(tokens) >= max_length:\n",
    "                        tokens = tokens[: max_length - 1]\n",
    "                        word_ids = word_ids[: max_length - 1]\n",
    "                    tokens.append(\"</s>\")\n",
    "                    word_ids.append(None)\n",
    "                    unique_tokens_original.update(tokens)\n",
    "                    batch_tokens.append(tokens)\n",
    "                    batch_word_ids.append(word_ids)\n",
    "        elif nli:\n",
    "            if isinstance(batch_examples, list):\n",
    "                for batch_example in batch_examples:\n",
    "                    tokens = (\n",
    "                        [\"<s>\"]\n",
    "                        + tokenize(batch_example[\"premise\"], self.tokenizer)\n",
    "                        + [\"</s>\", \"</s>\"]\n",
    "                        + tokenize(batch_example[\"hypothesis\"], self.tokenizer)\n",
    "                        + [\"</s>\"]\n",
    "                    )\n",
    "                    batch_tokens.append(tokens)\n",
    "                    unique_tokens_original.update(tokens)\n",
    "                    if self.debug:\n",
    "                        tokens_word = (\n",
    "                            [\"<s>\"]\n",
    "                            + pretokenize(batch_example[\"premise\"], self.tokenizer)\n",
    "                            + [\"</s>\", \"</s>\"]\n",
    "                            + pretokenize(batch_example[\"hypothesis\"], self.tokenizer)\n",
    "                            + [\"</s>\"]\n",
    "                        )\n",
    "                        batch_word_tokens.append(tokens_word)\n",
    "            else:\n",
    "                for idx, _ in enumerate(batch_examples[\"premise\"]):\n",
    "                    tokens = (\n",
    "                        [\"<s>\"]\n",
    "                        + tokenize(batch_examples[\"premise\"][idx], self.tokenizer)\n",
    "                        + [\"</s>\", \"</s>\"]\n",
    "                        + tokenize(batch_examples[\"hypothesis\"][idx], self.tokenizer)\n",
    "                        + [\"</s>\"]\n",
    "                    )\n",
    "                    batch_tokens.append(tokens)\n",
    "                    unique_tokens_original.update(tokens)\n",
    "                    if self.debug:\n",
    "                        tokens_word = (\n",
    "                            [\"<s>\"]\n",
    "                            + pretokenize(batch_examples[\"premise\"][idx], self.tokenizer)\n",
    "                            + [\"</s>\", \"</s>\"]\n",
    "                            + pretokenize(batch_examples[\"hypothesis\"][idx], self.tokenizer)\n",
    "                            + [\"</s>\"]\n",
    "                        )\n",
    "                        batch_word_tokens.append(tokens_word)\n",
    "        elif mlm:\n",
    "            if isinstance(batch_examples, list):\n",
    "                for batch_example in batch_examples:\n",
    "                    tokens = (\n",
    "                        [\"<s>\"]\n",
    "                        + tokenize(\n",
    "                            batch_example[\"text\"],\n",
    "                            self.tokenizer,\n",
    "                            max_length=max_length - 2,\n",
    "                        )\n",
    "                        + [\"</s>\"]\n",
    "                    )\n",
    "                    tokens = tokens[:max_length]\n",
    "                    batch_tokens.append(tokens)\n",
    "                    unique_tokens_original.update(tokens)\n",
    "                    if self.debug:\n",
    "                        tokens_word = (\n",
    "                            [\"<s>\"]\n",
    "                            + pretokenize(batch_example[\"text\"], self.tokenizer)\n",
    "                            + [\"</s>\"]\n",
    "                        )\n",
    "                        batch_word_tokens.append(tokens_word)\n",
    "            else:\n",
    "                for idx, _ in enumerate(batch_examples[\"text\"]):\n",
    "                    tokens = (\n",
    "                        [\"<s>\"]\n",
    "                        + tokenize(\n",
    "                            batch_examples[\"text\"][idx],\n",
    "                            max_length=max_length - 2,\n",
    "                            truncation=True,\n",
    "                            tokenizer=self.tokenizer,\n",
    "                        )\n",
    "                        + [\"</s>\"]\n",
    "                    )\n",
    "                    if self.debug:\n",
    "                        tokens_word = (\n",
    "                            [\"<s>\"]\n",
    "                            + pretokenize(batch_examples[\"text\"][idx], self.tokenizer)\n",
    "                            + [\"</s>\"]\n",
    "                        )\n",
    "                        batch_word_tokens.append(tokens_word)\n",
    "        return unique_tokens_original, batch_tokens, batch_word_tokens, batch_word_ids\n",
    "\n",
    "    def tokenize_batch(self, batch_examples: LazyBatch, max_nr_merges: int = 1000,\n",
    "                       mlm: bool = False, max_length: int = 128, ner: bool = False,\n",
    "                       nli: bool = False, mmlu: bool = False):\n",
    "        unique_tokens_original, batch_tokens, batch_word_tokens, batch_word_ids = (\n",
    "            self.tokenize_base_case(\n",
    "                batch_examples=batch_examples,\n",
    "                mlm=mlm,\n",
    "                max_length=max_length,\n",
    "                ner=ner,\n",
    "                nli=nli,\n",
    "                mmlu=mmlu,\n",
    "            )\n",
    "        )\n",
    "        total_merges = 0\n",
    "        while total_merges < max_nr_merges:\n",
    "            best_pair = self.get_most_frequent_pair(batch_tokens=batch_tokens)\n",
    "            if best_pair == \"\":\n",
    "\n",
    "                break\n",
    "            total_merges += 1\n",
    "            batch_tokens, batch_word_ids = self.merge_pair(\n",
    "                a=best_pair[0],\n",
    "                b=best_pair[1],\n",
    "                batch_tokens=batch_tokens,\n",
    "                ner=ner,\n",
    "                batch_word_ids=batch_word_ids,\n",
    "            )\n",
    "        unique_tokens_bpe = set()\n",
    "        batch_seq_lengths = []\n",
    "        for tokenised_text in batch_tokens:\n",
    "            unique_tokens_bpe.update(tokenised_text)\n",
    "            batch_seq_lengths.append(len(tokenised_text))\n",
    "        if self.debug:\n",
    "            for i in range(32):\n",
    "                if i < len(batch_tokens) and batch_tokens[i] != batch_word_tokens[i]:\n",
    "                    print(i)\n",
    "                    print(batch_tokens[i])\n",
    "                    print(batch_word_tokens[i])\n",
    "        return batch_tokens, unique_tokens_bpe, batch_seq_lengths, batch_word_ids\n",
    "\n",
    "    def tokenize_batch_for_seq_len(self, batch_examples: LazyBatch, max_nr_merges: int = 20000,\n",
    "                                   mlm: bool = False, max_length: int = 128, ner: bool = False,\n",
    "                                   nli: bool = False, mmlu: bool = False):\n",
    "        batch_tokens = []\n",
    "        total_merges = 0\n",
    "        _, batch_tokens, _, _ = self.tokenize_base_case(\n",
    "            batch_examples=batch_examples,\n",
    "            mlm=False,\n",
    "            max_length=max_length,\n",
    "            ner=False,\n",
    "            nli=False,\n",
    "            mmlu=True,\n",
    "        )\n",
    "        import copy\n",
    "        init_batch_tokens = copy.deepcopy(batch_tokens)\n",
    "        if total_merges not in self.merges2seqLen:\n",
    "            self.merges2seqLen[total_merges] = 0\n",
    "        for tokenised_text in batch_tokens:\n",
    "            self.merges2seqLen[total_merges] += len(tokenised_text)\n",
    "        while total_merges < max_nr_merges:\n",
    "            best_pair = self.get_most_frequent_pair(batch_tokens=batch_tokens)\n",
    "            if best_pair == \"\":\n",
    "                for i in range(total_merges + 1, max_nr_merges):\n",
    "                    if i not in self.merges2seqLen:\n",
    "                        self.merges2seqLen[i] = 0\n",
    "                    for tokenised_text in batch_tokens:\n",
    "                        self.merges2seqLen[i] += len(tokenised_text)\n",
    "                break\n",
    "            total_merges += 1\n",
    "            batch_tokens, _ = self.merge_pair(\n",
    "                a=best_pair[0], b=best_pair[1], batch_tokens=batch_tokens\n",
    "            )\n",
    "            if total_merges not in self.merges2seqLen:\n",
    "                self.merges2seqLen[total_merges] = 0\n",
    "            for tokenised_text in batch_tokens:\n",
    "                self.merges2seqLen[total_merges] += len(tokenised_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cu4MbwU7Q8n_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ArabicDynamicBPETokenizer:\n",
    "    \"\"\"\n",
    "    A wrapper that exposes a Hugging Face tokenizer interface while\n",
    "    applying dynamic merging via the ArabicDynamicBPE class.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_tokenizer, arabic_dynamic_bpe):\n",
    "        self.base_tokenizer = base_tokenizer\n",
    "        self.arabic_dynamic_bpe = arabic_dynamic_bpe\n",
    "        # Save the vocabulary for later use.\n",
    "        self._vocab = base_tokenizer.get_vocab()\n",
    "        self.pad_token = base_tokenizer.pad_token\n",
    "        self.eos_token = base_tokenizer.eos_token\n",
    "        self.pad_token_id = base_tokenizer.pad_token_id\n",
    "        self.eos_token_id = base_tokenizer.eos_token_id\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self._vocab\n",
    "\n",
    "    def __call__(self, text, max_length=512, truncation=True, return_tensors=None, max_nr_merges=1000, **kwargs):\n",
    "        \"\"\"\n",
    "        Tokenizes the input text without padding.\n",
    "\n",
    "        It applies dynamic merging using ArabicDynamicBPE and then converts the merged tokens to IDs.\n",
    "        No extra pad tokens are added.\n",
    "        \"\"\"\n",
    "        # Use the ArabicDynamicBPE's tokenize_batch method on a single-sample batch.\n",
    "        batch_tokens, _, batch_seq_lengths, _ = self.arabic_dynamic_bpe.tokenize_batch(\n",
    "            [text],\n",
    "            max_nr_merges=max_nr_merges,\n",
    "            mlm=False,\n",
    "            max_length=max_length,\n",
    "            ner=False,\n",
    "            nli=False,\n",
    "            mmlu=False\n",
    "        )\n",
    "        # Fallback: If no merges occur, fall back to the base tokenizer.\n",
    "        if not batch_tokens or len(batch_tokens) == 0 or len(batch_tokens[0]) == 0:\n",
    "            output = self.base_tokenizer(\n",
    "                text, max_length=max_length, truncation=truncation, return_tensors=return_tensors, **kwargs\n",
    "            )\n",
    "            return output\n",
    "\n",
    "        # Remove any special boundary markers if desired.\n",
    "        merged_tokens = [t for t in batch_tokens[0] if t not in {\"<|begin_of_text|>\", \"<|end_of_text|>\"}]\n",
    "        # Convert the merged tokens to IDs.\n",
    "        input_ids = self.base_tokenizer.convert_tokens_to_ids(merged_tokens)\n",
    "\n",
    "        # Apply truncation if necessary.\n",
    "        if truncation:\n",
    "            input_ids = input_ids[:max_length]\n",
    "        # Note: We are NOT performing any padding here.\n",
    "        # Create an attention mask that is 1 for each token in the unpadded sequence.\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        if return_tensors == \"pt\":\n",
    "            input_ids = torch.tensor(input_ids)\n",
    "            attention_mask = torch.tensor(attention_mask)\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "    def encode(self, text, **kwargs):\n",
    "        return self.__call__(text, **kwargs)[\"input_ids\"]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return self.base_tokenizer.convert_ids_to_tokens(ids)\n",
    "\n",
    "    def save_pretrained(self, path):\n",
    "        \"\"\"\n",
    "        Delegate saving to the wrapped base tokenizer.\n",
    "        \"\"\"\n",
    "        return self.base_tokenizer.save_pretrained(path)\n",
    "\n",
    "def get_arabic_dynamic_tokenizer(model_name=\"meta-llama/Llama-3.1-8B\", use_fast=True, tokenizer_boundary=\"pretokens\"):\n",
    "    \"\"\"\n",
    "    Loads the base Hugging Face tokenizer and wraps it with your ArabicDynamicBPE.\n",
    "    This function always applies Arabic-specific normalization and merging criteria.\n",
    "    \"\"\"\n",
    "    from transformers import AutoTokenizer\n",
    "    base_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=use_fast)\n",
    "    if base_tokenizer.pad_token is None:\n",
    "        base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "    # Instantiate your ArabicDynamicBPE (using your provided code).\n",
    "    arabic_dynamic_bpe = ArabicDynamicBPE(base_tokenizer, tokenizer_boundary=tokenizer_boundary)\n",
    "    # Return the wrapped tokenizer.\n",
    "    return ArabicDynamicBPETokenizer(base_tokenizer, arabic_dynamic_bpe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ce08a41a931e46cf9d03232c59cdf5a1",
      "68f98205905942fe9a3b062d76d78ba7",
      "c35373f658c94260961987bc196e71d1",
      "4890080a752b4379993bd1f24f0930a5",
      "ace74af83afa4e78b35a7c036b703941",
      "c4ccc77c25084c2ba3935dc7dbc6aab9",
      "e41883677afe45cdb324a5affeb01e89",
      "3a15c16cf199493fa78760b3fa51f9ed",
      "84d838cbe2c04331844bdf721d5cd4ac",
      "2e519c3f19674984b5b0982241a7e379",
      "84d423b9aa4a4ab998e1055514bd5cec",
      "2668019874d34b699d285a42db86dd07",
      "a0d3f3a5bfc243e0be229a2e67b502a1",
      "e109492cf13f4fc292bea346d5e8d853",
      "57e858d23f514879bec5cf9d9b8ce362",
      "ab6c6a1676b84a3db8cc6f96215dc3f0",
      "64f744c5b0834a72b671fee5f1f1665f",
      "ab62710fbbc7489885776cb28bbf9351",
      "c38f1e420adb4755b13e4f95c3b39673",
      "aa50c8d6d1984ee09fada7a36b19a891",
      "9b0e7da8d30e499eb9aa87e7bbe705d3",
      "d930d706873848f1b63cff299f9559e3",
      "04e8292532394d8b83c86c5eea5e0b53",
      "8d98c66899b24e6cbfeb4a316f92792e",
      "6c49f1960ef24f1f8ab7220b36f6855b",
      "b820c1378b254d20ad4f1ea66a5b35da",
      "7b6951f3ed6f4c63ad575f5fd2ee72c5",
      "b0c0358e9b4e48a9b3491168870f0058",
      "6456a8f2e2ac476c9ba711a3f445b28d",
      "cd08a381dced407184506fda7f752b3d",
      "6f3e33c1f06146bda9dd8a676bc92b5e",
      "f5d102c40b724e6fba937bc4f3238021",
      "5049e2449c454c998d0ff4921846b1b2",
      "572fc09554874c218ca138d0da95827b",
      "a0096f402366459987e2bdcc85f3d8e9",
      "28a1532892ef45a79f024095f094848d",
      "2ba9acc13fd64b4cafb396b906ea9d4e",
      "4ea32ef560794d6e8af719390045a5d7",
      "c5295e4f3b1e4d5c8a8ae2b9cc5d4f31",
      "9e3f852fff0d48699306c5e473fb4ea6",
      "7d8a8bf66118493aa990af001c1152e0",
      "705045ea02634af9ac4e22abbf270a6f",
      "d18a6d5887b845178f4db32e6dd4666d",
      "3b760ccc87f441308a8bfa0b21e2f87c",
      "2bd0d3e1903b494c8af812a872e814e0",
      "925a9e6362a14295aab7f86dac9bf16c",
      "83e4f195e7b34039893d34e5f8f72baa",
      "07e366ef214a466cbf783f3d2aa36541",
      "0a411b6379ec4b4daa4be97459aea050",
      "a5f5e5ec2066429db69344c7f717dc02",
      "b2f59e3748864179af373a5480c663ab",
      "8cb9e34f043e4c0a8c16129a89a5e996",
      "02bb3987df0a4dd3a3451515163e59a1",
      "01bb054275f14f69b7338b7f19b4f3d4",
      "6448f471373841ff984358c71e6c47fd",
      "56ec527a634047b7afb590dc0bdab61e",
      "7bc6157f7cdb417ebd1cf1953deed80b",
      "56703234a6df44c782f4f93478e850b2",
      "70deab89926d47b9974df71dba757873",
      "237a16c57dcd4f81a5fcf66b4bf7023e",
      "01a387f95f2d49f5aff47f6919df52a9",
      "84462ecbc054464abecd98a2cd2bbb93",
      "5ab60201dbc6417383e00949888a26ed",
      "a19800993ad440639d1a1e8e2cb40ef0",
      "b7f90879d6354ac08f41846faae02d9d",
      "9acf896e5eff4e2290f1d537a710325a",
      "4cd4d78d836a46468b0351d88d9f7e5a",
      "21ad76b51808467088a2d79c2ac2983c",
      "7e49ca61ca8b41f29bab3d1372b00c07",
      "0209cc8ec7c84aab898be0898b99df1d",
      "0b7f19beffca4da8af0d726e1bd54638",
      "dfb70a90c99340bcba4a5bc11ece4e1a",
      "13f8d32e849c4fb493559500068f629a",
      "cb3cd9ba37bc4681a849e42cb861c9f1",
      "a36b46e5fe904471a4fa94092707e7be",
      "a1a4d22e3cc340ddb91625bacadb1498",
      "3336b8fa58c24e6db34776e0c65d241f",
      "07508f02d033455aa7f63ccbc636af5a",
      "3f147b24ab4f4b589e36a164f39cd223",
      "80dcbd80c3134d45b4ce6a9d3f2be849",
      "e114fb1662264d6e82170b427f1ff432",
      "6f6cd63856984f6cb1f25b4318c20994",
      "9b1c6e4d81ea48f6acc7c97f24a32b64",
      "be6c946c4cc9496b8c740747a9fd6054",
      "d69ce74a3fc04bd985f3e8d813ca81e3",
      "8de6784fb9f348d5a5961c0c56531943",
      "b7f57af3a0f242f7a15fe2ac58bca0b9",
      "9394b3dc53004f019a2acc425312b755",
      "62777c944cbe4e339c4bf16e26732fc1",
      "b46300395a454777984907c602e51542",
      "18f41244f0e74fbd8636770031790846",
      "bc8aa861750f4ac2b31049ec36da1267",
      "6226f427289e4eecbc737e2d44ed2335",
      "4bfe7bb222e2426dbf7cc225591f772d",
      "a97e4084a23b411a85da01f77d95af92",
      "8c23e40a9ac3427c89676a37e8aa1ec8",
      "7278bafe99314f44b33bbb98a02185be",
      "98709da136ce447bba3b9e2701066f71",
      "9a3636dee9cf4c2b83a3358a97a53fa6",
      "e2a8b240b5994221af978808709871a2",
      "91d5a8c687034884b9adead3f97f1d5b",
      "ab08a8df3f9548ee98ba7b5f73799e03",
      "63feed798cfd461ca343044f28813181",
      "8db8d0b7afdf405a82bbdae26e14f919",
      "202f5f6bde824a42a7e25fc8c6eb60b4",
      "cea1408a9d684e12a98bf387ce679f3a",
      "4142fa8dab5d4d28b86066ec06497435",
      "5845c6f34c6e4330bb34375c86e7caa4",
      "b92e8affe144457da4614f52825e3f11",
      "234e4a0e6e6b4090bf4ae5f8fae51610",
      "5c785c47a6814385ba87211c96e59355",
      "4b78c76d40c04d9f9ed916db9c20c71c",
      "722723885a1d4feea7ba4a2598bba0d4",
      "e8a0b8388f0149c3999fede88bafe7f4",
      "f504efdb5333474ca6e8d69d9aa19563",
      "3136d79e05ee4d68a4dd3ccf0e1b0e05",
      "5d04eba587d54361aaa737f39369ac97",
      "55a5f1fff13844f7b21973b956875d2b",
      "a1f5023105d9498db05f03c8268740b0",
      "890b64d66b064196979d23b8821205eb",
      "cdf438cad3b948fcbd7c3c4225b79bce"
     ]
    },
    "id": "VC-ACc51wr_7",
    "outputId": "0ed28008-d37a-48c6-83ff-cecef213f985"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import html\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score, roc_curve, auc\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "# Import your LoRA/PEFT related modules\n",
    "# from peft import get_peft_model, LoraConfig, TaskType\n",
    "# and your custom get_arabic_dynamic_tokenizer\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset class remains largely unchanged.\n",
    "# You may also choose to change the default max_length here.\n",
    "# ---------------------------\n",
    "class StanceDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=10):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer  # This is now our dynamic tokenizer.\n",
    "        self.max_length = max_length\n",
    "        self.label_map = {'Agree': 0, 'Disagree': 1, 'Discuss': 2, 'Unrelated': 3}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        claim = html.unescape(row['claim'])\n",
    "        article = html.unescape(row['article'])\n",
    "        if len(article) > 5000:\n",
    "            article = article[:5000]\n",
    "        text = f\"Claim: {claim} Article: {article}\"\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',  # You can choose 'do_not_pad' if your model supports variable lengths.\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        encoding['labels'] = torch.tensor(self.label_map[row['stance']])\n",
    "        return encoding\n",
    "\n",
    "# ---------------------------\n",
    "# Analysis functions remain the same.\n",
    "# ---------------------------\n",
    "def analyze_tokenization(pairs_df, test_pairs_df, sample_size=100):\n",
    "    print(\"Loading dynamic arabic tokenizer for tokenization analysis...\")\n",
    "    tokenizer = get_arabic_dynamic_tokenizer(\"meta-llama/Llama-3.1-8B\", use_fast=True, tokenizer_boundary=\"pretokens\")\n",
    "    combined_df = pd.concat([pairs_df, test_pairs_df])\n",
    "    sample_df = combined_df\n",
    "    print(f\"Analyzing tokenization on {sample_size} samples...\")\n",
    "    results = []\n",
    "    total_chars = 0\n",
    "    total_tokens = 0\n",
    "    token_lengths = []\n",
    "    tokens_per_char_values = []\n",
    "    oov_count = 0\n",
    "    unique_tokens = set()\n",
    "    vocabulary_size = len(tokenizer.vocab)\n",
    "    highly_fragmented_words = []\n",
    "    words_analyzed = 0\n",
    "    for _, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "        claim = html.unescape(row['claim'])\n",
    "        article = html.unescape(row['article'][:500])\n",
    "        claim_chars = len(claim)\n",
    "        claim_tokens = tokenizer.encode(claim, add_special_tokens=False)\n",
    "        claim_tokens_texts = tokenizer.convert_ids_to_tokens(claim_tokens)\n",
    "        claim_fertility = len(claim_tokens) / claim_chars if claim_chars > 0 else 0\n",
    "        tokens_per_char_values.append(claim_fertility)\n",
    "        for token in claim_tokens_texts:\n",
    "            token_lengths.append(len(token))\n",
    "            unique_tokens.add(token)\n",
    "        total_chars += claim_chars\n",
    "        total_tokens += len(claim_tokens)\n",
    "        claim_words = claim.split()\n",
    "        for word in claim_words:\n",
    "            if len(word) >= 3:\n",
    "                words_analyzed += 1\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                if len(word_tokens) >= 3:\n",
    "                    highly_fragmented_words.append(word)\n",
    "                    oov_count += 1\n",
    "        results.append({\n",
    "            'text_length': len(claim) + len(article[:500]),\n",
    "            'llama_tokens': len(claim_tokens) + len(tokenizer.encode(article[:500])),\n",
    "            'fertility': claim_fertility,\n",
    "            'stance': row['stance']\n",
    "        })\n",
    "    token_df = pd.DataFrame(results)\n",
    "    avg_token_fertility = np.mean(tokens_per_char_values)\n",
    "    token_length_distribution = Counter(token_lengths)\n",
    "    avg_token_length = np.mean(token_lengths)\n",
    "    median_token_length = np.median(token_lengths)\n",
    "    compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0\n",
    "    vocabulary_coverage = len(unique_tokens) / vocabulary_size\n",
    "    oov_rate = oov_count / words_analyzed if words_analyzed > 0 else 0\n",
    "    print(\"\\n===== Tokenization Analysis Results =====\")\n",
    "    print(f\"1. Token Fertility (tokens/char): {avg_token_fertility:.4f}\")\n",
    "    print(f\"2. Token Length: Mean={avg_token_length:.2f}, Median={median_token_length:.2f}\")\n",
    "    print(f\"3. Compression Ratio (chars/token): {compression_ratio:.4f}\")\n",
    "    print(f\"4. Vocabulary: Used {len(unique_tokens)} of {vocabulary_size} tokens ({vocabulary_coverage:.2%})\")\n",
    "    print(f\"5. OOV Rate: {oov_rate:.4f} ({oov_count}/{words_analyzed} words)\")\n",
    "    if highly_fragmented_words:\n",
    "        print(\"\\nExample highly fragmented words (potential OOVs):\")\n",
    "        sample_oov = random.sample(highly_fragmented_words, min(10, len(highly_fragmented_words)))\n",
    "        for word in sample_oov:\n",
    "            tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "            token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
    "            print(f\"  '{word}' → {len(tokens)} tokens: {token_texts}\")\n",
    "    # (Plotting code omitted for brevity)\n",
    "    with open('tokenization_metrics.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'token_fertility': avg_token_fertility,\n",
    "            'token_length_mean': avg_token_length,\n",
    "            'token_length_median': median_token_length,\n",
    "            'compression_ratio': compression_ratio,\n",
    "            'vocabulary_size': vocabulary_size,\n",
    "            'vocabulary_coverage': vocabulary_coverage,\n",
    "            'oov_rate': oov_rate,\n",
    "            'token_length_distribution': {str(k): v for k, v in token_length_distribution.items()}\n",
    "        }, f, indent=2)\n",
    "    return token_df\n",
    "\n",
    "# ---------------------------\n",
    "# Evaluation function: we add a fix to ensure data is float32.\n",
    "# ---------------------------\n",
    "def evaluate_model_comprehensive(best_model, test_loader, device, label_names=['Agree', 'Disagree', 'Discuss', 'Unrelated']):\n",
    "    best_model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    test_start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating model\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = batch.pop('labels')\n",
    "            outputs = best_model(**batch)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().to(torch.float32).numpy())\n",
    "    test_time = time.time() - test_start_time\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.vstack(all_probs)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision_micro = precision_score(all_labels, all_preds, average='micro')\n",
    "    precision_macro = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall_micro = recall_score(all_labels, all_preds, average='micro')\n",
    "    recall_macro = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    try:\n",
    "        num_classes = len(np.unique(all_labels))\n",
    "        labels_one_hot = np.eye(num_classes)[all_labels]\n",
    "        roc_auc = roc_auc_score(labels_one_hot, all_probs, multi_class='ovr')\n",
    "        fpr = {}\n",
    "        tpr = {}\n",
    "        roc_auc_per_class = {}\n",
    "        for i in range(num_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(labels_one_hot[:, i], all_probs[:, i])\n",
    "            roc_auc_per_class[i] = auc(fpr[i], tpr[i])\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for i in range(num_classes):\n",
    "            plt.plot(fpr[i], tpr[i], lw=2, label=f'{label_names[i]} (AUC = {roc_auc_per_class[i]:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curves (One-vs-Rest, Overall AUC = {roc_auc:.2f})')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig('roc_curves.png')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not calculate ROC AUC due to {str(e)}\")\n",
    "        roc_auc = None\n",
    "        roc_auc_per_class = None\n",
    "    class_metrics = {}\n",
    "    for i, label in enumerate(label_names):\n",
    "        class_precision = precision_score([1 if l == i else 0 for l in all_labels],\n",
    "                                          [1 if p == i else 0 for p in all_preds], zero_division=0)\n",
    "        class_recall = recall_score([1 if l == i else 0 for l in all_labels],\n",
    "                                    [1 if p == i else 0 for p in all_preds], zero_division=0)\n",
    "        class_f1 = f1_score([1 if l == i else 0 for l in all_labels],\n",
    "                            [1 if p == i else 0 for p in all_preds], zero_division=0)\n",
    "        class_metrics[label] = {'precision': class_precision, 'recall': class_recall, 'f1': class_f1}\n",
    "    print(f\"\\nComprehensive Model Evaluation Results:\")\n",
    "    print(f\"  Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: Micro={precision_micro:.4f}, Macro={precision_macro:.4f}\")\n",
    "    print(f\"  Recall: Micro={recall_micro:.4f}, Macro={recall_macro:.4f}\")\n",
    "    print(f\"  F1 Score: Micro={f1_micro:.4f}, Macro={f1_macro:.4f}\")\n",
    "    if roc_auc is not None:\n",
    "        print(f\"  ROC AUC (OVR): {roc_auc:.4f}\")\n",
    "    print(f\"  Evaluation time: {test_time:.2f} seconds\")\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "    results = {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_micro': precision_micro,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_micro': recall_micro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'roc_auc': roc_auc,\n",
    "        'roc_auc_per_class': {label_names[i]: auc_val for i, auc_val in roc_auc_per_class.items()} if roc_auc_per_class else None,\n",
    "        'confusion_matrix': conf_matrix.tolist(),\n",
    "        'class_metrics': class_metrics,\n",
    "        'test_time': test_time\n",
    "    }\n",
    "    return results, all_preds, all_labels\n",
    "\n",
    "# ---------------------------\n",
    "# Train Model function with modifications\n",
    "# ---------------------------\n",
    "def train_model(model_name, pairs_df, test_pairs_df, output_dir=\"model_outputs\", use_lora=True, epochs=10):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    num_labels = 4\n",
    "    # Use batch_size=1 and gradient accumulation steps=16;\n",
    "    # Optionally, you may further reduce grad_accum_steps if needed.\n",
    "    batch_size = 1\n",
    "    grad_accum_steps = 16\n",
    "    learning_rate = 2e-5\n",
    "    # Reduce maximum length to 256 tokens to save memory.\n",
    "    max_length = 256\n",
    "    model_save_path = os.path.join(output_dir, f\"{model_name.split('/')[-1]}_stance_detector\")\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Training {model_name} model for stance detection\")\n",
    "    print(f\"{'='*40}\")\n",
    "    print(\"Loading arabic dynamic tokenizer...\")\n",
    "    tokenizer = get_arabic_dynamic_tokenizer(model_name, use_fast=True, tokenizer_boundary=\"pretokens\")\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"Set EOS token as padding token\")\n",
    "    print(\"Preparing datasets...\")\n",
    "    train_df, val_df = train_test_split(\n",
    "        pairs_df,\n",
    "        test_size=0.1,\n",
    "        random_state=42,\n",
    "        stratify=pairs_df['stance']\n",
    "    )\n",
    "    print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_pairs_df)}\")\n",
    "    train_dataset = StanceDataset(train_df, tokenizer, max_length)\n",
    "    val_dataset = StanceDataset(val_df, tokenizer, max_length)\n",
    "    test_dataset = StanceDataset(test_pairs_df, tokenizer, max_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    print(\"Initializing model...\")\n",
    "    if use_lora:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        if model.config.pad_token_id is None:\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        # Enable gradient checkpointing to save memory\n",
    "        if hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "            model.gradient_checkpointing_enable()\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            inference_mode=False,\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "        )\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        if model.config.pad_token_id is None:\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        # Optionally enable gradient checkpointing here as well.\n",
    "        if hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "            model.gradient_checkpointing_enable()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_loader) * epochs // grad_accum_steps\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=total_steps // 10,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    print(\"Starting training...\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_f1s = []\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\")):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / grad_accum_steps\n",
    "            loss.backward()\n",
    "            train_loss += loss.item() * grad_accum_steps\n",
    "            if (step + 1) % grad_accum_steps == 0 or step == len(train_loader) - 1:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        # Clear cache after each epoch to reduce fragmentation\n",
    "        torch.cuda.empty_cache()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        valid_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                labels = batch.pop('labels')\n",
    "                outputs = model(**batch)\n",
    "                if outputs.loss is not None:\n",
    "                    loss = outputs.loss\n",
    "                    val_loss += loss.item()\n",
    "                    valid_batches += 1\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        if valid_batches > 0:\n",
    "            val_loss /= valid_batches\n",
    "        val_losses.append(val_loss)\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        val_precision = precision_score(all_labels, all_preds, average='macro')\n",
    "        val_recall = recall_score(all_labels, all_preds, average='macro')\n",
    "        val_f1s.append(val_f1)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Accuracy: {val_acc:.4f}\")\n",
    "        print(f\"  Val F1 (macro): {val_f1:.4f}\")\n",
    "        print(f\"  Val Precision: {val_precision:.4f}\")\n",
    "        print(f\"  Val Recall: {val_recall:.4f}\")\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            model_to_save.save_pretrained(model_save_path)\n",
    "            # If your dynamic tokenizer lacks a save_pretrained method, save the underlying base tokenizer:\n",
    "            tokenizer.base_tokenizer.save_pretrained(model_save_path)\n",
    "            print(f\"  Model saved to {model_save_path}\")\n",
    "    # (Plotting and test evaluation code remains as before.)\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    if use_lora:\n",
    "        config = PeftConfig.from_pretrained(model_save_path)\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            config.base_model_name_or_path,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        if base_model.config.pad_token_id is None:\n",
    "            base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        best_model = PeftModel.from_pretrained(base_model, model_save_path)\n",
    "    else:\n",
    "        best_model = AutoModelForSequenceClassification.from_pretrained(model_save_path)\n",
    "        if best_model.config.pad_token_id is None:\n",
    "            best_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "    label_names = ['Agree', 'Disagree', 'Discuss', 'Unrelated']\n",
    "    eval_results, test_preds, test_labels = evaluate_model_comprehensive(best_model, test_loader, device, label_names)\n",
    "    with open(os.path.join(output_dir, f'{model_name.split(\"/\")[-1]}_results.json'), 'w') as f:\n",
    "        json.dump(eval_results, f, indent=2)\n",
    "    return eval_results\n",
    "\n",
    "# ---------------------------\n",
    "# Pipeline functions (unchanged)\n",
    "# ---------------------------\n",
    "def run_pipeline(pairs_df, test_pairs_df, run_tokenization=True, epochs=10):\n",
    "    print(f\"Training set: {len(pairs_df)} claim-article pairs\")\n",
    "    print(\"Training stance distribution:\")\n",
    "    print(pairs_df['stance'].value_counts())\n",
    "    print(f\"\\nTest set: {len(test_pairs_df)} claim-article pairs\")\n",
    "    print(\"Test stance distribution:\")\n",
    "    print(test_pairs_df['stance'].value_counts())\n",
    "    if run_tokenization:\n",
    "        print(\"\\n=== Running Enhanced Tokenization Analysis ===\")\n",
    "        token_df = analyze_tokenization(pairs_df, test_pairs_df)\n",
    "    model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "    results = train_model(model_name, pairs_df, test_pairs_df, epochs=epochs)\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run pipeline for 10 epochs\n",
    "    results = run_pipeline(pairs_df, test_pairs_df, run_tokenization=True, epochs=10)\n",
    "    print(\"\\n=== Final Results Summary ===\")\n",
    "    print(f\"Model: meta-llama/Llama-3.1-8B\")\n",
    "    print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"F1 Score (macro): {results['f1_macro']:.4f}\")\n",
    "    print(f\"F1 Score (micro): {results['f1_micro']:.4f}\")\n",
    "    print(f\"Precision (macro): {results['precision_macro']:.4f}\")\n",
    "    print(f\"Recall (macro): {results['recall_macro']:.4f}\")\n",
    "    if results['roc_auc'] is not None:\n",
    "        print(f\"ROC AUC: {results['roc_auc']:.4f}\")\n",
    "    print(\"\\nClass-specific results:\")\n",
    "    for label, metrics in results['class_metrics'].items():\n",
    "        print(f\"  {label}: F1={metrics['f1']:.4f}, Precision={metrics['precision ']:.4f}, Recall={metrics['recall']:.4f}\")\n",
    "\n",
    "# Enhanced tokenization analysis function for Arabic with all five requested metrics\n",
    "def analyze_tokenization_fertility(pairs_df, test_pairs_df, sample_size=100):\n",
    "    \"\"\"\n",
    "    Analyze tokenization with all five specific metrics for Arabic text with Llama:\n",
    "    - Token Count (Token Fertility)\n",
    "    - Token Length Distribution\n",
    "    - Compression Ratio\n",
    "    - Vocabulary Size\n",
    "    - Out-of-Vocabulary (OOV) Rate\n",
    "    \"\"\"\n",
    "    print(\"Loading tokenizer for detailed tokenization analysis...\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = get_arabic_dynamic_tokenizer(\"meta-llama/Llama-3.1-8B\", use_fast=True, tokenizer_boundary=\"pretokens\")\n",
    "    # Sample data for analysis\n",
    "    combined_df = pd.concat([pairs_df, test_pairs_df])\n",
    "    sample_df = combined_df  # Use all data\n",
    "\n",
    "    # Initialize metrics\n",
    "    results = []\n",
    "\n",
    "    # For vocabulary size metrics\n",
    "    vocabulary_size = len(tokenizer.get_vocab())\n",
    "    unique_tokens_used = set()\n",
    "\n",
    "    # For token length distribution\n",
    "    token_lengths = []\n",
    "\n",
    "    # For overall metrics\n",
    "    total_chars = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for _, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Analyzing token metrics\"):\n",
    "        # Clean text\n",
    "        claim = html.unescape(row['claim'])\n",
    "        article = html.unescape(row['article'][:500])  # Truncate for speed\n",
    "\n",
    "        # Tokenize\n",
    "        claim_tokens = tokenizer.encode(claim, add_special_tokens=False)\n",
    "        article_tokens = tokenizer.encode(article, add_special_tokens=False)\n",
    "\n",
    "        # Get token texts\n",
    "        claim_token_texts = tokenizer.convert_ids_to_tokens(claim_tokens)\n",
    "        article_token_texts = tokenizer.convert_ids_to_tokens(article_tokens)\n",
    "\n",
    "        # Add to unique tokens set\n",
    "        unique_tokens_used.update(claim_tokens + article_tokens)\n",
    "\n",
    "        # Track token lengths\n",
    "        for token in claim_token_texts + article_token_texts:\n",
    "            token_lengths.append(len(token))\n",
    "\n",
    "        # Update total counts\n",
    "        total_chars += len(claim) + len(article)\n",
    "        total_tokens += len(claim_tokens) + len(article_tokens)\n",
    "\n",
    "        # Calculate fertility (tokens per character)\n",
    "        claim_fertility = len(claim_tokens) / len(claim) if len(claim) > 0 else 0\n",
    "        article_fertility = len(article_tokens) / len(article) if len(article) > 0 else 0\n",
    "\n",
    "        # Calculate fragmentation (tokens per word) - for OOV estimation\n",
    "        claim_words = len(claim.split())\n",
    "        article_words = len(article.split())\n",
    "        claim_fragmentation = len(claim_tokens) / claim_words if claim_words > 0 else 0\n",
    "        article_fragmentation = len(article_tokens) / article_words if article_words > 0 else 0\n",
    "\n",
    "        # Estimate OOV by checking for multi-token words\n",
    "        # In Arabic, words broken into many tokens often indicate OOV issues\n",
    "        claim_words_list = claim.split()\n",
    "        article_words_list = article.split()\n",
    "\n",
    "        # Track highly fragmented words (potential OOVs)\n",
    "        highly_fragmented_words = []\n",
    "        for word in claim_words_list + article_words_list:\n",
    "            if len(word) >= 3:  # Only check non-trivial words\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                if len(word_tokens) >= 3:  # If a word is broken into 3+ tokens\n",
    "                    highly_fragmented_words.append(word)\n",
    "\n",
    "        results.append({\n",
    "            'claim_tokens': len(claim_tokens),\n",
    "            'article_tokens': len(article_tokens),\n",
    "            'claim_fertility': claim_fertility,\n",
    "            'article_fertility': article_fertility,\n",
    "            'claim_fragmentation': claim_fragmentation,\n",
    "            'article_fragmentation': article_fragmentation,\n",
    "            'highly_fragmented_words': len(highly_fragmented_words),\n",
    "            'highly_fragmented_word_examples': highly_fragmented_words[:5] if highly_fragmented_words else [],\n",
    "            'stance': row['stance']\n",
    "        })\n",
    "\n",
    "    token_df = pd.DataFrame(results)\n",
    "\n",
    "    # Calculate final metrics\n",
    "\n",
    "    # 1. Token Fertility (tokens per character)\n",
    "    avg_claim_fertility = token_df['claim_fertility'].mean()\n",
    "    avg_article_fertility = token_df['article_fertility'].mean()\n",
    "    avg_fertility = (avg_claim_fertility + avg_article_fertility) / 2\n",
    "\n",
    "    # 2. Token Length Distribution\n",
    "    token_length_dist = Counter(token_lengths)\n",
    "    avg_token_length = np.mean(token_lengths)\n",
    "\n",
    "    # 3. Compression Ratio (characters per token)\n",
    "    compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "    # 4. Vocabulary Size metrics\n",
    "    vocabulary_coverage = len(unique_tokens_used) / vocabulary_size\n",
    "\n",
    "    # 5. OOV Rate\n",
    "    avg_oov_count = token_df['highly_fragmented_words'].mean()\n",
    "    total_words = sum(len(row['claim'].split()) + len(row['article'][:500].split()) for _, row in sample_df.iterrows())\n",
    "    total_highly_fragmented = sum(row['highly_fragmented_words'] for _, row in token_df.iterrows())\n",
    "    oov_rate = total_highly_fragmented / total_words if total_words > 0 else 0\n",
    "\n",
    "    # Display statistics\n",
    "    print(\"\\n=== Tokenization Metrics Analysis ===\")\n",
    "    print(f\"1. Token Fertility (tokens/char):\")\n",
    "    print(f\"   - Claim: {avg_claim_fertility:.4f} tokens/char\")\n",
    "    print(f\"   - Article: {avg_article_fertility:.4f} tokens/char\")\n",
    "    print(f\"   - Overall: {avg_fertility:.4f} tokens/char\")\n",
    "\n",
    "    print(f\"\\n2. Token Length Distribution:\")\n",
    "    print(f\"   - Average token length: {avg_token_length:.2f} characters\")\n",
    "    print(f\"   - Median token length: {np.median(token_lengths):.2f} characters\")\n",
    "\n",
    "    print(f\"\\n3. Compression Ratio: {compression_ratio:.4f} chars/token\")\n",
    "\n",
    "    print(f\"\\n4. Vocabulary Size:\")\n",
    "    print(f\"   - Total vocabulary size: {vocabulary_size} tokens\")\n",
    "    print(f\"   - Unique tokens used: {len(unique_tokens_used)} tokens\")\n",
    "    print(f\"   - Vocabulary coverage: {vocabulary_coverage:.2%}\")\n",
    "\n",
    "    print(f\"\\n5. OOV Rate:\")\n",
    "    print(f\"   - Average highly fragmented words per sample: {avg_oov_count:.2f}\")\n",
    "    print(f\"   - OOV rate: {oov_rate:.2%} ({total_highly_fragmented}/{total_words} words)\")\n",
    "\n",
    "    # Print some example highly fragmented words (potential OOVs)\n",
    "    if sum(len(row) for row in token_df['highly_fragmented_word_examples']) > 0:\n",
    "        print(\"\\nExample highly fragmented words (potential OOVs):\")\n",
    "        all_examples = [w for row in token_df['highly_fragmented_word_examples'] for w in row]\n",
    "        for word in random.sample(all_examples, min(10, len(all_examples))):\n",
    "            tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "            token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
    "            print(f\"  '{word}' → {len(tokens)} tokens: {token_texts}\")\n",
    "\n",
    "    # Visualize fertility metrics\n",
    "    plt.figure(figsize=(16, 12))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.boxplot(x='stance', y='claim_fertility', data=token_df)\n",
    "    plt.title('Token Fertility (Claim) by Stance')\n",
    "    plt.ylabel('Tokens per Character')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.boxplot(x='stance', y='article_fertility', data=token_df)\n",
    "    plt.title('Token Fertility (Article) by Stance')\n",
    "    plt.ylabel('Tokens per Character')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.bar(token_length_dist.keys(), token_length_dist.values())\n",
    "    plt.xlabel('Token Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Token Length Distribution')\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.boxplot(x='stance', y='highly_fragmented_words', data=token_df)\n",
    "    plt.title('OOV Words by Stance')\n",
    "    plt.ylabel('Count of Highly Fragmented Words')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tokenization_metrics_analysis.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Save results to file\n",
    "    metrics_results = {\n",
    "        'token_fertility': {\n",
    "            'claim': avg_claim_fertility,\n",
    "            'article': avg_article_fertility,\n",
    "            'overall': avg_fertility\n",
    "        },\n",
    "        'token_length_distribution': {\n",
    "            'mean': avg_token_length,\n",
    "            'median': float(np.median(token_lengths)),\n",
    "            'distribution': {str(k): v for k, v in token_length_dist.items()}\n",
    "        },\n",
    "        'compression_ratio': compression_ratio,\n",
    "        'vocabulary_size': {\n",
    "            'total': vocabulary_size,\n",
    "            'used': len(unique_tokens_used),\n",
    "            'coverage': vocabulary_coverage\n",
    "        },\n",
    "        'oov_rate': {\n",
    "            'avg_highly_fragmented_per_sample': avg_oov_count,\n",
    "            'overall_rate': oov_rate\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open('tokenization_metrics_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics_results, f, indent=2)\n",
    "\n",
    "    return token_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a587b8c796d3410bbd97c0731ba417da",
      "2443d3467eff466a9bd3d234227d10c1",
      "4091f11242eb4dce9b155367be01ab8a",
      "eba0b69c9cfd471888315590c75a27bc",
      "f1c47c66b95c4de8abf8494933d0ce6c",
      "dd31dba2dabd47c89396d054cc51d7ec",
      "a0efda160f35429f909bf2c1baec0dba",
      "6eeda532e05040768166ec20e9a2fa3c",
      "c802a1b577774af1ae3f198f9f2a47d4",
      "0ce9d9df6a104359b6e2230b8a27f86b",
      "2dac00a2085e4f9b8233950aa5427144"
     ]
    },
    "id": "GH2FPMciB9ly",
    "outputId": "9d50510e-0bc8-41b0-aa26-a423adc555b8"
   },
   "outputs": [],
   "source": [
    "class StanceDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=10):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer  # This is now our dynamic tokenizer.\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Map stance labels to integers\n",
    "        self.label_map = {'Agree': 0, 'Disagree': 1, 'Discuss': 2, 'Unrelated': 3}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        claim = html.unescape(row['claim'])\n",
    "        article = html.unescape(row['article'])\n",
    "        if len(article) > 5000:\n",
    "            article = article[:5000]\n",
    "        text = f\"Claim: {claim} Article: {article}\"\n",
    "\n",
    "        # Tokenize using the dynamic tokenizer\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        encoding['labels'] = torch.tensor(self.label_map[row['stance']])\n",
    "        return encoding\n",
    "\n",
    "# Enhanced Tokenization Analysis with all 5 metrics\n",
    "def analyze_tokenization(pairs_df, test_pairs_df, sample_size=100):\n",
    "    \"\"\"\n",
    "    Analyze tokenization for Llama with 5 specific metrics:\n",
    "    - Token Count (Token Fertility)\n",
    "    - Token Length Distribution\n",
    "    - Compression Ratio\n",
    "    - Vocabulary Size\n",
    "    - OOV Rate\n",
    "    \"\"\"\n",
    "    print(\"Loading dynamic arabic tokenizer for tokenization analysis...\")\n",
    "    tokenizer = get_arabic_dynamic_tokenizer(\"meta-llama/Llama-3.1-8B\", use_fast=True, tokenizer_boundary=\"pretokens\")\n",
    "\n",
    "    # Sample data for analysis\n",
    "    combined_df = pd.concat([pairs_df, test_pairs_df])\n",
    "    sample_df = combined_df  # Use all data\n",
    "\n",
    "    print(f\"Analyzing tokenization on {sample_size} samples...\")\n",
    "\n",
    "    # Initialize metrics\n",
    "    results = []\n",
    "    total_chars = 0\n",
    "    total_tokens = 0\n",
    "    token_lengths = []\n",
    "    tokens_per_char_values = []\n",
    "    oov_count = 0\n",
    "    unique_tokens = set()\n",
    "    vocabulary_size = len(tokenizer.vocab)\n",
    "    highly_fragmented_words = []\n",
    "    words_analyzed = 0\n",
    "\n",
    "    for _, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "        # Clean HTML entities\n",
    "        claim = html.unescape(row['claim'])\n",
    "        article = html.unescape(row['article'][:500])  # Truncate article for speed\n",
    "\n",
    "        # Process claim\n",
    "        claim_chars = len(claim)\n",
    "        claim_tokens = tokenizer.encode(claim, add_special_tokens=False)\n",
    "        claim_tokens_texts = tokenizer.convert_ids_to_tokens(claim_tokens)\n",
    "\n",
    "        # Token fertility (tokens per character)\n",
    "        claim_fertility = len(claim_tokens) / claim_chars if claim_chars > 0 else 0\n",
    "        tokens_per_char_values.append(claim_fertility)\n",
    "\n",
    "        # Track token lengths for distribution\n",
    "        for token in claim_tokens_texts:\n",
    "            token_lengths.append(len(token))\n",
    "            unique_tokens.add(token)\n",
    "\n",
    "        # Update counts\n",
    "        total_chars += claim_chars\n",
    "        total_tokens += len(claim_tokens)\n",
    "\n",
    "        # Check word-level fragmentation (OOV estimation)\n",
    "        claim_words = claim.split()\n",
    "        for word in claim_words:\n",
    "            if len(word) >= 3:  # Only check non-trivial words\n",
    "                words_analyzed += 1\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                if len(word_tokens) >= 3:  # If a word is broken into 3+ tokens\n",
    "                    highly_fragmented_words.append(word)\n",
    "                    oov_count += 1\n",
    "\n",
    "        # Store results for this sample\n",
    "        results.append({\n",
    "            'text_length': len(claim) + len(article[:500]),\n",
    "            'llama_tokens': len(claim_tokens) + len(tokenizer.encode(article[:500])),\n",
    "            'fertility': claim_fertility,\n",
    "            'stance': row['stance']\n",
    "        })\n",
    "\n",
    "    token_df = pd.DataFrame(results)\n",
    "\n",
    "    # 1. Token Fertility (tokens per character)\n",
    "    avg_token_fertility = np.mean(tokens_per_char_values)\n",
    "\n",
    "    # 2. Token Length Distribution\n",
    "    token_length_distribution = Counter(token_lengths)\n",
    "    avg_token_length = np.mean(token_lengths)\n",
    "    median_token_length = np.median(token_lengths)\n",
    "\n",
    "    # 3. Compression Ratio (characters per token)\n",
    "    compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "    # 4. Vocabulary Size\n",
    "    vocabulary_coverage = len(unique_tokens) / vocabulary_size\n",
    "\n",
    "    # 5. OOV Rate\n",
    "    oov_rate = oov_count / words_analyzed if words_analyzed > 0 else 0\n",
    "\n",
    "    # Display statistics\n",
    "    print(\"\\n===== Tokenization Analysis Results =====\")\n",
    "    print(f\"1. Token Fertility (tokens/char): {avg_token_fertility:.4f}\")\n",
    "    print(f\"2. Token Length: Mean={avg_token_length:.2f}, Median={median_token_length:.2f}\")\n",
    "    print(f\"3. Compression Ratio (chars/token): {compression_ratio:.4f}\")\n",
    "    print(f\"4. Vocabulary: Used {len(unique_tokens)} of {vocabulary_size} tokens ({vocabulary_coverage:.2%})\")\n",
    "    print(f\"5. OOV Rate: {oov_rate:.4f} ({oov_count}/{words_analyzed} words)\")\n",
    "\n",
    "    # Print some example highly fragmented words (potential OOVs)\n",
    "    if highly_fragmented_words:\n",
    "        print(\"\\nExample highly fragmented words (potential OOVs):\")\n",
    "        sample_oov = random.sample(highly_fragmented_words, min(10, len(highly_fragmented_words)))\n",
    "        for word in sample_oov:\n",
    "            tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "            token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
    "            print(f\"  '{word}' → {len(tokens)} tokens: {token_texts}\")\n",
    "\n",
    "    # Visualize tokenization\n",
    "    plt.figure(figsize=(16, 12))\n",
    "\n",
    "    # Plot 1: Token Count vs Text Length\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(token_df['text_length'], token_df['llama_tokens'], alpha=0.7)\n",
    "    plt.xlabel('Text Length (characters)')\n",
    "    plt.ylabel('Llama 3.1 Token Count')\n",
    "    plt.title('Tokenization Analysis: Llama 3.1')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Token Fertility by Stance\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.boxplot(x='stance', y='fertility', data=token_df)\n",
    "    plt.title('Token Fertility by Stance')\n",
    "    plt.ylabel('Tokens per Character')\n",
    "\n",
    "    # Plot 3: Token Length Distribution\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.bar(token_length_distribution.keys(), token_length_distribution.values())\n",
    "    plt.xlabel('Token Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Token Length Distribution')\n",
    "\n",
    "    # Plot 4: Compression Ratio Distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    compression_values = [1/f if f > 0 else 0 for f in token_df['fertility']]\n",
    "    plt.hist(compression_values, bins=20)\n",
    "    plt.xlabel('Compression Ratio (chars/token)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Compression Ratio Distribution')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tokenization_analysis.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Save results to file\n",
    "    tokenization_metrics = {\n",
    "        'token_fertility': avg_token_fertility,\n",
    "        'token_length_mean': avg_token_length,\n",
    "        'token_length_median': median_token_length,\n",
    "        'compression_ratio': compression_ratio,\n",
    "        'vocabulary_size': vocabulary_size,\n",
    "        'vocabulary_coverage': vocabulary_coverage,\n",
    "        'oov_rate': oov_rate,\n",
    "        'token_length_distribution': {str(k): v for k, v in token_length_distribution.items()}\n",
    "    }\n",
    "\n",
    "    with open('tokenization_metrics.json', 'w') as f:\n",
    "        json.dump(tokenization_metrics, f, indent=2)\n",
    "\n",
    "    return token_df\n",
    "def evaluate_model_comprehensive(best_model, test_loader, device, label_names=['Agree', 'Disagree', 'Discuss', 'Unrelated']):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with comprehensive metrics:\n",
    "    - AUC, ROC curves\n",
    "    - Micro F1, Macro F1\n",
    "    - Precision, Recall (micro and macro)\n",
    "    - Accuracy\n",
    "    - Loss\n",
    "    \"\"\"\n",
    "    best_model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Use CrossEntropyLoss for loss calculation\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    test_start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating model\"):\n",
    "            # FIX: Do not pop labels; instead pass them so that loss is computed.\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            # Save labels for later metric calculation\n",
    "            labels = batch['labels']\n",
    "\n",
    "            outputs = best_model(**batch)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Get predictions and probabilities\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            # Convert BF16 to float32 (if needed) before converting to numpy.\n",
    "            all_probs.append(probs.cpu().to(torch.float32).numpy())\n",
    "\n",
    "    test_time = time.time() - test_start_time\n",
    "\n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "\n",
    "    # Convert predictions and labels to numpy arrays\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.vstack(all_probs)\n",
    "\n",
    "    # 1. Accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    # 2-3. Precision (Micro and Macro)\n",
    "    precision_micro = precision_score(all_labels, all_preds, average='micro')\n",
    "    precision_macro = precision_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    # 4-5. Recall (Micro and Macro)\n",
    "    recall_micro = recall_score(all_labels, all_preds, average='micro')\n",
    "    recall_macro = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    # 6-7. F1 Score (Micro and Macro)\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # 8. AUC-ROC (One-vs-Rest for multiclass)\n",
    "    try:\n",
    "        num_classes = len(np.unique(all_labels))\n",
    "        labels_one_hot = np.eye(num_classes)[all_labels]\n",
    "        roc_auc = roc_auc_score(labels_one_hot, all_probs, multi_class='ovr')\n",
    "        fpr = {}\n",
    "        tpr = {}\n",
    "        roc_auc_per_class = {}\n",
    "        for i in range(num_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(labels_one_hot[:, i], all_probs[:, i])\n",
    "            roc_auc_per_class[i] = auc(fpr[i], tpr[i])\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for i in range(num_classes):\n",
    "            plt.plot(fpr[i], tpr[i], lw=2, label=f'{label_names[i]} (AUC = {roc_auc_per_class[i]:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curves (One-vs-Rest, Overall AUC = {roc_auc:.2f})')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig('roc_curves.png')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not calculate ROC AUC due to {str(e)}\")\n",
    "        roc_auc = None\n",
    "        roc_auc_per_class = None\n",
    "\n",
    "    # Class-wise metrics\n",
    "    class_metrics = {}\n",
    "    for i, label in enumerate(label_names):\n",
    "        class_precision = precision_score(\n",
    "            [1 if l == i else 0 for l in all_labels],\n",
    "            [1 if p == i else 0 for p in all_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "        class_recall = recall_score(\n",
    "            [1 if l == i else 0 for l in all_labels],\n",
    "            [1 if p == i else 0 for p in all_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "        class_f1 = f1_score(\n",
    "            [1 if l == i else 0 for l in all_labels],\n",
    "            [1 if p == i else 0 for p in all_preds],\n",
    "            zero_division=0\n",
    "        )\n",
    "        class_metrics[label] = {'precision': class_precision, 'recall': class_recall, 'f1': class_f1}\n",
    "\n",
    "    print(f\"\\nComprehensive Model Evaluation Results:\")\n",
    "    print(f\"  Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: Micro={precision_micro:.4f}, Macro={precision_macro:.4f}\")\n",
    "    print(f\"  Recall: Micro={recall_micro:.4f}, Macro={recall_macro:.4f}\")\n",
    "    print(f\"  F1 Score: Micro={f1_micro:.4f}, Macro={f1_macro:.4f}\")\n",
    "    if roc_auc is not None:\n",
    "        print(f\"  ROC AUC (OVR): {roc_auc:.4f}\")\n",
    "    print(f\"  Evaluation time: {test_time:.2f} seconds\")\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "\n",
    "    results = {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_micro': precision_micro,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_micro': recall_micro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'roc_auc': roc_auc,\n",
    "        'roc_auc_per_class': {label_names[i]: auc_val for i, auc_val in roc_auc_per_class.items()} if roc_auc_per_class else None,\n",
    "        'confusion_matrix': conf_matrix.tolist(),\n",
    "        'class_metrics': class_metrics,\n",
    "        'test_time': test_time\n",
    "    }\n",
    "\n",
    "    return results, all_preds, all_labels\n",
    "def train_model(model_name, pairs_df, test_pairs_df, output_dir=\"model_outputs\", use_lora=True, epochs=10):\n",
    "    \"\"\"Train and evaluate a stance detection model.\"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Configuration\n",
    "    num_labels = 4\n",
    "    batch_size = 1  # Small batch size due to model size\n",
    "    grad_accum_steps = 16  # Effective batch size = batch_size * grad_accum_steps\n",
    "    learning_rate = 2e-5\n",
    "    max_length = 512  # Consider lowering this if memory remains an issue\n",
    "    model_save_path = os.path.join(output_dir, f\"{model_name.split('/')[-1]}_stance_detector\")\n",
    "\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Training {model_name} model for stance detection\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    print(\"Loading arabic dynamic tokenizer...\")\n",
    "    tokenizer = get_arabic_dynamic_tokenizer(\"meta-llama/Llama-3.1-8B\", use_fast=True, tokenizer_boundary=\"pretokens\")\n",
    "\n",
    "    # Fix for the padding token issue\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"Set EOS token as padding token\")\n",
    "\n",
    "    # Split training data into train and validation\n",
    "    print(\"Preparing datasets...\")\n",
    "    train_df, val_df = train_test_split(\n",
    "        pairs_df,\n",
    "        test_size=0.1,\n",
    "        random_state=42,\n",
    "        stratify=pairs_df['stance']\n",
    "    )\n",
    "\n",
    "    print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_pairs_df)}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = StanceDataset(train_df, tokenizer, max_length)\n",
    "    val_dataset = StanceDataset(val_df, tokenizer, max_length)\n",
    "    test_dataset = StanceDataset(test_pairs_df, tokenizer, max_length)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"Initializing model...\")\n",
    "    if use_lora:\n",
    "        # Load base model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,  # Using default precision (can use bfloat16 if stable)\n",
    "        )\n",
    "\n",
    "        # Set padding token id in the model config\n",
    "        if model.config.pad_token_id is None:\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        # Enable gradient checkpointing if available (saves memory)\n",
    "        if hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        # Define LoRA configuration\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            inference_mode=False,\n",
    "            r=16,  # rank\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "        )\n",
    "\n",
    "        # Create PEFT model\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "        )\n",
    "        if model.config.pad_token_id is None:\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Move model to GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_loader) * epochs // grad_accum_steps\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=total_steps // 10,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_f1s = []\n",
    "    best_val_f1 = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\")):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / grad_accum_steps  # Normalize loss for gradient accumulation\n",
    "            loss.backward()\n",
    "            train_loss += loss.item() * grad_accum_steps\n",
    "\n",
    "            if (step + 1) % grad_accum_steps == 0 or step == len(train_loader) - 1:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation loop FIX: Do not pop labels so that the loss is computed\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        valid_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                # Keep labels inside the batch so that loss is computed.\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "                valid_batches += 1\n",
    "\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "        if valid_batches > 0:\n",
    "            val_loss /= valid_batches\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        val_precision = precision_score(all_labels, all_preds, average='macro')\n",
    "        val_recall = recall_score(all_labels, all_preds, average='macro')\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Accuracy: {val_acc:.4f}\")\n",
    "        print(f\"  Val F1 (macro): {val_f1:.4f}\")\n",
    "        print(f\"  Val Precision: {val_precision:.4f}\")\n",
    "        print(f\"  Val Recall: {val_recall:.4f}\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            model_to_save.save_pretrained(model_save_path)\n",
    "            tokenizer.save_pretrained(model_save_path)\n",
    "            print(f\"  Model saved to {model_save_path}\")\n",
    "\n",
    "        # Clear GPU cache after each epoch to help avoid fragmentation\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # (Optional) Early stopping\n",
    "        if epoch > 2 and val_losses[-1] > val_losses[-2] and val_losses[-2] > val_losses[-3]:\n",
    "            print(\"Early stopping triggered - validation loss increasing for 3 consecutive epochs\")\n",
    "            break\n",
    "\n",
    "    # Plot training curves (plots remain unchanged)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(val_losses, 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_f1s, 'g-', label='Validation F1')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Validation F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{model_name.split(\"/\")[-1]}_training_curve.png'))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    if use_lora:\n",
    "        config = PeftConfig.from_pretrained(model_save_path)\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            config.base_model_name_or_path,\n",
    "            num_labels=num_labels,\n",
    "        )\n",
    "        if base_model.config.pad_token_id is None:\n",
    "            base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        best_model = PeftModel.from_pretrained(base_model, model_save_path)\n",
    "    else:\n",
    "        best_model = AutoModelForSequenceClassification.from_pretrained(model_save_path)\n",
    "        if best_model.config.pad_token_id is None:\n",
    "            best_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "\n",
    "    label_names = ['Agree', 'Disagree', 'Discuss', 'Unrelated']\n",
    "    eval_results, test_preds, test_labels = evaluate_model_comprehensive(\n",
    "        best_model,\n",
    "        test_loader,\n",
    "        device,\n",
    "        label_names\n",
    "    )\n",
    "\n",
    "    with open(os.path.join(output_dir, f'{model_name.split(\"/\")[-1]}_results.json'), 'w') as f:\n",
    "        json.dump(eval_results, f, indent=2)\n",
    "\n",
    "    return eval_results\n",
    "def run_pipeline(pairs_df, test_pairs_df, run_tokenization=True, epochs=10):\n",
    "    \"\"\"Run the stance detection pipeline with Llama 3.1.\"\"\"\n",
    "    print(f\"Training set: {len(pairs_df)} claim-article pairs\")\n",
    "    print(\"Training stance distribution:\")\n",
    "    print(pairs_df['stance'].value_counts())\n",
    "    print(f\"\\nTest set: {len(test_pairs_df)} claim-article pairs\")\n",
    "    print(\"Test stance distribution:\")\n",
    "    print(test_pairs_df['stance'].value_counts())\n",
    "\n",
    "    if run_tokenization:\n",
    "        print(\"\\n=== Running Enhanced Tokenization Analysis ===\")\n",
    "        token_df = analyze_tokenization(pairs_df, test_pairs_df)\n",
    "\n",
    "    model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "    results = train_model(model_name, pairs_df, test_pairs_df, epochs=epochs)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the complete pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_pipeline(pairs_df, test_pairs_df, run_tokenization=True, epochs=10)\n",
    "\n",
    "    print(\"\\n=== Final Results Summary ===\")\n",
    "    print(f\"Model: meta-llama/Llama-3.1-8B\")\n",
    "    print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"F1 Score (macro): {results['f1_macro']:.4f}\")\n",
    "    print(f\"F1 Score (micro): {results['f1_micro']:.4f}\")\n",
    "    print(f\"Precision (macro): {results['precision_macro']:.4f}\")\n",
    "    print(f\"Recall (macro): {results['recall_macro']:.4f}\")\n",
    "    if results['roc_auc'] is not None:\n",
    "        print(f\"ROC AUC: {results['roc_auc']:.4f}\")\n",
    "\n",
    "    print(\"\\nClass-specific results:\")\n",
    "    for label, metrics in results['class_metrics'].items():\n",
    "        print(f\"  {label}: F1={metrics['f1']:.4f}, Precision={metrics['precision ']:.4f}, Recall={metrics['recall']:.4f}\")\n",
    "\n",
    "# Enhanced tokenization analysis function for Arabic with all five requested metrics\n",
    "def analyze_tokenization_fertility(pairs_df, test_pairs_df, sample_size=100):\n",
    "    \"\"\"\n",
    "    Analyze tokenization with all five specific metrics for Arabic text with Llama:\n",
    "    - Token Count (Token Fertility)\n",
    "    - Token Length Distribution\n",
    "    - Compression Ratio\n",
    "    - Vocabulary Size\n",
    "    - Out-of-Vocabulary (OOV) Rate\n",
    "    \"\"\"\n",
    "    print(\"Loading tokenizer for detailed tokenization analysis...\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = get_arabic_dynamic_tokenizer(\"meta-llama/Llama-3.1-8B\", use_fast=True, tokenizer_boundary=\"pretokens\")\n",
    "    # Sample data for analysis\n",
    "    combined_df = pd.concat([pairs_df, test_pairs_df])\n",
    "    sample_df = combined_df  # Use all data\n",
    "\n",
    "    # Initialize metrics\n",
    "    results = []\n",
    "\n",
    "    # For vocabulary size metrics\n",
    "    vocabulary_size = len(tokenizer.get_vocab())\n",
    "    unique_tokens_used = set()\n",
    "\n",
    "    # For token length distribution\n",
    "    token_lengths = []\n",
    "\n",
    "    # For overall metrics\n",
    "    total_chars = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for _, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Analyzing token metrics\"):\n",
    "        # Clean text\n",
    "        claim = html.unescape(row['claim'])\n",
    "        article = html.unescape(row['article'][:500])  # Truncate for speed\n",
    "\n",
    "        # Tokenize\n",
    "        claim_tokens = tokenizer.encode(claim, add_special_tokens=False)\n",
    "        article_tokens = tokenizer.encode(article, add_special_tokens=False)\n",
    "\n",
    "        # Get token texts\n",
    "        claim_token_texts = tokenizer.convert_ids_to_tokens(claim_tokens)\n",
    "        article_token_texts = tokenizer.convert_ids_to_tokens(article_tokens)\n",
    "\n",
    "        # Add to unique tokens set\n",
    "        unique_tokens_used.update(claim_tokens + article_tokens)\n",
    "\n",
    "        # Track token lengths\n",
    "        for token in claim_token_texts + article_token_texts:\n",
    "            token_lengths.append(len(token))\n",
    "\n",
    "        # Update total counts\n",
    "        total_chars += len(claim) + len(article)\n",
    "        total_tokens += len(claim_tokens) + len(article_tokens)\n",
    "\n",
    "        # Calculate fertility (tokens per character)\n",
    "        claim_fertility = len(claim_tokens) / len(claim) if len(claim) > 0 else 0\n",
    "        article_fertility = len(article_tokens) / len(article) if len(article) > 0 else 0\n",
    "\n",
    "        # Calculate fragmentation (tokens per word) - for OOV estimation\n",
    "        claim_words = len(claim.split())\n",
    "        article_words = len(article.split())\n",
    "        claim_fragmentation = len(claim_tokens) / claim_words if claim_words > 0 else 0\n",
    "        article_fragmentation = len(article_tokens) / article_words if article_words > 0 else 0\n",
    "\n",
    "        # Estimate OOV by checking for multi-token words\n",
    "        # In Arabic, words broken into many tokens often indicate OOV issues\n",
    "        claim_words_list = claim.split()\n",
    "        article_words_list = article.split()\n",
    "\n",
    "        # Track highly fragmented words (potential OOVs)\n",
    "        highly_fragmented_words = []\n",
    "        for word in claim_words_list + article_words_list:\n",
    "            if len(word) >= 3:  # Only check non-trivial words\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                if len(word_tokens) >= 3:  # If a word is broken into 3+ tokens\n",
    "                    highly_fragmented_words.append(word)\n",
    "\n",
    "        results.append({\n",
    "            'claim_tokens': len(claim_tokens),\n",
    "            'article_tokens': len(article_tokens),\n",
    "            'claim_fertility': claim_fertility,\n",
    "            'article_fertility': article_fertility,\n",
    "            'claim_fragmentation': claim_fragmentation,\n",
    "            'article_fragmentation': article_fragmentation,\n",
    "            'highly_fragmented_words': len(highly_fragmented_words),\n",
    "            'highly_fragmented_word_examples': highly_fragmented_words[:5] if highly_fragmented_words else [],\n",
    "            'stance': row['stance']\n",
    "        })\n",
    "\n",
    "    token_df = pd.DataFrame(results)\n",
    "\n",
    "    # Calculate final metrics\n",
    "\n",
    "    # 1. Token Fertility (tokens per character)\n",
    "    avg_claim_fertility = token_df['claim_fertility'].mean()\n",
    "    avg_article_fertility = token_df['article_fertility'].mean()\n",
    "    avg_fertility = (avg_claim_fertility + avg_article_fertility) / 2\n",
    "\n",
    "    # 2. Token Length Distribution\n",
    "    token_length_dist = Counter(token_lengths)\n",
    "    avg_token_length = np.mean(token_lengths)\n",
    "\n",
    "    # 3. Compression Ratio (characters per token)\n",
    "    compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "    # 4. Vocabulary Size metrics\n",
    "    vocabulary_coverage = len(unique_tokens_used) / vocabulary_size\n",
    "\n",
    "    # 5. OOV Rate\n",
    "    avg_oov_count = token_df['highly_fragmented_words'].mean()\n",
    "    total_words = sum(len(row['claim'].split()) + len(row['article'][:500].split()) for _, row in sample_df.iterrows())\n",
    "    total_highly_fragmented = sum(row['highly_fragmented_words'] for _, row in token_df.iterrows())\n",
    "    oov_rate = total_highly_fragmented / total_words if total_words > 0 else 0\n",
    "\n",
    "    # Display statistics\n",
    "    print(\"\\n=== Tokenization Metrics Analysis ===\")\n",
    "    print(f\"1. Token Fertility (tokens/char):\")\n",
    "    print(f\"   - Claim: {avg_claim_fertility:.4f} tokens/char\")\n",
    "    print(f\"   - Article: {avg_article_fertility:.4f} tokens/char\")\n",
    "    print(f\"   - Overall: {avg_fertility:.4f} tokens/char\")\n",
    "\n",
    "    print(f\"\\n2. Token Length Distribution:\")\n",
    "    print(f\"   - Average token length: {avg_token_length:.2f} characters\")\n",
    "    print(f\"   - Median token length: {np.median(token_lengths):.2f} characters\")\n",
    "\n",
    "    print(f\"\\n3. Compression Ratio: {compression_ratio:.4f} chars/token\")\n",
    "\n",
    "    print(f\"\\n4. Vocabulary Size:\")\n",
    "    print(f\"   - Total vocabulary size: {vocabulary_size} tokens\")\n",
    "    print(f\"   - Unique tokens used: {len(unique_tokens_used)} tokens\")\n",
    "    print(f\"   - Vocabulary coverage: {vocabulary_coverage:.2%}\")\n",
    "\n",
    "    print(f\"\\n5. OOV Rate:\")\n",
    "    print(f\"   - Average highly fragmented words per sample: {avg_oov_count:.2f}\")\n",
    "    print(f\"   - OOV rate: {oov_rate:.2%} ({total_highly_fragmented}/{total_words} words)\")\n",
    "\n",
    "    # Print some example highly fragmented words (potential OOVs)\n",
    "    if sum(len(row) for row in token_df['highly_fragmented_word_examples']) > 0:\n",
    "        print(\"\\nExample highly fragmented words (potential OOVs):\")\n",
    "        all_examples = [w for row in token_df['highly_fragmented_word_examples'] for w in row]\n",
    "        for word in random.sample(all_examples, min(10, len(all_examples))):\n",
    "            tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "            token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
    "            print(f\"  '{word}' → {len(tokens)} tokens: {token_texts}\")\n",
    "\n",
    "    # Visualize fertility metrics\n",
    "    plt.figure(figsize=(16, 12))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.boxplot(x='stance', y='claim_fertility', data=token_df)\n",
    "    plt.title('Token Fertility (Claim) by Stance')\n",
    "    plt.ylabel('Tokens per Character')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.boxplot(x='stance', y='article_fertility', data=token_df)\n",
    "    plt.title('Token Fertility (Article) by Stance')\n",
    "    plt.ylabel('Tokens per Character')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.bar(token_length_dist.keys(), token_length_dist.values())\n",
    "    plt.xlabel('Token Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Token Length Distribution')\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.boxplot(x='stance', y='highly_fragmented_words', data=token_df)\n",
    "    plt.title('OOV Words by Stance')\n",
    "    plt.ylabel('Count of Highly Fragmented Words')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tokenization_metrics_analysis.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Save results to file\n",
    "    metrics_results = {\n",
    "        'token_fertility': {\n",
    "            'claim': avg_claim_fertility,\n",
    "            'article': avg_article_fertility,\n",
    "            'overall': avg_fertility\n",
    "        },\n",
    "        'token_length_distribution': {\n",
    "            'mean': avg_token_length,\n",
    "            'median': float(np.median(token_lengths)),\n",
    "            'distribution': {str(k): v for k, v in token_length_dist.items()}\n",
    "        },\n",
    "        'compression_ratio': compression_ratio,\n",
    "        'vocabulary_size': {\n",
    "            'total': vocabulary_size,\n",
    "            'used': len(unique_tokens_used),\n",
    "            'coverage': vocabulary_coverage\n",
    "        },\n",
    "        'oov_rate': {\n",
    "            'avg_highly_fragmented_per_sample': avg_oov_count,\n",
    "            'overall_rate': oov_rate\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open('tokenization_metrics_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics_results, f, indent=2)\n",
    "\n",
    "    return token_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Z-Xj8XTwpUE"
   },
   "source": [
    "# Rest of code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UtmAv2RnT92T",
    "outputId": "60322e9b-c769-4a35-8995-ed766ba37ee7"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "import os\n",
    "import html\n",
    "import re\n",
    "\n",
    "# Set up paths\n",
    "output_dir = \"tokenization_dynamic_analysis_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load your tokenizer\n",
    "print(\"Loading dynamic arabic tokenizer for tokenization analysis...\")\n",
    "tokenizer = get_arabic_dynamic_tokenizer(\"meta-llama/Llama-3.1-8B\", use_fast=True, tokenizer_boundary=\"pretokens\")\n",
    "\n",
    "# Convert prediction labels from stance names to indices\n",
    "def convert_labels_to_indices(labels):\n",
    "    \"\"\"Convert stance labels to numerical indices.\"\"\"\n",
    "    label_map = {'Agree': 0, 'Disagree': 1, 'Discuss': 2, 'Unrelated': 3}\n",
    "    if isinstance(labels[0], str):\n",
    "        return [label_map[label] for label in labels]\n",
    "    return labels\n",
    "\n",
    "# 1. Calculate vocabulary coverage and OOV rate metrics\n",
    "def analyze_vocabulary_coverage(pairs_df, sample_size=None, min_word_length=3):\n",
    "    \"\"\"\n",
    "    Analyze vocabulary coverage and OOV rates for Arabic text with Llama tokenizer.\n",
    "    \"\"\"\n",
    "    print(\"Analyzing vocabulary coverage and OOV rates...\")\n",
    "\n",
    "    # Sample data if requested\n",
    "    if sample_size and len(pairs_df) > sample_size:\n",
    "        sample_df = pairs_df.sample(sample_size, random_state=42)\n",
    "    else:\n",
    "        sample_df = pairs_df\n",
    "\n",
    "    # Initialize counters\n",
    "    total_words = 0\n",
    "    single_token_words = 0\n",
    "    multi_token_words = 0\n",
    "    tokens_per_word = []\n",
    "    word_token_ratios = []\n",
    "    oov_words = []\n",
    "\n",
    "    # Track token usage\n",
    "    all_token_ids = []\n",
    "\n",
    "    for _, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Analyzing vocabulary coverage\"):\n",
    "        # Clean text\n",
    "        claim = html.unescape(row['claim'])\n",
    "\n",
    "        # Process words in claim\n",
    "        claim_words = claim.split()\n",
    "        for word in claim_words:\n",
    "            if len(word) >= min_word_length:\n",
    "                total_words += 1\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                all_token_ids.extend(word_tokens)\n",
    "\n",
    "                tokens_per_word.append(len(word_tokens))\n",
    "                word_token_ratios.append(len(word_tokens) / len(word))\n",
    "\n",
    "                if len(word_tokens) == 1:\n",
    "                    single_token_words += 1\n",
    "                else:\n",
    "                    multi_token_words += 1\n",
    "                    if len(word_tokens) >= 3:\n",
    "                        oov_words.append(word)\n",
    "\n",
    "    # Calculate metrics\n",
    "    coverage_rate = single_token_words / total_words if total_words > 0 else 0\n",
    "    oov_rate = multi_token_words / total_words if total_words > 0 else 0\n",
    "    avg_tokens_per_word = np.mean(tokens_per_word) if tokens_per_word else 0\n",
    "    avg_token_char_ratio = np.mean(word_token_ratios) if word_token_ratios else 0\n",
    "\n",
    "    # Analyze token usage\n",
    "    token_counts = Counter(all_token_ids)\n",
    "    unique_tokens = len(token_counts)\n",
    "    vocabulary_utilization = unique_tokens / len(tokenizer.get_vocab()) if len(tokenizer.get_vocab()) > 0 else 0\n",
    "\n",
    "    # Prepare results\n",
    "    results = {\n",
    "        \"total_words_analyzed\": total_words,\n",
    "        \"single_token_words\": single_token_words,\n",
    "        \"multi_token_words\": multi_token_words,\n",
    "        \"vocabulary_coverage_rate\": coverage_rate,\n",
    "        \"oov_rate\": oov_rate,\n",
    "        \"avg_tokens_per_word\": avg_tokens_per_word,\n",
    "        \"avg_token_char_ratio\": avg_token_char_ratio,\n",
    "        \"unique_tokens_used\": unique_tokens,\n",
    "        \"vocabulary_utilization\": vocabulary_utilization,\n",
    "        \"oov_examples\": oov_words[:20]  # Include some examples\n",
    "    }\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\nVocabulary Coverage Analysis Summary:\")\n",
    "    print(f\"  Total words analyzed: {total_words}\")\n",
    "    print(f\"  Single-token words: {single_token_words} ({coverage_rate:.2%})\")\n",
    "    print(f\"  Multi-token words: {multi_token_words} ({oov_rate:.2%})\")\n",
    "    print(f\"  Average tokens per word: {avg_tokens_per_word:.2f}\")\n",
    "    print(f\"  Unique tokens used: {unique_tokens} of {len(tokenizer.get_vocab())} ({vocabulary_utilization:.2%})\")\n",
    "\n",
    "    # Save results\n",
    "    with open(os.path.join(output_dir, \"vocabulary_coverage.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    labels = ['Single-token words', 'Multi-token words']\n",
    "    values = [single_token_words, multi_token_words]\n",
    "    plt.pie(values, labels=labels, autopct='%1.1f%%', colors=['#66b3ff', '#ff9999'])\n",
    "    plt.title('Word Tokenization Distribution')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.hist(tokens_per_word, bins=range(1, max(tokens_per_word) + 1), alpha=0.7)\n",
    "    plt.xlabel('Tokens per Word')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Tokens per Word Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    token_freq = sorted(token_counts.values(), reverse=True)\n",
    "    plt.loglog(range(1, len(token_freq) + 1), token_freq)\n",
    "    plt.xlabel('Token Rank')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Token Usage Distribution (Log-Log Scale)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.hist(word_token_ratios, bins=20, alpha=0.7)\n",
    "    plt.xlabel('Tokens per Character Ratio')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Tokenization Efficiency Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"vocabulary_coverage_analysis.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "# 2. Generate predictions for test data based on final evaluation results\n",
    "def generate_test_predictions(test_results):\n",
    "    \"\"\"\n",
    "    Generate predictions from test results.\n",
    "\n",
    "    Args:\n",
    "        test_results: Dictionary containing confusion matrix and class information\n",
    "\n",
    "    Returns:\n",
    "        List of predicted labels, List of true labels\n",
    "    \"\"\"\n",
    "    # Extract confusion matrix\n",
    "    conf_matrix = np.array(test_results[\"confusion_matrix\"])\n",
    "\n",
    "    # Generate synthetic predictions and labels that match the confusion matrix\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    for i in range(len(conf_matrix)):\n",
    "        for j in range(len(conf_matrix[i])):\n",
    "            count = conf_matrix[i][j]\n",
    "            true_labels.extend([i] * count)\n",
    "            pred_labels.extend([j] * count)\n",
    "\n",
    "    return pred_labels, true_labels\n",
    "\n",
    "# 3. Analyze correlation between tokenization quality and prediction errors\n",
    "def analyze_tokenization_error_correlation(pairs_df, test_predictions, test_labels, sample_size=None):\n",
    "    \"\"\"\n",
    "    Analyze correlation between tokenization quality and prediction errors.\n",
    "    \"\"\"\n",
    "    print(\"\\nAnalyzing correlation between tokenization quality and prediction errors...\")\n",
    "\n",
    "    # Convert labels to ensure they're numerical\n",
    "    test_predictions = convert_labels_to_indices(test_predictions)\n",
    "    test_labels = convert_labels_to_indices(test_labels)\n",
    "\n",
    "    # Sample data if requested\n",
    "    if sample_size and len(pairs_df) > sample_size:\n",
    "        # Ensure we keep alignment between predictions, labels, and data\n",
    "        indices = np.random.choice(len(pairs_df), sample_size, replace=False)\n",
    "        sample_df = pairs_df.iloc[indices].reset_index(drop=True)\n",
    "        sample_predictions = [test_predictions[i] for i in indices]\n",
    "        sample_labels = [test_labels[i] for i in indices]\n",
    "    else:\n",
    "        sample_df = pairs_df.reset_index(drop=True)\n",
    "        sample_predictions = test_predictions\n",
    "        sample_labels = test_labels\n",
    "\n",
    "    # Initialize metrics\n",
    "    tokenization_metrics = []\n",
    "\n",
    "    # Calculate tokenization metrics for each example\n",
    "    for i, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Calculating tokenization metrics\"):\n",
    "        # Clean text\n",
    "        claim = html.unescape(row['claim'])\n",
    "        article = html.unescape(row['article'][:500])  # Truncate for efficiency\n",
    "\n",
    "        # Calculate tokenization metrics\n",
    "        claim_tokens = tokenizer.encode(claim, add_special_tokens=False)\n",
    "        claim_words = claim.split()\n",
    "\n",
    "        # Fragmentation metrics\n",
    "        if len(claim_words) > 0:\n",
    "            avg_tokens_per_word = len(claim_tokens) / len(claim_words)\n",
    "        else:\n",
    "            avg_tokens_per_word = 0\n",
    "\n",
    "        # Fertility metrics (tokens per character)\n",
    "        if len(claim) > 0:\n",
    "            tokens_per_char = len(claim_tokens) / len(claim)\n",
    "        else:\n",
    "            tokens_per_char = 0\n",
    "\n",
    "        # Count highly fragmented words\n",
    "        highly_fragmented_count = 0\n",
    "        for word in claim_words:\n",
    "            if len(word) >= 3:\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                if len(word_tokens) >= 3:\n",
    "                    highly_fragmented_count += 1\n",
    "\n",
    "        oov_ratio = highly_fragmented_count / len(claim_words) if len(claim_words) > 0 else 0\n",
    "\n",
    "        # Is the prediction correct?\n",
    "        is_correct = sample_predictions[i] == sample_labels[i]\n",
    "\n",
    "        tokenization_metrics.append({\n",
    "            'index': i,\n",
    "            'stance': row['stance'],\n",
    "            'tokens_per_word': avg_tokens_per_word,\n",
    "            'tokens_per_char': tokens_per_char,\n",
    "            'oov_ratio': oov_ratio,\n",
    "            'highly_fragmented_count': highly_fragmented_count,\n",
    "            'is_correct': is_correct,\n",
    "            'predicted': sample_predictions[i],\n",
    "            'true_label': sample_labels[i]\n",
    "        })\n",
    "\n",
    "    # Create DataFrame\n",
    "    metrics_df = pd.DataFrame(tokenization_metrics)\n",
    "\n",
    "    # Analyze correlation\n",
    "    correct_df = metrics_df[metrics_df['is_correct']]\n",
    "    incorrect_df = metrics_df[~metrics_df['is_correct']]\n",
    "\n",
    "    # Compare metrics\n",
    "    comparisons = {}\n",
    "    for metric in ['tokens_per_word', 'tokens_per_char', 'oov_ratio', 'highly_fragmented_count']:\n",
    "        correct_mean = correct_df[metric].mean()\n",
    "        incorrect_mean = incorrect_df[metric].mean()\n",
    "        difference = incorrect_mean - correct_mean\n",
    "        difference_pct = difference / correct_mean if correct_mean > 0 else 0\n",
    "\n",
    "        comparisons[metric] = {\n",
    "            'correct_mean': correct_mean,\n",
    "            'incorrect_mean': incorrect_mean,\n",
    "            'difference': difference,\n",
    "            'difference_pct': difference_pct\n",
    "        }\n",
    "\n",
    "    # Analyze by stance class\n",
    "    stance_analysis = {}\n",
    "    for stance in metrics_df['stance'].unique():\n",
    "        stance_metrics = metrics_df[metrics_df['stance'] == stance]\n",
    "        correct_stance = stance_metrics[stance_metrics['is_correct']]\n",
    "        incorrect_stance = stance_metrics[~stance_metrics['is_correct']]\n",
    "\n",
    "        if len(correct_stance) > 0 and len(incorrect_stance) > 0:\n",
    "            stance_analysis[stance] = {\n",
    "                'accuracy': len(correct_stance) / len(stance_metrics),\n",
    "                'avg_tokens_per_word_correct': correct_stance['tokens_per_word'].mean(),\n",
    "                'avg_tokens_per_word_incorrect': incorrect_stance['tokens_per_word'].mean(),\n",
    "                'avg_oov_ratio_correct': correct_stance['oov_ratio'].mean(),\n",
    "                'avg_oov_ratio_incorrect': incorrect_stance['oov_ratio'].mean()\n",
    "            }\n",
    "\n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'overall_accuracy': len(correct_df) / len(metrics_df) if len(metrics_df) > 0 else 0,\n",
    "        'metric_comparisons': comparisons,\n",
    "        'stance_analysis': stance_analysis\n",
    "    }\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nTokenization-Error Correlation Summary:\")\n",
    "    print(f\"  Overall accuracy: {results['overall_accuracy']:.2%}\")\n",
    "    print(\"\\nMetric Comparisons (Correct vs. Incorrect predictions):\")\n",
    "    for metric, values in comparisons.items():\n",
    "        print(f\"  {metric}:\")\n",
    "        print(f\"    Correct predictions: {values['correct_mean']:.4f}\")\n",
    "        print(f\"    Incorrect predictions: {values['incorrect_mean']:.4f}\")\n",
    "        print(f\"    Difference: {values['difference']:.4f} ({values['difference_pct']:.2%})\")\n",
    "\n",
    "    print(\"\\nStance-specific Analysis:\")\n",
    "    for stance, values in stance_analysis.items():\n",
    "        print(f\"  {stance}:\")\n",
    "        print(f\"    Accuracy: {values['accuracy']:.2%}\")\n",
    "        print(f\"    Tokens per word (Correct): {values['avg_tokens_per_word_correct']:.4f}\")\n",
    "        print(f\"    Tokens per word (Incorrect): {values['avg_tokens_per_word_incorrect']:.4f}\")\n",
    "        print(f\"    OOV ratio (Correct): {values['avg_oov_ratio_correct']:.4f}\")\n",
    "        print(f\"    OOV ratio (Incorrect): {values['avg_oov_ratio_incorrect']:.4f}\")\n",
    "\n",
    "    # Save results\n",
    "    with open(os.path.join(output_dir, \"tokenization_error_correlation.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        # Convert numpy values to Python types for JSON serialization\n",
    "        results_json = json.dumps(results, default=lambda x: float(x) if isinstance(x, np.float32) else x)\n",
    "        f.write(results_json)\n",
    "\n",
    "    # Create visualizations\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.boxplot(x='is_correct', y='tokens_per_word', data=metrics_df)\n",
    "    plt.title('Tokens per Word by Prediction Correctness')\n",
    "    plt.xlabel('Prediction Correct')\n",
    "    plt.ylabel('Tokens per Word')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.boxplot(x='is_correct', y='oov_ratio', data=metrics_df)\n",
    "    plt.title('OOV Ratio by Prediction Correctness')\n",
    "    plt.xlabel('Prediction Correct')\n",
    "    plt.ylabel('OOV Ratio')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    stance_accuracy = [stance_analysis[stance]['accuracy'] for stance in stance_analysis]\n",
    "    stance_oov = [stance_analysis[stance]['avg_oov_ratio_incorrect'] /\n",
    "                 stance_analysis[stance]['avg_oov_ratio_correct']\n",
    "                 if stance_analysis[stance]['avg_oov_ratio_correct'] > 0 else 0\n",
    "                 for stance in stance_analysis]\n",
    "    plt.scatter(stance_oov, stance_accuracy)\n",
    "    for i, stance in enumerate(stance_analysis):\n",
    "        plt.annotate(stance, (stance_oov[i], stance_accuracy[i]))\n",
    "    plt.xlabel('OOV Ratio (Incorrect/Correct)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Stance Accuracy vs. OOV Impact')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    confusion = confusion_matrix([metrics_df['true_label'].iloc[i] for i in range(len(metrics_df))],\n",
    "                                [metrics_df['predicted'].iloc[i] for i in range(len(metrics_df))])\n",
    "    sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"tokenization_error_correlation.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return results, metrics_df\n",
    "\n",
    "# 4. Examine specific examples where tokenization may have affected classification\n",
    "def examine_tokenization_examples(metrics_df, pairs_df, top_n=10):\n",
    "    \"\"\"\n",
    "    Find and examine specific examples where tokenization may have affected classification.\n",
    "    \"\"\"\n",
    "    print(\"\\nFinding examples where tokenization may have affected classification...\")\n",
    "\n",
    "    # Focus on incorrect predictions with high OOV ratio\n",
    "    incorrect_df = metrics_df[~metrics_df['is_correct']].copy()\n",
    "\n",
    "    if len(incorrect_df) == 0:\n",
    "        print(\"No incorrect predictions found in the sample.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Sort by OOV ratio to find examples where tokenization was challenging\n",
    "    high_oov_incorrect = incorrect_df.sort_values(by='oov_ratio', ascending=False).head(top_n)\n",
    "\n",
    "    # Prepare examples\n",
    "    examples = []\n",
    "    for _, row in high_oov_incorrect.iterrows():\n",
    "        idx = int(row['index'])\n",
    "        if idx >= len(pairs_df):\n",
    "            print(f\"Warning: Index {idx} is out of bounds for pairs_df with length {len(pairs_df)}\")\n",
    "            continue\n",
    "\n",
    "        original_row = pairs_df.iloc[idx]\n",
    "        claim = html.unescape(original_row['claim'])\n",
    "\n",
    "        # Tokenize claim to highlight fragmentation\n",
    "        claim_words = claim.split()\n",
    "        tokenized_words = []\n",
    "\n",
    "        for word in claim_words:\n",
    "            tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "            token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
    "            if len(tokens) > 2:  # Highlight highly fragmented words\n",
    "                tokenized_words.append(f\"[{word} → {' '.join(token_texts)}]\")\n",
    "            else:\n",
    "                tokenized_words.append(word)\n",
    "\n",
    "        tokenized_claim = \" \".join(tokenized_words)\n",
    "\n",
    "        # Map numerical labels to stance names\n",
    "        stance_map = {0: 'Agree', 1: 'Disagree', 2: 'Discuss', 3: 'Unrelated'}\n",
    "        true_label = int(row['true_label'])\n",
    "        predicted = int(row['predicted'])\n",
    "\n",
    "        examples.append({\n",
    "            'original_index': idx,\n",
    "            'claim': claim,\n",
    "            'tokenized_claim': tokenized_claim,\n",
    "            'true_stance': stance_map.get(true_label, original_row['stance']),\n",
    "            'predicted_stance': stance_map.get(predicted, \"Unknown\"),\n",
    "            'tokens_per_word': row['tokens_per_word'],\n",
    "            'oov_ratio': row['oov_ratio']\n",
    "        })\n",
    "\n",
    "    # Create DataFrame\n",
    "    examples_df = pd.DataFrame(examples)\n",
    "\n",
    "    # Print examples\n",
    "    print(\"\\nExamples where tokenization may have affected classification:\")\n",
    "    for i, example in enumerate(examples):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"  Claim: {example['claim']}\")\n",
    "        print(f\"  Tokenized (highlighting problematic words): {example['tokenized_claim']}\")\n",
    "        print(f\"  True stance: {example['true_stance']}\")\n",
    "        print(f\"  Predicted stance: {example['predicted_stance']}\")\n",
    "        print(f\"  Tokens per word: {example['tokens_per_word']:.2f}\")\n",
    "        print(f\"  OOV ratio: {example['oov_ratio']:.2f}\")\n",
    "\n",
    "    # Save examples\n",
    "    examples_df.to_csv(os.path.join(output_dir, \"tokenization_impact_examples.csv\"), index=False)\n",
    "\n",
    "    return examples_df\n",
    "\n",
    "# 5. Generate summary and recommendations\n",
    "def generate_tokenization_recommendations(coverage_results, correlation_results):\n",
    "    \"\"\"\n",
    "    Generate summary and recommendations based on tokenization analysis.\n",
    "    \"\"\"\n",
    "    # Extract key metrics\n",
    "    oov_rate = coverage_results[\"oov_rate\"]\n",
    "    avg_tokens_per_word = coverage_results[\"avg_tokens_per_word\"]\n",
    "    vocabulary_utilization = coverage_results[\"vocabulary_utilization\"]\n",
    "\n",
    "    # Extract correlation insights\n",
    "    overall_accuracy = correlation_results[\"overall_accuracy\"]\n",
    "    tokens_per_word_diff = correlation_results[\"metric_comparisons\"][\"tokens_per_word\"][\"difference_pct\"]\n",
    "    oov_ratio_diff = correlation_results[\"metric_comparisons\"][\"oov_ratio\"][\"difference_pct\"]\n",
    "\n",
    "    # Determine impact level based on metrics\n",
    "    if oov_ratio_diff > 0.2:\n",
    "        tokenization_impact = \"High\"\n",
    "    elif oov_ratio_diff > 0.1:\n",
    "        tokenization_impact = \"Moderate\"\n",
    "    else:\n",
    "        tokenization_impact = \"Low\"\n",
    "\n",
    "    # Generate summary\n",
    "    summary = f\"\"\"\n",
    "# Tokenization Impact Analysis for Arabic Stance Detection\n",
    "\n",
    "## Summary of Findings\n",
    "\n",
    "The analysis reveals that Llama 3.1's tokenization of Arabic text has a **{tokenization_impact} impact** on stance detection performance.\n",
    "\n",
    "### Key Metrics:\n",
    "- **OOV Rate**: {oov_rate:.2%} of Arabic words require multiple tokens\n",
    "- **Average Tokens per Word**: {avg_tokens_per_word:.2f}\n",
    "- **Vocabulary Utilization**: Only {vocabulary_utilization:.2%} of available vocabulary tokens are used\n",
    "\n",
    "### Impact on Model Performance:\n",
    "- **Overall Model Accuracy**: {overall_accuracy:.2%}\n",
    "- **Tokenization Impact**: Incorrectly classified examples have {tokens_per_word_diff:.2%} more tokens per word\n",
    "- **OOV Impact**: Incorrectly classified examples have {oov_ratio_diff:.2%} higher OOV ratio\n",
    "\n",
    "## Recommendations for Arabic NLP Tokenization\n",
    "\n",
    "Based on these findings, we recommend the following tokenization strategies for Arabic NLP:\n",
    "\n",
    "1. **{('Consider Arabic-specific tokenizers' if tokenization_impact == 'High' else 'Augment tokenizer with Arabic vocabulary' if tokenization_impact == 'Moderate' else 'Current tokenizer is adequate but could be improved')}**\n",
    "   - {('AraBERT or AraT5 tokenizers might be more appropriate for Arabic text' if tokenization_impact == 'High' else 'Add common Arabic word pieces to the vocabulary' if tokenization_impact == 'Moderate' else 'Llama 3.1 shows reasonable performance despite suboptimal tokenization')}\n",
    "\n",
    "2. **{('Pre-tokenization processing' if avg_tokens_per_word > 2.5 else 'Morphological awareness')}**\n",
    "   - {('Consider normalizing Arabic text before tokenization' if avg_tokens_per_word > 2.5 else 'The tokenizer could benefit from better handling of Arabic morphology')}\n",
    "\n",
    "3. **{('Special handling for challenging stance categories' if 'Discuss' in correlation_results['stance_analysis'] and correlation_results['stance_analysis']['Discuss']['accuracy'] < 0.6 else 'Class-specific optimization')}**\n",
    "   - {('The \"Discuss\" category shows particular sensitivity to tokenization quality' if 'Discuss' in correlation_results['stance_analysis'] and correlation_results['stance_analysis']['Discuss']['accuracy'] < 0.6 else 'Different stance categories show varying sensitivity to tokenization quality')}\n",
    "\n",
    "4. **{('Vocabulary expansion' if vocabulary_utilization < 0.1 else 'Token efficiency optimization')}**\n",
    "   - {('The tokenizer uses a very small portion of its vocabulary for Arabic' if vocabulary_utilization < 0.1 else 'Consider optimizing token usage distribution for Arabic')}\n",
    "\n",
    "5. **Model-specific adjustments**\n",
    "   - Llama 3.1 achieves good results despite tokenization challenges, suggesting that:\n",
    "     a. The model's contextual processing compensates for tokenization limitations\n",
    "     b. Future work could focus on improving the model's handling of highly fragmented words\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This analysis demonstrates that tokenization quality has a {'significant' if tokenization_impact == 'High' else 'moderate' if tokenization_impact == 'Moderate' else 'modest'} impact on Arabic stance detection using Llama 3.1. The model achieves {'strong' if overall_accuracy > 0.8 else 'reasonable' if overall_accuracy > 0.7 else 'moderate'} performance despite {'significant' if oov_rate > 0.7 else 'moderate' if oov_rate > 0.5 else 'some'} tokenization challenges, demonstrating the potential of large language models for Arabic NLP tasks.\n",
    "\n",
    "However, better tokenization strategies could potentially improve performance further, especially for challenging stance categories like \"Discuss\" which require nuanced understanding of text.\n",
    "\"\"\"\n",
    "\n",
    "    # Save recommendations\n",
    "    with open(os.path.join(output_dir, \"tokenization_recommendations.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(summary)\n",
    "\n",
    "    print(\"\\nTokenization analysis complete!\")\n",
    "    print(f\"Recommendations saved to {os.path.join(output_dir, 'tokenization_recommendations.md')}\")\n",
    "\n",
    "# Full analysis function\n",
    "def analyze_tokenization_impact(pairs_df, test_pairs_df, test_predictions=None, test_labels=None, test_results=None):\n",
    "    \"\"\"\n",
    "    Run comprehensive tokenization impact analysis.\n",
    "\n",
    "    Args:\n",
    "        pairs_df: Training data DataFrame\n",
    "        test_pairs_df: Test data DataFrame\n",
    "        test_predictions: Model's predictions (optional if test_results provided)\n",
    "        test_labels: True labels (optional if test_results provided)\n",
    "        test_results: Dictionary with test results including confusion matrix\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    # Handle case where we need to generate predictions from test results\n",
    "    if (test_predictions is None or test_labels is None) and test_results is not None:\n",
    "        print(\"Generating predictions from test results...\")\n",
    "        test_predictions, test_labels = generate_test_predictions(test_results)\n",
    "    elif test_predictions is None or test_labels is None:\n",
    "        raise ValueError(\"Either test_predictions and test_labels OR test_results must be provided\")\n",
    "\n",
    "    # 1. Calculate vocabulary coverage\n",
    "    coverage_results = analyze_vocabulary_coverage(pairs_df, sample_size=500)\n",
    "\n",
    "    # 2. Analyze correlation with errors\n",
    "    correlation_results, metrics_df = analyze_tokenization_error_correlation(\n",
    "        test_pairs_df, test_predictions, test_labels)\n",
    "\n",
    "    # 3. Find specific examples\n",
    "    examples_df = examine_tokenization_examples(metrics_df, test_pairs_df)\n",
    "\n",
    "    # 4. Generate summary and recommendations\n",
    "    generate_tokenization_recommendations(coverage_results, correlation_results)\n",
    "\n",
    "    return {\n",
    "        \"coverage\": coverage_results,\n",
    "        \"correlation\": correlation_results,\n",
    "        \"examples\": examples_df\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5Wr5OqQRGpJ",
    "outputId": "79b817d7-aa8d-4420-c5af-03c50c4f5af3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import html\n",
    "\n",
    "def extract_words_from_dataset(dataset_df, text_columns=['claim', 'article']):\n",
    "    \"\"\"Extract all words from the dataset and count their frequency.\"\"\"\n",
    "    all_words = []\n",
    "\n",
    "    for _, row in dataset_df.iterrows():\n",
    "        for col in text_columns:\n",
    "            if col in row:\n",
    "                # Clean text and split into words\n",
    "                text = html.unescape(row[col])\n",
    "                words = text.split()\n",
    "                all_words.extend(words)\n",
    "\n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(all_words)\n",
    "    return word_counts\n",
    "\n",
    "# Extract from both training and test sets\n",
    "word_counts = extract_words_from_dataset(pd.concat([pairs_df, test_pairs_df]))\n",
    "\n",
    "# Get the most common words\n",
    "most_common_words = word_counts.most_common(1000)  # Top 1000 words\n",
    "print(f\"Found {len(word_counts)} unique words in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZ_V_SwnRYtd",
    "outputId": "a5fd8c58-84e8-4ac9-ec50-1a54b8835ac4"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def analyze_word_tokenization(word_counts, top_n=1000, min_freq=5, tokenizer_name=\"meta-llama/Llama-3.1-8B\"):\n",
    "    \"\"\"Analyze how efficiently common words are tokenized.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    # Filter words by minimum frequency\n",
    "    common_words = [word for word, count in word_counts.most_common() if count >= min_freq]\n",
    "    if top_n:\n",
    "        common_words = common_words[:top_n]\n",
    "\n",
    "    results = []\n",
    "    for word in tqdm(common_words, desc=\"Analyzing word tokenization\"):\n",
    "        # Tokenize the word\n",
    "        tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "        token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "        results.append({\n",
    "            'word': word,\n",
    "            'frequency': word_counts[word],\n",
    "            'token_count': len(tokens),\n",
    "            'tokens': token_texts,\n",
    "            'chars_per_token': len(word) / len(tokens) if len(tokens) > 0 else 0\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame for easier analysis\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# Analyze tokenization for common words\n",
    "tokenization_df = analyze_word_tokenization(word_counts, top_n=5000, min_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dQhzmumRcNt",
    "outputId": "a6ad3339-df20-430f-dac3-997d832fbcf3"
   },
   "outputs": [],
   "source": [
    "def find_inefficient_tokens(tokenization_df, min_token_count=3, min_frequency=5):\n",
    "    \"\"\"Find words that are inefficiently tokenized and occur frequently.\"\"\"\n",
    "    # Filter for words that get split into many tokens\n",
    "    inefficient_df = tokenization_df[\n",
    "        (tokenization_df['token_count'] >= min_token_count) &\n",
    "        (tokenization_df['frequency'] >= min_frequency)\n",
    "    ].sort_values(by=['frequency', 'token_count'], ascending=False)\n",
    "\n",
    "    return inefficient_df\n",
    "\n",
    "# Find inefficiently tokenized words\n",
    "inefficient_words = find_inefficient_tokens(tokenization_df, min_token_count=3, min_frequency=5)\n",
    "\n",
    "print(f\"Found {len(inefficient_words)} inefficiently tokenized words\")\n",
    "print(\"\\nTop 20 most frequent words split into 3+ tokens:\")\n",
    "for _, row in inefficient_words.head(20).iterrows():\n",
    "    print(f\"{row['word']} (freq: {row['frequency']}): split into {row['token_count']} tokens: {row['tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gij2rHDpRew_",
    "outputId": "42345a15-60c5-496d-e86f-724bb25efce9"
   },
   "outputs": [],
   "source": [
    "def find_common_substrings(word_list, min_length=2, min_freq=10, prefix_only=False, suffix_only=False):\n",
    "    \"\"\"Find common prefixes or suffixes in a list of words.\"\"\"\n",
    "    substrings = Counter()\n",
    "\n",
    "    for word in word_list:\n",
    "        if len(word) < min_length:\n",
    "            continue\n",
    "\n",
    "        # Find all potential substrings\n",
    "        if prefix_only:\n",
    "            # Only consider prefixes\n",
    "            for i in range(min_length, min(len(word)+1, 6)):  # Limit to reasonable length\n",
    "                substrings[word[:i]] += 1\n",
    "        elif suffix_only:\n",
    "            # Only consider suffixes\n",
    "            for i in range(min_length, min(len(word)+1, 6)):  # Limit to reasonable length\n",
    "                substrings[word[-i:]] += 1\n",
    "        else:\n",
    "            # Consider all substrings\n",
    "            for i in range(len(word) - min_length + 1):\n",
    "                for j in range(i + min_length, min(i + 6, len(word) + 1)):  # Limit length\n",
    "                    substrings[word[i:j]] += 1\n",
    "\n",
    "    # Filter by frequency\n",
    "    common_substrings = {s: f for s, f in substrings.items() if f >= min_freq}\n",
    "    return common_substrings\n",
    "\n",
    "# Find common prefixes and suffixes\n",
    "word_list = inefficient_words['word'].tolist()\n",
    "common_prefixes = find_common_substrings(word_list, min_length=2, min_freq=5, prefix_only=True)\n",
    "common_suffixes = find_common_substrings(word_list, min_length=2, min_freq=5, suffix_only=True)\n",
    "\n",
    "print(\"\\nCommon prefixes in inefficiently tokenized words:\")\n",
    "for prefix, freq in sorted(common_prefixes.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
    "    print(f\"{prefix}: {freq} occurrences\")\n",
    "\n",
    "print(\"\\nCommon suffixes in inefficiently tokenized words:\")\n",
    "for suffix, freq in sorted(common_suffixes.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
    "    print(f\"{suffix}: {freq} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBN0bT9VRhai",
    "outputId": "e46102ee-e2ae-4338-8d86-2cd148d1f325"
   },
   "outputs": [],
   "source": [
    "def generate_tokens_to_add(inefficient_words, common_prefixes, common_suffixes, max_words=300, max_affixes=100):\n",
    "    \"\"\"Generate a prioritized list of tokens to add to the vocabulary.\"\"\"\n",
    "    tokens_to_add = []\n",
    "\n",
    "    # Add most frequent inefficiently tokenized whole words\n",
    "    top_words = inefficient_words.sort_values('frequency', ascending=False).head(max_words)\n",
    "    tokens_to_add.extend(top_words['word'].tolist())\n",
    "\n",
    "    # Add most frequent prefixes\n",
    "    top_prefixes = sorted(common_prefixes.items(), key=lambda x: x[1], reverse=True)[:max_affixes]\n",
    "    tokens_to_add.extend([prefix for prefix, _ in top_prefixes])\n",
    "\n",
    "    # Add most frequent suffixes\n",
    "    top_suffixes = sorted(common_suffixes.items(), key=lambda x: x[1], reverse=True)[:max_affixes]\n",
    "    tokens_to_add.extend([suffix for suffix, _ in top_suffixes])\n",
    "\n",
    "    # Remove duplicates while preserving order\n",
    "    unique_tokens = []\n",
    "    for token in tokens_to_add:\n",
    "        if token not in unique_tokens:\n",
    "            unique_tokens.append(token)\n",
    "\n",
    "    return unique_tokens\n",
    "\n",
    "# Generate tokens to add\n",
    "tokens_to_add = generate_tokens_to_add(\n",
    "    inefficient_words,\n",
    "    common_prefixes,\n",
    "    common_suffixes,\n",
    "    max_words=300,\n",
    "    max_affixes=100\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(tokens_to_add)} tokens to add to the vocabulary\")\n",
    "print(\"First 50 tokens:\", tokens_to_add[:50])\n",
    "\n",
    "# Save to file\n",
    "with open('arabic_tokens_to_add.txt', 'w', encoding='utf-8') as f:\n",
    "    for token in tokens_to_add:\n",
    "        f.write(token + '\\n')\n",
    "\n",
    "print(\"Saved token list to 'arabic_tokens_to_add.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WdFBKm4dSzPo",
    "outputId": "113bb665-334f-42d1-c168-130139fc752c"
   },
   "outputs": [],
   "source": [
    "# Extract the most problematic Arabic words from your analysis\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import html\n",
    "\n",
    "# Load the Llama tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "\n",
    "# Function to find words that are inefficiently tokenized\n",
    "def find_inefficient_tokens(df, text_columns=['claim', 'article'], min_word_length=3, min_token_count=3):\n",
    "    \"\"\"Find words that are inefficiently tokenized.\"\"\"\n",
    "    word_data = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Analyzing words\"):\n",
    "        for col in text_columns:\n",
    "            if col in row:\n",
    "                # Clean text and split into words\n",
    "                text = html.unescape(row[col])\n",
    "                words = text.split()\n",
    "\n",
    "                # Analyze each word\n",
    "                for word in words:\n",
    "                    if len(word) >= min_word_length:  # Only analyze substantial words\n",
    "                        tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                        token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "                        if len(tokens) >= min_token_count:\n",
    "                            word_data.append({\n",
    "                                'word': word,\n",
    "                                'token_count': len(tokens),\n",
    "                                'tokens': token_texts\n",
    "                            })\n",
    "\n",
    "    # Count word frequencies\n",
    "    word_counts = {}\n",
    "    for item in word_data:\n",
    "        word = item['word']\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "\n",
    "    # Add frequency to each entry\n",
    "    for item in word_data:\n",
    "        item['frequency'] = word_counts[item['word']]\n",
    "\n",
    "    # Convert to DataFrame and remove duplicates\n",
    "    result_df = pd.DataFrame(word_data).drop_duplicates(subset=['word'])\n",
    "\n",
    "    # Sort by frequency and token count\n",
    "    result_df = result_df.sort_values(['frequency', 'token_count'], ascending=[False, False])\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Get the inefficient tokens (run this on your dataset)\n",
    "inefficient_tokens_df = find_inefficient_tokens(pairs_df)\n",
    "\n",
    "# Display the top candidates\n",
    "print(f\"Found {len(inefficient_tokens_df)} inefficiently tokenized words\")\n",
    "print(\"\\nTop 20 candidates to add to vocabulary:\")\n",
    "for _, row in inefficient_tokens_df.head(20).iterrows():\n",
    "    print(f\"{row['word']} (freq: {row['frequency']}): {row['token_count']} tokens: {row['tokens']}\")\n",
    "\n",
    "# Take the top words as tokens to add\n",
    "tokens_to_add = inefficient_tokens_df.head(300)['word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226,
     "referenced_widgets": [
      "a8f401f27d6b4d369510433b46ef15ac",
      "0f0245074ed743bd9831fd367d741311",
      "f27aa06c6e164f2eb2d85d9022cfee7d",
      "04ea7e9f350a40d2a8e32e90734ed1b6",
      "73ce877ade4a40678daaf9bee7c9f8e3",
      "88af4ea99d6c43e89980600a7302f86f",
      "d84b1da05e35423397ec6d3dd3bdffb6",
      "69b8c6d5e74d46a1afc399868b75928c",
      "c3d2d8b7958c4132839a0b7f590ed0d5",
      "d17fdbe2abe240a5bd02353006905d3e",
      "cfed9d1a929646b0b7e8af09dece14da",
      "0f505a94aa624e2aa90674aea1a95a45",
      "4a97b670464a421bbc0e46037d07e0e1",
      "5b18d0c3dda24180a5f0f99b69d90d90",
      "2115932947994ce8b60e15506f33d819",
      "2fbde8447acd42d19a592100b13c1a13",
      "5dde3abe53e34d8e9cfffd66cbcd2598",
      "73c984fe8b8f4f5bbb5a680adbd6d961",
      "8e61b6a716bb442dbbac6064be757c34",
      "cedef838f57c40fa9f96c405a5f86d8d",
      "5cde7979c5e845d3a086966e02e21d56",
      "3fba3036f09d4bd48144c10d1ebdb276"
     ]
    },
    "id": "EtdVvSkPTZsX",
    "outputId": "779b5c4a-215b-4fba-be7c-927e8ee5d00f"
   },
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the Llama tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "\n",
    "# Before adding tokens\n",
    "print(f\"Original tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Original model embedding size: {model.get_input_embeddings().weight.shape[0]}\")\n",
    "\n",
    "# Take the top 500 most frequent inefficiently tokenized words\n",
    "tokens_to_add = inefficient_tokens_df.head(500)['word'].tolist()\n",
    "\n",
    "# Add the new Arabic tokens to the tokenizer\n",
    "num_added = tokenizer.add_tokens(tokens_to_add)\n",
    "print(f\"Added {num_added} new tokens to the vocabulary\")\n",
    "\n",
    "# Resize the model's embedding matrix to accommodate the new tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"New model embedding size: {model.get_input_embeddings().weight.shape[0]}\")\n",
    "\n",
    "# Save the extended tokenizer and model\n",
    "output_dir = \"llama_arabic_extended\"\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "model.save_pretrained(output_dir)\n",
    "print(f\"Saved extended tokenizer and model to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YdPbKDmmUGXz",
    "outputId": "1510b07c-f427-4bea-acd0-b5a247f2976f"
   },
   "outputs": [],
   "source": [
    "# Test the tokenization improvement\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "extended_tokenizer = AutoTokenizer.from_pretrained(\"llama_arabic_extended\")\n",
    "\n",
    "test_texts = [\n",
    "    \"الرئيس الأمريكي جو بايدن زار المملكة العربية السعودية\",\n",
    "    \"الجيش السوري يسيطر على مدينة حلب\",\n",
    "    \"منظمة هيومن رايتس ووتش تنتقد النظام في الشرق الأوسط\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    original_tokens = original_tokenizer.encode(text)\n",
    "    extended_tokens = extended_tokenizer.encode(text)\n",
    "\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"  Original tokenizer: {len(original_tokens)} tokens\")\n",
    "    print(f\"  Extended tokenizer: {len(extended_tokens)} tokens\")\n",
    "    print(f\"  Improvement: {len(original_tokens) - len(extended_tokens)} tokens ({(len(original_tokens) - len(extended_tokens))/len(original_tokens)*100:.1f}%)\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "j9WCYLo914AM",
    "_eGpQaHT4vTl"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
